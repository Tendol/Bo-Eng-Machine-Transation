{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "        input='../data/boMonoData.txt', \n",
    "        model_prefix='m', \n",
    "        vocab_size=40000)\n",
    "#         user_defined_symbols=['སྒྲོ'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use user defined symbols to make sure the diactrics are never segmented, and same for vowels on the aplhabets. \n",
    "Problem: If we do that then it always breaks་་བསྟན་ into [བ, སྟ, ན] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁', 'སུམ་ཅུ་རྩ་གསུམ་པ', 'འི་ལྷ་རྣམས་', 'ཀྱི་', 'ཁ་དོག་', 'གི་', 'མཐུ་', 'བས་', 'ལྷག་པའི་', 'སྣང་བ་', 'རྒྱ་', 'ཆེན་པོས་', 'ཁྱབ་', 'པར་གྱུར་', 'འཇིག་རྟེན་གྱི་', 'འཇིག་རྟེན་གྱི་', 'བར་', 'གང་ན་', 'ཉི་མ་དང་ཟླ་བ་', 'འདི་ལྟར་', 'རྫུ་འཕྲུལ་ཆེ་བ་', 'འདི་ལྟར་', 'མཐུ་ཆེ་བ་', 'འདི་', 'གཉིས་ཀྱི་', 'འོད་', 'དག་', 'ཉམས་སུ་', 'མི་', 'མྱོང་', 'བའི་', 'མུན་པ་མུན་ནག་', 'མུན་', 'པར་བྱེད་', 'པས་', 'གནག་', 'པར་གྱུར་པ་', 'གང་དག་ཡིན་པ་', 'དེ་དག་ཀྱང་', 'དེའི་ཚེ་ན་', 'སྣང་བ་', 'རྒྱ་', 'ཆེན་པོས་', 'ཁྱབ་', 'པར་གྱུར་ནས་', 'སེམས་ཅན་གང་དག་', 'དེར་', 'སྐྱེས་པ་', 'དག་གིས་', 'རང་གི་', 'ལག་པ་', 'བརྐྱང་བ', '་', 'ཡང་མི་', 'མཐོང་བ་', 'དེ་དག་གིས་ཀྱང་', 'འོད་', 'དེས་', 'སེམས་ཅན་', 'གཅིག་གིས་གཅིག་', 'མཐོང་', 'ནས', 'ཤེས་ལྡན་དག་', 'སེམས་ཅན་', 'གཞན་ཡང་', 'འདིར་', 'སྐྱེས་སོ་་', 'ཤེས་ལྡན་དག་', 'སེམས་ཅན་', 'གཞན་ཡང་', 'འདིར་', 'སྐྱེས་', 'སོ་ཞེས་', 'ཤེས་', 'པར་གྱུར་ཏོ་', 'མ་', 'མ་', 'བརྒྱད་', 'པོ་', 'པ', 'ང་', 'ན་', 'འཚོ་བའི་', 'མ་', 'མ་', 'གཉིས་དང་', 'ནུ་མ་', 'སྣ', 'ུན་', 'པའི་', 'མ་', 'མ་', 'གཉིས་དང་', 'དྲི་མ་', 'འ', 'ཕྱི་', 'བའི་', 'མ་', 'མ་', 'གཉིས་དང་', 'རྩེ་', 'འགྲོགས་', 'ཀྱི་', 'མ་', 'མ་', 'གཉིས་ལ་', 'རྗེས་སུ་', 'གཏད་དོ་་', 'དེ་', 'མ་', 'མ་', 'བརྒྱད་', 'པོ་', 'དག་གིས་', 'འོ་མ་དང་ཞོ་དང་', 'མར་དང་ཞུན་མར་དང་', 'མར་', 'གྱི་', 'སྙིང་ཁུ', '་དང་', 'གཞན་ཡང་', 'ཡོ་བྱད་', 'ཀྱི་', 'བྱེ་བྲག་', 'གཙོ་བོ་', 'གཙོ་བོ་', 'དག་གིས་', 'བ', 'སྲི', 'ངས་', 'པར་བྱེད་', 'སྐྱེད་པར་བྱེད་', 'ཅིང་', 'རྫིང་', 'ན་གནས་པའི་', 'པདྨ་', 'བཞིན་དུ་', 'སྐྱེད་', 'པར་བྱེད་དོ་']]\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='m.model')\n",
    "print(sp.encode(['སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས་ལྷག་པའི་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་འཇིག་རྟེན་གྱི་འཇིག་རྟེན་གྱི་བར་གང་ན་ཉི་མ་དང་ཟླ་བ་འདི་ལྟར་རྫུ་འཕྲུལ་ཆེ་བ་འདི་ལྟར་མཐུ་ཆེ་བ་འདི་གཉིས་ཀྱི་འོད་དག་ཉམས་སུ་མི་མྱོང་བའི་མུན་པ་མུན་ནག་མུན་པར་བྱེད་པས་གནག་པར་གྱུར་པ་གང་དག་ཡིན་པ་དེ་དག་ཀྱང་དེའི་ཚེ་ན་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་ནས་སེམས་ཅན་གང་དག་དེར་སྐྱེས་པ་དག་གིས་རང་གི་ལག་པ་བརྐྱང་བ་ཡང་མི་མཐོང་བ་དེ་དག་གིས་ཀྱང་འོད་དེས་སེམས་ཅན་གཅིག་གིས་གཅིག་མཐོང་ནསཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་་ཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་ཞེས་ཤེས་པར་གྱུར་ཏོ་མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུན་པའི་མ་མ་གཉིས་དང་དྲི་མ་འཕྱི་བའི་མ་མ་གཉིས་དང་རྩེ་འགྲོགས་ཀྱི་མ་མ་གཉིས་ལ་རྗེས་སུ་གཏད་དོ་་དེ་མ་མ་བརྒྱད་པོ་དག་གིས་འོ་མ་དང་ཞོ་དང་མར་དང་ཞུན་མར་དང་མར་གྱི་སྙིང་ཁུ་དང་གཞན་ཡང་ཡོ་བྱད་ཀྱི་བྱེ་བྲག་གཙོ་བོ་གཙོ་བོ་དག་གིས་བསྲིངས་པར་བྱེད་སྐྱེད་པར་བྱེད་ཅིང་རྫིང་ན་གནས་པའི་པདྨ་བཞིན་དུ་སྐྱེད་པར་བྱེད་དོ་'], out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiny vitory: using 10000 vocab_size finally tokenized my name correctly. `"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
