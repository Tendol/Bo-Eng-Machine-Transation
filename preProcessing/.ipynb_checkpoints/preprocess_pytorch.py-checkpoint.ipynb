{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "from unicodedata import normalize\n",
    "import os\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clean the data**\n",
    "\n",
    "*Tibetan*\n",
    "\n",
    "* Remove shey \"།\" (end of the line punctuation) \n",
    "* Remove any numbers \n",
    "* Remove any charcter that isn't Tibetan alphabet example  ༼༄༸ or non-Tibetan characters \n",
    "* There is no upper or lower case in Tibetan so no need to normalize \n",
    "* Make sure that there is no space between words (seperated by tsek \"་\")\n",
    "* Remove words that contain non Tibetan alphabets example: བས྄ྟན༼་\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Tibetan unicode array for easy access**\n",
    "\n",
    "*Tibetan Unicode Array:*\n",
    "\n",
    "* Tibetan Vowel : (ུ): 3956 ( ི) : 3954 ( ེ) : 3962  ( ོ) : 3964\n",
    "* Consonants : 3904 - 3946 \n",
    "* Subjoined Consonants : 3984 - 4028 \n",
    "* Numbers : 3872 - 3881 \n",
    "* punctuation: Tsek (་) : 3851 ; shey (།) : 3853 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tib_unicode = []\n",
    "# adding consonants \n",
    "for i in range(3904, 3947):\n",
    "    tib_unicode.append(i)\n",
    "# adding subjoined consonants \n",
    "for i in range(3984, 4029):\n",
    "    tib_unicode.append(i)\n",
    "# adding numbers \n",
    "for i in range(3872, 3881):\n",
    "    tib_unicode.append(i)\n",
    "# adding punctuations \n",
    "tib_unicode.append(3851)\n",
    "tib_unicode.append(3853)\n",
    "# adding vowels\n",
    "tib_unicode.append(3956)\n",
    "tib_unicode.append(3954)\n",
    "tib_unicode.append(3962)\n",
    "tib_unicode.append(3964)\n",
    "\n",
    "tib_str = \"\" # Contains all Tibetan alphabets, numbers, and special characters.\n",
    "tib_alph_str = \"\" # Contains only Tibetan alphabets\n",
    "tib_num = \"\" # Contains only Tibetan numbers \n",
    "\n",
    "for i in range(3904, 3947):\n",
    "    tib_str += chr(i)\n",
    "    tib_alph_str += chr(i)\n",
    "# adding subjoined consonants \n",
    "for i in range(3984, 4029):\n",
    "    tib_str += chr(i)\n",
    "    tib_alph_str += chr(i)\n",
    "# adding numbers \n",
    "for i in range(3872, 3881):\n",
    "    tib_str += chr(i)\n",
    "    tib_num += chr(i)\n",
    "# adding punctuations \n",
    "tib_str += chr(3851)\n",
    "tib_str += chr(3853)\n",
    "# adding vowels\n",
    "tib_str += chr(3956)\n",
    "tib_str += chr(3954)\n",
    "tib_str += chr(3962)\n",
    "tib_str += chr(3964)\n",
    "\n",
    "tib_alph_str += chr(3956)\n",
    "tib_alph_str += chr(3954)\n",
    "tib_alph_str += chr(3962)\n",
    "tib_alph_str += chr(3964)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# checks if the word contains non Tibetan alphabets\n",
    "def isalpha(word):\n",
    "    for w in word:\n",
    "        if w not in tib_alph_str:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# clean a list of lines (Tibetan)\n",
    "def clean_lines_bo(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(tib_str))\n",
    "\n",
    "    for line in lines:\n",
    "        \n",
    "        #  remove strings between [] that was not translated into English (this is for this specific data)\n",
    "        line = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", line)\n",
    "\n",
    "        # tokenize on tsek and shek\n",
    "        line = re.split(\"་|།\", line)\n",
    "\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if isalpha(word)]\n",
    "\n",
    "        line = '་'.join(line)\n",
    "\n",
    "        # remove any empty line or white spaces at the end of the line\n",
    "#         if line.rstrip():\n",
    "            \n",
    "        # store as string (removed shek)\n",
    "        cleaned.append(line)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*English*\n",
    "\n",
    "* Remove punctuation \n",
    "* Remove any numbers \n",
    "* Remove any character that is not an English alphabet \n",
    "* Normalize everything to lower letters \n",
    "* Remove words that contain non English alphabets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines (English)\n",
    "def clean_lines_en(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "\n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "\n",
    "        # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        \n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "    \n",
    "        # # store as string\n",
    "#         if not line == []:    \n",
    "        cleaned.append(' '.join(line))\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the clean sentences to a file** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as filehandle:\n",
    "        filehandle.writelines(\"%s\\n\" % sentence for sentence in sentences)\n",
    "\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_clean_sentences_binary(sentences, folder, filename):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get information on the shortest and longest sentences in the two data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*English*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortest and longest sentence lengths\n",
    "def sentence_lengths_en(sentences):\n",
    "\tlengths = [len(s.split()) for s in sentences]\n",
    "\treturn min(lengths), max(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tibetan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_lengths_bo(sentences):\n",
    "\tlengths = [len(s.split(\"་\")) for s in sentences]\n",
    "\treturn min(lengths), max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan data: sentences=106872, min=1, max=894\n",
      "Saved: ../data/train.bo\n",
      "རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ་བ་ལོ་ལེགས་པ་སྐྱེ་བོ་དང་མི་མང་པོས་གང་བ་བྱེད་དུ་བཅུག་གོ་\n",
      "དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོགས་དང་རྟ་པའི་ཚོགས་དང་ཤིང་རྟ་པའི་ཚོགས་དང་དཔུང་བུ་ཆུང་གི་ཚོགས་གོ་བསྐོན་ཏེ་ཡུལ་མ་ག་དཧའི་རྒྱལ་པོའི་ཁ་བ་མ་གཏོགས་པ་བཅོམ་ནས་ཕྱིར་ལྡོག་པར་བྱེད་དོ་\n",
      "སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས་ལྷག་པའི་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་འཇིག་རྟེན་གྱི་འཇིག་རྟེན་གྱི་བར་གང་ན་ཉི་མ་དང་ཟླ་བ་འདི་ལྟར་རྫུ་འཕྲུལ་ཆེ་བ་འདི་ལྟར་མཐུ་ཆེ་བ་འདི་གཉིས་ཀྱི་འོད་དག་ཉམས་སུ་མི་མྱོང་བའི་མུན་པ་མུན་ནག་མུན་པར་བྱེད་པས་གནག་པར་གྱུར་པ་གང་དག་ཡིན་པ་དེ་དག་ཀྱང་དེའི་ཚེ་ན་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་ནས་སེམས་ཅན་གང་དག་དེར་སྐྱེས་པ་དག་གིས་རང་གི་ལག་པ་བརྐྱང་བ་ཡང་མི་མཐོང་བ་དེ་དག་གིས་ཀྱང་འོད་དེས་སེམས་ཅན་གཅིག་གིས་གཅིག་མཐོང་ནསཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་་ཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་ཞེས་ཤེས་པར་གྱུར་ཏོ་\n",
      "མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུན་པའི་མ་མ་གཉིས་དང་དྲི་མ་འཕྱི་བའི་མ་མ་གཉིས་དང་རྩེ་འགྲོགས་ཀྱི་མ་མ་གཉིས་ལ་རྗེས་སུ་གཏད་དོ་་དེ་མ་མ་བརྒྱད་པོ་དག་གིས་འོ་མ་དང་ཞོ་དང་མར་དང་ཞུན་མར་དང་མར་གྱི་སྙིང་ཁུ་དང་གཞན་ཡང་ཡོ་བྱད་ཀྱི་བྱེ་བྲག་གཙོ་བོ་གཙོ་བོ་དག་གིས་བསྲིངས་པར་བྱེད་སྐྱེད་པར་བྱེད་ཅིང་རྫིང་ན་གནས་པའི་པདྨ་བཞིན་དུ་སྐྱེད་པར་བྱེད་དོ་\n",
      "རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀྱི་དབང་ཕྱུག་མཐུ་དང་བརྩོན་འགྲུས་ཐོབ་པ་ས་ཆེན་པོའི་དཀྱིལ་འཁོར་མངོན་པར་རྒྱས་པར་བྱས་ཏེ་གནས་པ་རྣམས་ཀྱི་བཟོའི་གནས་དང་ལས་ཀྱི་གནས་ཐ་དད་པར་གྱུར་པ་གང་དག་ཡིན་པ་འདི་ལྟ་སྟེ་གླང་པོ་ཆེའི་གཉར་ཞོན་པ་དང་རྟ་ལ་ཞོན་པ་དང་ཤིང་རྟའི་ཐབས་དང་རལ་གྲིའི་ཐབས་དང་འཕོང་དང་ཕྱིར་བསྣུར་བ་དང་མདུན་དུ་བསྣུར་བ་དང་ལྕགས་ཀྱུས་སྒྱུར་ཐབས་དང་ཞགས་པ་གདབ་པ་དང་མདའ་བོ་ཆེ་འཕེན་ཐབས་དང་འཛིན་སྟངས་དང་གོམ་སྟངས་དང་ཐོར་ཚུགས་དང་གཅད་པ་དང་དྲལ་བ་དང་དབུག་པ་དང་གནས་ལྔ་པོ་འདི་ལྟ་སྟེ་རྒྱང་ནས་ཕོག་པ་དང་སྒྲ་གྲག་པར་ཕོག་པ་དང་གནད་དུ་ཕོག་པ་དང་མི་འཆོར་བར་ཕོག་པ་དང་ཚབས་ཆེ་བ་དེ་དག་ལ་ཡང་ཞུགས་ཤིང་བྱང་བར་གྱུར་ཏོ་\n",
      "\n",
      "English data: sentences=106872, min=0, max=390\n",
      "Saved: ../data/train.en\n",
      "under his rule the kingdom prospered and thrived crops were bountiful and the land teemed with animals and people\n",
      "he called up the four branches of his armed forcesthe elephant corps the cavalry the charioteer corps and the infantryand laid waste to all of magadha save rajagrha before returning\n",
      "bathed in a vast light more luminous than the glow of the gods of the thirtythree so great was this miraculous manifestation it was as if the sun and moon shone in the gulf between worlds so great was its strength that darkness everywhere even the pitchblack darkness of dark places dark from never knowing the light of the sun and moon was filled with a vast light beings born in those places had never even seen so far as their outstretched hands yet by this light these beings saw one another and exclaimed you there there are others who have been born here there are others who have been born here\n",
      "was entrusted to eight nursemaids two to cuddle him two to breastfeed him two to change his diapers and two to play with him fortified with milk curd butter ghee cream and other nourishing foods he grew quickly shooting up like a lotus in a pond\n",
      "he trained in and mastered those arts and skills needed to be crowned and anointed a ksatriya king to attain the might and dedication of a field marshal and to conquer and occupy the world riding on the neck of an elephant riding horseback charioteering swordsmanship archery advancing yielding wielding a hook throwing a lasso casting a spear and how to hold a weapon march tie a topknot slash quarter pierce and strike in five waysstriking from a distance striking a target using acoustic location striking a fatal blow striking without hesitation and striking forcefully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    bo_text = \"../data/bo.txt\"\n",
    "    en_text = \"../data/en.txt\"\n",
    "    clean_bo = \"../data/train.bo\"\n",
    "    clean_en = \"../data/train.en\"\n",
    "\n",
    "    # Tibetan\n",
    "    doc = load_doc(bo_text)\n",
    "    sentences = to_sentences(doc)\n",
    "    sentences = clean_lines_bo(sentences)\n",
    "    minlen, maxlen = sentence_lengths_bo(sentences)\n",
    "    print('Tibetan data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "    save_clean_sentences(sentences, clean_bo) \n",
    "    # spot check\n",
    "    for i in range(5):\n",
    "        print(sentences[i])\n",
    "    print()\n",
    "    # English\n",
    "    doc = load_doc(en_text)\n",
    "    sentences = to_sentences(doc)\n",
    "    sentences = clean_lines_en(sentences)\n",
    "    minlen, maxlen = sentence_lengths_en(sentences)\n",
    "    print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "\n",
    "    save_clean_sentences(sentences, clean_en)\n",
    "    \n",
    "    # spot check\n",
    "    for i in range(5):\n",
    "        print(sentences[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 : Tokenize the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a Tibetan tokenizer using sentencepiece and monolingual Tibetan data**\n",
    "\n",
    "Using 32000 vocabulary size .. just because (also it was used by the author who wrote sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tibetan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/boTokenData.txt', \n",
    "    model_prefix='bo', \n",
    "    vocab_size=32000, \n",
    "    pad_id = 3\n",
    ")\n",
    "# sp = spm.SentencePieceProcessor(model_file='train.model')\n",
    "# print(sp.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*English*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.train(\n",
    "    input='../data/enTokenData.txt', \n",
    "    model_prefix='en', \n",
    "    vocab_size=25000, \n",
    "    pad_id = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Segmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tibetan Segmentation Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3645, 18003, 531, 6258, 2155], [5, 3334, 0, 6082, 4, 6751, 1031, 2262, 1962, 0]]\n",
      "བྲག་སྐུ་དང་ དེའི་ཚེ་མུ་སྟེགས་ཅན་ལོངས་སྤྱོད་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='bo.model')\n",
    "print(sp.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(sp.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(sp.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(sp.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "sp.get_piece_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English Segmentation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]\n",
      "['offices know  wrongdoing verdictukje', 'universallina']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='en.model')\n",
    "print(sp.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(sp.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(sp.decode([[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]))\n",
    "sp.get_piece_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizing training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tibetan*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/tokenized.bo\n",
      "['▁', 'རྒྱལ་པོ་ཞེས་བྱ་བ', 'ས་', 'རྒྱལ་སྲིད་', 'འབྱོར་པ་རྒྱས་པ་', 'བདེ་བ་ལོ་ལེགས་པ་', 'སྐྱེ་བོ་དང་', 'མི་མང་པོས་གང་བ་', 'བྱེད་དུ་བཅུག་གོ་']\n",
      "['▁དེས་', 'དཔུང་གི་ཚོགས་', 'ཡན་ལག་བཞི་པ་', 'གླང་པོ་ཆེ་', 'པའི་', 'ཚོགས་དང་', 'རྟ', '་', 'པའི་', 'ཚོགས་དང་', 'ཤིང་རྟ་', 'པའི་', 'ཚོགས་དང་', 'དཔུང་བུ་ཆུང་གི', '་', 'ཚོགས་', 'གོ་བསྐོན་ཏེ་', 'ཡུལ་', 'མ་', 'ག་', 'ད', 'ཧ', 'འི་', 'རྒྱལ་པོའི་', 'ཁ་བ་', 'མ་གཏོགས་པ་', 'བཅོམ་ནས་', 'ཕྱིར་ལྡོག་པ', 'ར་བྱེད་དོ་']\n",
      "['▁', 'སུམ་ཅུ་རྩ་གསུམ་པ', 'འི་ལྷ་རྣམས་', 'ཀྱི་', 'ཁ་དོག་', 'གི་', 'མཐུ་', 'བས་', 'ལྷག་པའི་', 'སྣང་བ་རྒྱ་ཆེན་པོ', 'ས་ཁྱབ་པར་གྱུར་', 'འཇིག་རྟེན་གྱི་', 'འཇིག་རྟེན་གྱི་', 'བར་', 'གང་ན་', 'ཉི་མ་དང་ཟླ་བ་', 'འདི་ལྟར་', 'རྫུ་འཕྲུལ་ཆེ་བ་', 'འདི་ལྟར་', 'མཐུ་ཆེ་བ་', 'འདི་', 'གཉིས་ཀྱི་', 'འོད་', 'དག་', 'ཉམས་སུ་', 'མི་', 'མྱོང་བ', 'འི་', 'མུན་པ་མུན་ནག་', 'མུན་པར་', 'བྱེད་པས་', 'གནག་', 'པར་གྱུར་པ་', 'གང་དག་ཡིན་པ་', 'དེ་དག་ཀྱང་', 'དེའི་ཚེ་ན་', 'སྣང་བ་རྒྱ་ཆེན་པོ', 'ས་ཁྱབ་པར་གྱུར་', 'ནས་', 'སེམས་ཅན་གང་དག་', 'དེར་སྐྱེས་པ', '་དག་གིས་', 'རང་གི་', 'ལག་པ་བརྐྱང་བ', '་', 'ཡང་', 'མི་མཐོང་བ་', 'དེ་དག་གིས་ཀྱང་', 'འོད་དེས་', 'སེམས་ཅན་', 'གཅིག་གིས་གཅིག་', 'མཐོང་ནས', 'ཤེས་ལྡན་དག་', 'སེམས་ཅན་', 'གཞན་ཡང་', 'འདིར་སྐྱེས་སོ་', '་', 'ཤེས་ལྡན་དག་', 'སེམས་ཅན་', 'གཞན་ཡང་', 'འདིར་སྐྱེས་སོ་', 'ཞེས་', 'ཤེས་', 'པར་གྱུར་ཏོ་']\n",
      "['▁', 'མ་མ་བརྒྱད་པོ་', 'པང་ན་འཚོ་བའི་མ་མ', '་', 'གཉིས་དང་', 'ན', 'ུ་མ་སྣུན་པའི་མ་མ', '་', 'གཉིས་དང་', 'དྲི་མ་', 'འ', 'ཕྱི་', 'བའི་མ་མ་གཉིས་དང་', 'རྩེ་', 'འགྲོགས་', 'ཀྱི་', 'མ་', 'མ་', 'གཉིས་ལ་', 'རྗེས་སུ་', 'གཏད་དོ་་', 'དེ་', 'མ་མ་བརྒྱད་པོ་', 'དག་གིས་', 'འོ་མ་དང་ཞོ་དང་', 'མར་དང་ཞུན་མར་དང་', 'མར་གྱི་སྙིང་ཁུ་', 'དང་', 'གཞན་ཡང་', 'ཡོ་བྱད་ཀྱི་', 'བྱེ་བྲག་', 'གཙོ་བོ་', 'གཙོ་བོ་', 'དག་གིས་', 'བསྲིངས་', 'པར་བྱེད་', 'སྐྱེད་པར་བྱེད་', 'ཅིང་', 'རྫིང་', 'ན་གནས་པའི་', 'པདྨ་', 'བཞིན་དུ་', 'སྐྱེད་', 'པར་བྱེད་དོ་']\n",
      "['▁རྒྱལ་པོ་', 'རྒྱལ་རིགས་', 'སྤྱི་བོར་', 'དབང་བསྐུར་བ་', 'ལྗོངས་ཀྱི་', 'དབང་ཕྱུག་', 'མཐུ་དང་', 'བརྩོན་འགྲུས་', 'ཐོབ་པ་', 'ས་', 'ཆེན་པོའི་', 'དཀྱིལ་འཁོར་', 'མངོན་པར་', 'རྒྱས་པར་བྱས་ཏེ་', 'གནས་པ་', 'རྣམས་ཀྱི་', 'བཟོའི་གནས་', 'དང་', 'ལས་ཀྱི་གནས་', 'ཐ་དད་', 'པར་གྱུར་པ་', 'གང་དག་ཡིན་པ་', 'འདི་ལྟ་སྟེ་', 'གླང་པོ་ཆེའི་', 'ག', 'ཉར་', 'ཞོན་', 'པ་དང་', 'རྟ', '་', 'ལ་ཞོན་', 'པ་དང་', 'ཤིང་རྟའི་', 'ཐབས་དང་', 'རལ་གྲིའི་', 'ཐབས་དང་', 'འཕོང་', 'དང་', 'ཕྱིར་བསྣུར་བ་དང་', 'མདུན་དུ་', 'བ', 'སྣུར་', 'བ་དང་', 'ལ', 'ྕ', 'གས་ཀྱུས་', 'སྒྱུར་ཐབས་', 'དང་', 'ཞགས་པ་', 'གདབ་པ', '་དང་', 'མདའ་བོ་ཆེ་འཕ', 'ེན་', 'ཐབས་དང་', 'འཛིན་སྟངས་', 'དང་', 'གོ', 'མ་', 'སྟངས་དང་', 'ཐོར་ཚུགས་', 'དང་', 'གཅད་', 'པ་དང་', 'དྲལ་བ', '་དང་', 'དབ', 'ུག་པ་', 'དང་', 'གནས་', 'ལྔ་པོ་', 'འདི་ལྟ་སྟེ་', 'རྒྱང་', 'ནས་', 'ཕོག་པ་དང་', 'སྒྲ་', 'གྲག་', 'པར་', 'ཕོག་པ་དང་', 'གནད་དུ་', 'ཕོག་པ་དང་', 'མི་འཆ', 'ོར་', 'བར་', 'ཕོག་པ་དང་', 'ཚབས་ཆེ་', 'བ་', 'དེ་དག་ལ་ཡང་', 'ཞུགས་ཤིང་', 'བྱང་བར་གྱུར་ཏོ་']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='bo.model')\n",
    "doc = load_doc(\"../data/train.bo\")\n",
    "sentences = to_sentences(doc)\n",
    "bo_token = sp.encode(sentences, out_type=str)\n",
    "save_clean_sentences_binary(bo_token, \"../data\", \"../data/tokenized.bo\")\n",
    "# spot check\n",
    "for i in range(5):\n",
    "    print(bo_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*English*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/tokenized.en\n",
      "['▁under', '▁his', '▁rule', '▁the', '▁kingdom', '▁prosper', 'ed', '▁and', '▁thrive', 'd', '▁crops', '▁were', '▁boun', 'tiful', '▁and', '▁the', '▁land', '▁teem', 'ed', '▁with', '▁animals', '▁and', '▁people']\n",
      "['▁he', '▁called', '▁up', '▁the', '▁four', '▁branch', 'es', '▁of', '▁his', '▁armed', '▁forces', 'the', '▁elephant', '▁corps', '▁the', '▁cavalry', '▁the', '▁charioteer', '▁corps', '▁and', '▁the', '▁infantry', 'and', '▁laid', '▁waste', '▁to', '▁all', '▁of', '▁magadha', '▁save', '▁rajagrha', '▁before', '▁return', 'ing']\n",
      "['▁bath', 'ed', '▁in', '▁a', '▁vast', '▁light', '▁more', '▁luminous', '▁than', '▁the', '▁glow', '▁of', '▁the', '▁gods', '▁of', '▁the', '▁thirty', 'three', '▁so', '▁great', '▁was', '▁this', '▁miraculous', '▁manifestation', '▁it', '▁was', '▁as', '▁if', '▁the', '▁sun', '▁and', '▁moon', '▁', 'shone', '▁in', '▁the', '▁', 'gulf', '▁bet', 'ween', '▁worlds', '▁so', '▁great', '▁was', '▁its', '▁strength', '▁that', '▁darkness', '▁everywhere', '▁even', '▁the', '▁pitch', 'black', '▁darkness', '▁of', '▁dark', '▁places', '▁dark', '▁from', '▁never', '▁knowing', '▁the', '▁light', '▁of', '▁the', '▁sun', '▁and', '▁moon', '▁was', '▁', 'filled', '▁with', '▁a', '▁vast', '▁light', '▁beings', '▁born', '▁in', '▁those', '▁places', '▁had', '▁never', '▁even', '▁seen', '▁so', '▁far', '▁as', '▁their', '▁out', 'stretch', 'ed', '▁hands', '▁yet', '▁by', '▁this', '▁light', '▁these', '▁beings', '▁saw', '▁one', '▁another', '▁and', '▁exclaimed', '▁you', '▁there', '▁there', '▁are', '▁others', '▁who', '▁have', '▁be', 'en', '▁born', '▁here', '▁there', '▁are', '▁others', '▁who', '▁have', '▁be', 'en', '▁born', '▁here']\n",
      "['▁was', '▁entrust', 'ed', '▁to', '▁eight', '▁nurse', 'maid', 's', '▁two', '▁to', '▁c', 'uddle', '▁him', '▁two', '▁to', '▁breastfeed', '▁him', '▁two', '▁to', '▁change', '▁his', '▁di', 'apers', '▁and', '▁two', '▁to', '▁play', '▁with', '▁him', '▁for', 'tified', '▁with', '▁milk', '▁curd', '▁butter', '▁ghee', '▁cream', '▁and', '▁other', '▁nourish', 'ing', '▁foods', '▁he', '▁grew', '▁quickly', '▁shooting', '▁up', '▁like', '▁a', '▁lotus', '▁in', '▁a', '▁pond']\n",
      "['▁he', '▁trained', '▁in', '▁and', '▁master', 'ed', '▁those', '▁art', 's', '▁and', '▁skills', '▁needed', '▁to', '▁be', '▁crown', 'ed', '▁and', '▁anoint', 'ed', '▁a', '▁ksatriya', '▁king', '▁to', '▁attain', '▁the', '▁might', '▁and', '▁dedication', '▁of', '▁a', '▁field', '▁mar', 'shal', '▁and', '▁to', '▁conquer', '▁and', '▁occupy', '▁the', '▁world', '▁riding', '▁on', '▁the', '▁neck', '▁of', '▁an', '▁elephant', '▁riding', '▁horseback', '▁charioteer', 'ing', '▁swordsman', 'ship', '▁archery', '▁advanc', 'ing', '▁yield', 'ing', '▁wield', 'ing', '▁a', '▁hook', '▁throw', 'ing', '▁a', '▁lasso', '▁cast', 'ing', '▁a', '▁spear', '▁and', '▁how', '▁to', '▁hold', '▁a', '▁weapon', '▁march', '▁tie', '▁a', '▁topknot', '▁slash', '▁quarter', '▁pierce', '▁and', '▁strike', '▁in', '▁five', '▁ways', 'strik', 'ing', '▁from', '▁a', '▁distance', '▁strik', 'ing', '▁a', '▁target', '▁us', 'ing', '▁ac', 'ous', 'tic', '▁location', '▁strik', 'ing', '▁a', '▁fatal', '▁blow', '▁strik', 'ing', '▁without', '▁hesitation', '▁and', '▁strik', 'ing', '▁forceful', 'ly']\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor(model_file='en.model')\n",
    "doc = load_doc(\"../data/train.en\")\n",
    "sentences = to_sentences(doc)\n",
    "en_token = sp.encode(sentences, out_type=str)\n",
    "save_clean_sentences_binary(en_token, \"../data\", \"../data/tokenized.en\")\n",
    "# spot check\n",
    "for i in range(5):\n",
    "    print(en_token[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
