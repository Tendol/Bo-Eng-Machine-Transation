Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_shard_count=1, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/tokenized.bo-en', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=1, source_lang='bo', srcdict=None, target_lang='en', task='translation', tensorboard_logdir=None, testpref='data/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/train', user_dir=None, validpref='data/valid', workers=20)
[bo] Dictionary: 96880 types
[bo] data/train.bo: 106872 sents, 213743 tokens, 0.0% replaced by <unk>
[bo] Dictionary: 96880 types
[bo] data/valid.bo: 90000 sents, 179999 tokens, 0.0% replaced by <unk>
[bo] Dictionary: 96880 types
[bo] data/test.bo: 1000 sents, 2000 tokens, 0.0% replaced by <unk>
[en] Dictionary: 22416 types
[en] data/train.en: 106872 sents, 1737232 tokens, 0.0% replaced by <unk>
[en] Dictionary: 22416 types
[en] data/valid.en: 90000 sents, 1442333 tokens, 0.0% replaced by <unk>
[en] Dictionary: 22416 types
[en] data/test.en: 1000 sents, 12923 tokens, 0.0% replaced by <unk>
Wrote preprocessed data to data-bin/tokenized.bo-en
