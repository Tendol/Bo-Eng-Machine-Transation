{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplistic T5 model with no fancy tricks\n",
    "\n",
    "The previous example of Chinese-English machine translation has the following problems: \n",
    "\n",
    "<ul>\n",
    "    <li>Dataset is trash</li>\n",
    "    <li>Includes too many tricks (scheduler, parameter freezing, callback, metrics) that I cannot handle</li>\n",
    "</ul>\n",
    "\n",
    "Now write a T5 Chinese-English translator with better data and no fancy trick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The entire data is too large to load directly into memory. For now, only load the first `nLine` lines.  \n",
    "\n",
    "Learn to handle big data with PyTorch dataloader if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 258 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zh</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>联盟希望俄罗斯不久批准《第二阶段裁减战略武器条约》,从而再向前迈出重要一步,并因此为就《第三...</td>\n",
       "      <td>The Union hopes that another important step fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>为了有效地评价屏蔽的效能和选择屏蔽的性能，正在研制以设计工艺为基础的计算方法。</td>\n",
       "      <td>For the purposes of efficient assessment of sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>第四章. 与外国法院和外国代表之间的合作</td>\n",
       "      <td>CHAPTER IV. COOPERATION WITH FOREIGN COURTS AN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>国际法发展中心</td>\n",
       "      <td>Center for Development of International Law</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>这一权利的行使，产生一些具体的技术问题。</td>\n",
       "      <td>The exercise of this right raises specific tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>A. 本指南所包括的交易.</td>\n",
       "      <td>A. Transactions covered by the Guide 1-2 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>这项协议是为了进一步促进两国之间的空间合作，在载人空间飞行、空间科学、以及地球观测飞行任务方...</td>\n",
       "      <td>This agreement is to facilitate further space ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>535. 委员会满意地注意到，根据1994年宪法第四十三条，在发生不论是什么性质的歧视时，都...</td>\n",
       "      <td>535. The Committee notes with satisfaction tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>不过,它打算尽快恢复这种重要的协调工作。</td>\n",
       "      <td>However, it intends to resume that important c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>联合国 A</td>\n",
       "      <td>UNITED A NATIONS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      zh  \\\n",
       "0      联盟希望俄罗斯不久批准《第二阶段裁减战略武器条约》,从而再向前迈出重要一步,并因此为就《第三...   \n",
       "1                为了有效地评价屏蔽的效能和选择屏蔽的性能，正在研制以设计工艺为基础的计算方法。   \n",
       "2                                   第四章. 与外国法院和外国代表之间的合作   \n",
       "3                                                国际法发展中心   \n",
       "4                                   这一权利的行使，产生一些具体的技术问题。   \n",
       "...                                                  ...   \n",
       "99995                                      A. 本指南所包括的交易.   \n",
       "99996  这项协议是为了进一步促进两国之间的空间合作，在载人空间飞行、空间科学、以及地球观测飞行任务方...   \n",
       "99997  535. 委员会满意地注意到，根据1994年宪法第四十三条，在发生不论是什么性质的歧视时，都...   \n",
       "99998                               不过,它打算尽快恢复这种重要的协调工作。   \n",
       "99999                                              联合国 A   \n",
       "\n",
       "                                                      en  \n",
       "0      The Union hopes that another important step fo...  \n",
       "1      For the purposes of efficient assessment of sh...  \n",
       "2      CHAPTER IV. COOPERATION WITH FOREIGN COURTS AN...  \n",
       "3            Center for Development of International Law  \n",
       "4      The exercise of this right raises specific tec...  \n",
       "...                                                  ...  \n",
       "99995         A. Transactions covered by the Guide 1-2 2  \n",
       "99996  This agreement is to facilitate further space ...  \n",
       "99997  535. The Committee notes with satisfaction tha...  \n",
       "99998  However, it intends to resume that important c...  \n",
       "99999                                   UNITED A NATIONS  \n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "enFile = open('./en-zh/UNv1.0.en-zh.en', 'r', encoding = 'utf-8')\n",
    "zhFile = open('./en-zh/UNv1.0.en-zh.zh', 'r', encoding = 'utf-8')\n",
    "\n",
    "nSkip = 0\n",
    "nLine = 100000\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "for i in range(nSkip): \n",
    "    zhFile.readline()\n",
    "    enFile.readline()\n",
    "\n",
    "for i in range(nLine): \n",
    "    zhLine = zhFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    dataMatrix.append([zhLine, enLine])\n",
    "    \n",
    "df_UN = pd.DataFrame(dataMatrix, columns = ['zh', 'en']).sample(frac=1).reset_index(drop=True) # Shuffle the data\n",
    "df_UN\n",
    "\n",
    "# Notice: The run time of appending rows in DataFrame is notoriously long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and PyTorch `Dataset`\n",
    "\n",
    "We first instantiate SentencePiece tokenizers and train them on our data. \n",
    "\n",
    "<b style=\"color:red;\">Warning!</b> For some reason I can no longer find the API for `SentencePieceBPETokenizer`. Did huggingface deprecate the old version tokenizer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to store all texts in file before training tokenizer\n",
    "pathAllZh = './en-zh/allZh.txt'\n",
    "pathAllEn = './en-zh/allEn.txt'\n",
    "\n",
    "zhTextsUN = df_UN['zh'].tolist()\n",
    "enTextsUN = df_UN['en'].tolist()\n",
    "\n",
    "with open(pathAllZh, 'w', encoding = 'utf-8') as file:\n",
    "    for line in zhTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTextsUN:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese tokenizer vocab size: 32128\n",
      "English tokenizer vocab size: 32128\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train tokenizers \n",
    "# Warning T5 tokenizer has default vocab size 32128. We should make sure the vocab size of tokenizers and T5 model match. \n",
    "\n",
    "zhTokenizer = SentencePieceBPETokenizer()\n",
    "zhTokenizer.train([pathAllZh], vocab_size = 32128, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([pathAllEn], vocab_size = 32128, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "print('Chinese tokenizer vocab size:', zhTokenizer.get_vocab_size())\n",
    "print('English tokenizer vocab size:', enTokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about tokenizer, see `Bo-Eng-Machine-Transation/warm_up_Chinese_English/01_practice_ch_en_tranlation.ipynb`. \n",
    "\n",
    "Now define PyTorch `DataLoader`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, zhTexts, enTexts, zhTokenizer, enTokenizer, zhMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.zhTexts = zhTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.zhTokenizer = zhTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.zhTokenizer.enable_padding(length = zhMaxLen)\n",
    "        self.zhTokenizer.enable_truncation(max_length = zhMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.zhTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        zhOutputs = self.zhTokenizer.encode(self.zhTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        zhEncoding = zhOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        zhMask = zhOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(zhEncoding), \n",
    "            'source_mask': torch.tensor(zhMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "Use Pytorch-lighning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.zhTokenizer = hparams['zhTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "            enTexts = enTextsUN[start_idx:end_idx], \n",
    "            zhTokenizer = self.hparams['zhTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            zhMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(zhTextsUN))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(zhTextsUN))\n",
    "        end_idx = len(zhTextsUN)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'zhTokenizer': zhTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'num_gpu': 1, \n",
    "    'weight_decay': 0, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8016661f2a804fa8bad76eb8708402b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcb3a94947f4c8893b60513ea07e1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f478ffd943ce45479e821381cb1ae0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dea9932cc724c1db2a6228c723c2dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "390a545c4c0d44779393fe7996cc7d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(6.0866, device='cuda:0'),\n",
      " 'train_loss': tensor(6.0115, device='cuda:0'),\n",
      " 'val_loss': tensor(6.0825, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 6.011471271514893,\n",
       "  'val_loss': 6.082517623901367,\n",
       "  'test_loss': 6.086567401885986}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('t5simple_' + now.strftime(\"%Y-%d-%m-%Y--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__t5simple_2020-06-12-2020--19=55=07.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese Text: 联盟希望俄罗斯不久批准《第二阶段裁减战略武器条约》,从而再向前出重要一步,并因此为就《第三阶段裁减战略武器条约》开始进行谈判开道路。\n",
      "\n",
      "Actual translation: The Union hopes that another important step forward will soon be made with the ratification of the START II Treaty by Russia, and that the way will thus be open for negotiations to begin on START III.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has not been made by the Government to be held in order to ensure that it was a more than one of the Convention on the Rights of the Child and the Convention. of the Convention on the Rights of the Child as a result of the Child. It is an important role in the field of human rights violations in the report of the Commission on Human Rights at its fifth session. session with the Committee on the Rights of the Child on the Rights of the Child on the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 为了有效地评价的效能和选择的性能,正在研制以设计工为基础的计算方法。\n",
      "\n",
      "Actual translation: For the purposes of efficient assessment of shield effectiveness and the selection of shield characteristics, calculation methods based on design engineering are being developed.\n",
      "\n",
      "Predicted translation: In this context, the Government of a number of space technology and development in order to ensure that they had been taken into account. It is not only by the United Nations. The Special Rapporteur on the situation of human rights in the field of space science and technology.”. A) and other international organizations.\" was adopted at its fifty-third sessions. such as the International Covenant on Civil and Political Rights. As regards the International Covenant on Civil and Political Rights.ai2)., which would be\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 第四章. 与外国法院和外国代表之间的合作\n",
      "\n",
      "Actual translation: CHAPTER IV. COOPERATION WITH FOREIGN COURTS AND FOREIGN REPRESENTATIVES\n",
      "\n",
      "Predicted translation: Recalling also its appreciation to the report of the Secretary-General on the implementation of the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights, and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on Civil and Political Rights and the International Covenant on\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 国际法发展中心\n",
      "\n",
      "Actual translation: Center for Development of International Law\n",
      "\n",
      "Predicted translation: United Kingdom of Great Britain and Northern Ireland 3 July 1996 1 August 1995 1 May 1993 1 April 1996 1 January 1990 1 November 1994 1 June 1994 1 September 1992 1 December 1997 - 8 May 1991 1 May 1994 1 October 1994 13 adopted 1 May 1994 1 4 February 1994 1A 1 May 1994 1/) said that the Commission on Crime Prevention and Criminal Justice THE UNITED NATIONS OF THE RIGHTS OF THE COMMISSION ON THE audiovisual AND insufficiency OF OUTER SPACE A 6 regulations 2on Parliament against Torture 24\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 这一权利的行使,产生一些具体的技术问题。\n",
      "\n",
      "Actual translation: The exercise of this right raises specific technical problems.\n",
      "\n",
      "Predicted translation: In this context, the Government of a number of space debris. It was not to be taken into account. The Commission on Narcotic Drugs.”.\" is in order to ensure that they had been made available. A and for the period under review, no longer.ai2)., with the United States of America.e) were also held at its fifty-third session..o3; it has been established.A per cent of the General Assembly. This0 per cent of the draft resolution.\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 30. 建议1999年进行下列活动:\n",
      "\n",
      "Actual translation: 30. The following activities are proposed for 1999:\n",
      "\n",
      "Predicted translation: The Working Group was informed that the Government of a number of human rights in order to promote and fundamental freedoms: it had not yet been taken into account. It should be made available: from the following persons with regard to the report of the Commission on Human Rights: the following its efforts: an important role in the field of human rights violations: the International Covenant on Civil and Political Rights: the United Nations High Commissioner for Human Rights,: the following their own Office at its fifty-third session. session.: the\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 用于制造安非他明型兴和用于制造亚二基基安非他明(\"药\")类的物质的贩运在文件CN.7/1996/12中有所讨论。\n",
      "\n",
      "Actual translation: Trafficking in substances used in the manufacture of the amphetamine-type stimulants and substances in the MDMA (\"ecstasy\") group is discussed in document CN.7/1996/12.\n",
      "\n",
      "Predicted translation: In this context, the Government replied that a number of human rights and fundamental freedoms were not to be taken into account in the field of space technology and economic development and social security. It was also expressed by the United Nations High Commissioner for Human Rights and the International Covenant on Civil and Political Rights as well as the establishment of international law enforcement agencies. The Commission on Narcotic Drugs with regard to the implementation of the Convention on the Elimination of All Forms of Racial Discrimination and the relevant Special Rapporteur on the situation of\n",
      "==================================================\n",
      "\n",
      "Chinese Text: 我们还呼吁这两个国家各派停止暴力行动并将总的国家利益置于其部落和种族利益之上。\n",
      "\n",
      "Actual translation: We also appeal to all parties in these two countries to put an end to acts of violence and to put overall national interests ahead of their tribal and ethnic interests.\n",
      "\n",
      "Predicted translation: The Committee recommends that the State party has not been made available to the Convention on the Rights of the Child in order to ensure that it is no longer had been taken into account. It should be provided for in the field of human rights and fundamental freedoms. In this context, the author or other international organizations. A”.\" with a view to the use of space debris. The Government of national level.a was held at its fifty-third session under article 9, paragraph 1, of the Covenant. As regards\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testting without help of `pl.LightningModule`\n",
    "# start_idx = int((hparams['train_percentage'] + hparams['val_percentage']) * len(zhTextsUN))\n",
    "# end_idx = len(zhTextsUN)\n",
    "start_idx = 0\n",
    "end_idx = 8\n",
    "\n",
    "testset = MyDataset(\n",
    "    zhTexts = zhTextsUN[start_idx:end_idx], \n",
    "    enTexts = enTextsUN[start_idx:end_idx], \n",
    "    zhTokenizer = hparams['zhTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    zhMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "    num_beams = 2, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    early_stopping = True\n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [zhTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Chinese Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(T5FineTuner(hparams).parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
