{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boDataPath = '../data/train.bo'\n",
    "enDataPath = '../data/train.en'\n",
    "\n",
    "boTokenizerPath = '../preProcessing/bo.model'\n",
    "enTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bo</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       bo  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                       en  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boFile = open(boDataPath, 'r', encoding = 'utf-8')\n",
    "enFile = open(enDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    boLine = boFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    if not boLine or not enLine: \n",
    "        break \n",
    "    dataMatrix.append([boLine, enLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['bo', 'en'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boTextsAll = df['bo'].tolist()\n",
    "enTextsAll = df['en'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3645, 18003, 531, 6258, 2155], [5, 3334, 0, 6082, 4, 6751, 1031, 2262, 1962, 0]]\n",
      "བྲག་སྐུ་དང་ དེའི་ཚེ་མུ་སྟེགས་ཅན་ལོངས་སྤྱོད་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers that are already trained\n",
    "boTokenizer = spm.SentencePieceProcessor(model_file=boTokenizerPath)\n",
    "enTokenizer = spm.SentencePieceProcessor(model_file=enTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(boTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(boTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', boTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(enTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(enTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(enTokenizer.decode([[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]))\n",
    "print('Vocab size of English Tokenizer:', enTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the ids for our special tokens `<s>`, `</s>`, `<pad>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 1 2 3\n"
     ]
    }
   ],
   "source": [
    "bo_bos_id = boTokenizer.piece_to_id('<s>')\n",
    "bo_eos_id = boTokenizer.piece_to_id('</s>')\n",
    "bo_pad_id = boTokenizer.piece_to_id('<pad>')\n",
    "en_bos_id = enTokenizer.piece_to_id('<s>')\n",
    "en_eos_id = enTokenizer.piece_to_id('</s>')\n",
    "en_pad_id = enTokenizer.piece_to_id('<pad>')\n",
    "\n",
    "print(bo_bos_id, bo_eos_id, bo_pad_id, en_bos_id, en_eos_id, en_pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors of tokenization must have the same length. We thus define several helper functions for truncation and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(sentvec, maxlen, enable_bos_eos, **kwargs): \n",
    "    '''\n",
    "    Truncate a sentence vector to maxlen by deleting the trailing ids. \n",
    "    Args\n",
    "    -- sentvec. List. Vector of tokenization of a sentence \n",
    "    -- maxlen. Int. The max length of tokenization. Must >=3 \n",
    "    -- pad_id. Int. The id for <pad>\n",
    "    -- enable_bos_eos. Bool. Indicate whether to wrap a sentence with <s> and </s> \n",
    "    -- kwargs['bos_id']. Int. The id for <s>\n",
    "    -- kwargs['eos_id']. Int. The id for </s> \n",
    "    '''\n",
    "    \n",
    "    # No error checking for now\n",
    "    ## For a transformer model, the target sentences have to be wrapped by <s> and </s>, but the source sentences don't have to \n",
    "    \n",
    "    if enable_bos_eos: \n",
    "        maxlen = maxlen - 2    # Need to reserve two positions for <s></s>\n",
    "        bos_id = kwargs['bos_id']\n",
    "        eos_id = kwargs['eos_id']\n",
    "        \n",
    "    # Truncate the sentence if needed \n",
    "    if len(sentvec) > maxlen: \n",
    "        newvec = sentvec[:maxlen].copy()\n",
    "    else: \n",
    "        newvec = sentvec.copy()\n",
    "        \n",
    "    # Return the new vector\n",
    "    if enable_bos_eos: \n",
    "        return [bos_id] + newvec + [eos_id]\n",
    "    else: \n",
    "        return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_get_attention_mask(sentvec, maxlen, pad_id): \n",
    "    ''' \n",
    "    Pad a sentence to maxlen and get the attention mask where 0--><pad> and 1-->non-pad characters \n",
    "    '''\n",
    "    \n",
    "    sentlen = len(sentvec)\n",
    "    \n",
    "    # No need to pad if the sentence is long enough \n",
    "    if len(sentvec) >= maxlen: \n",
    "        return sentvec, [1] * sentlen\n",
    "    \n",
    "    else: \n",
    "        return sentvec + [pad_id] * (maxlen - sentlen), [1] * sentlen + [0] * (maxlen - sentlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(sentvec, maxlen, pad_id, enable_bos_eos, **kwargs): \n",
    "    '''truncate and then pad a sentence. Return a tuple with ids and attention mask'''\n",
    "    \n",
    "    ids = truncate(sentvec, maxlen, enable_bos_eos, **kwargs)\n",
    "    ids, attention_mask = pad_and_get_attention_mask(ids, maxlen, pad_id)\n",
    "    return ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some examples to verify that our `trim()` function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400] [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], maxlen = 4, pad_id = en_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400, 500, 3, 3, 3, 3] [1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], maxlen = 9, pad_id = en_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100, 200, 2] [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], maxlen = 4, pad_id = en_pad_id, enable_bos_eos = True, bos_id = en_pad_id, eos_id = en_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100, 200, 300, 400, 500, 2, 3, 3] [1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], maxlen = 9, pad_id = en_pad_id, enable_bos_eos = True, bos_id = en_pad_id, eos_id = en_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we find out appropriate max_len for our Tibetan and English data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxBoLen: 415\n",
      "maxEnLen: 469\n",
      "Number bo-text longer than 100: 47\n",
      "Number en-text longer than 100: 167\n",
      "Number bo-text longer than 50: 268\n",
      "Number en-text longer than 70: 532\n"
     ]
    }
   ],
   "source": [
    "maxBoLen, maxEnLen = 0, 0\n",
    "boOver100, enOver100 = 0, 0 \n",
    "boOver50, enOver70 = 0, 0 \n",
    "\n",
    "for bo in boTextsAll: \n",
    "    x = boTokenizer.encode(bo)\n",
    "    if len(x) > maxBoLen:\n",
    "        maxBoLen = len(x)\n",
    "    if len(x) > 100: \n",
    "        boOver100 += 1\n",
    "    if len(x) > 50: \n",
    "        boOver50 += 1\n",
    "        \n",
    "for en in enTextsAll: \n",
    "    x = enTokenizer.encode(en)\n",
    "    if len(x) > maxEnLen:\n",
    "        maxEnLen = len(x)  \n",
    "    if len(x) > 100: \n",
    "        enOver100 += 1\n",
    "    if len(x) > 70:\n",
    "        enOver70 += 1\n",
    "        \n",
    "print('maxBoLen:', maxBoLen)\n",
    "print('maxEnLen:', maxEnLen)\n",
    "print('Number bo-text longer than 100:', boOver100)\n",
    "print('Number en-text longer than 100:', enOver100)\n",
    "print('Number bo-text longer than 50:', boOver50)\n",
    "print('Number en-text longer than 70:', enOver70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, boTexts, enTexts, boTokenizer, enTokenizer, boMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.boTexts = boTexts\n",
    "        self.enTexts = enTexts\n",
    "        self.boTokenizer = boTokenizer\n",
    "        self.enTokenizer = enTokenizer\n",
    "        self.boMaxLen = boMaxLen\n",
    "        self.enMaxLen = enMaxLen\n",
    "        \n",
    "    ''' Return the size of dataset '''\n",
    "    def __len__(self): \n",
    "        return len(self.boTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer\n",
    "        boOutputs = self.boTokenizer.encode(self.boTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Truncation and padding \n",
    "        boIds, boMask = trim(\n",
    "            boOutputs, \n",
    "            maxlen = self.boMaxLen, \n",
    "            pad_id = bo_pad_id, \n",
    "            enable_bos_eos = False\n",
    "        )\n",
    "        \n",
    "        enIds, enMask = trim(\n",
    "            enOutputs, \n",
    "            maxlen = self.enMaxLen, \n",
    "            pad_id = en_pad_id, \n",
    "            enable_bos_eos = True, \n",
    "            bos_id = en_pad_id,    # According to huggingface doc, target sequence is prepended by <pad> for T5  \n",
    "            eos_id = en_eos_id\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(boIds), \n",
    "            'source_mask': torch.tensor(boMask), \n",
    "            'target_ids': torch.tensor(enIds), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.boTokenizer = hparams['boTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        self.scheduler_is_created = False\n",
    "        self.epoch_counter = 0\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # Optimizer\n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        \n",
    "        # Scheduler\n",
    "        # To create a scheduler with linear decay, we need to manually compute the number of training steps and pass it as an argument for the schduler \n",
    "        train_size = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        batch_size = self.hparams['batch_size']\n",
    "        num_processor = max(1, self.hparams['num_gpu'])\n",
    "        num_epoch = self.hparams['num_train_epochs']\n",
    "        total_training_steps = train_size // (batch_size * num_processor) * num_epoch\n",
    "        \n",
    "        # Create a scheduler for adjusting learning rate \n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer = self.optimizer, \n",
    "            num_warmup_steps = self.hparams['warmup_steps'], \n",
    "            num_training_steps = total_training_steps\n",
    "        )\n",
    "        \n",
    "        self.lr_dict = {\n",
    "            'scheduler': self.lr_scheduler, # The LR schduler\n",
    "            'interval': 'step', # The unit of the scheduler's step size\n",
    "            'frequency': 1, # The frequency of the scheduler\n",
    "        }\n",
    "        \n",
    "        # Do constant rate this time\n",
    "        return [self.optimizer]# , [self.lr_dict]\n",
    "\n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        # For monitoring purpose, log learning rate \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr']:\n",
    "                self.log('learning_rate*e-4', param_group['lr'] * 1e4)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids']\n",
    "        # labels[labels[:, :] == en_pad_id] = -100\n",
    "        # Explanation in huggingface doc: All labels set to -100 are ignored (masked), the loss is only computed for labels in [0, ..., config.vocab_size]\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            # decoder_input_ids = batch['target_ids'], \n",
    "            decoder_attention_mask = batch['target_mask'], \n",
    "            labels = labels\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            boTexts = boTextsAll[start_idx:end_idx], \n",
    "            enTexts = enTextsAll[start_idx:end_idx], \n",
    "            boTokenizer = self.hparams['boTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            boMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        end_idx = len(boTextsAll)\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    ''' Part 6: hooks and callbacks '''\n",
    "    def on_train_epoch_end(self, outputs): \n",
    "        start_idx = 0\n",
    "        end_idx = 8\n",
    "\n",
    "        testset = MyDataset(\n",
    "            boTexts = boTextsAll[start_idx:end_idx], \n",
    "            enTexts = enTextsAll[start_idx:end_idx], \n",
    "            boTokenizer = self.hparams['boTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            boMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "\n",
    "        test_dataloader = DataLoader(testset, batch_size = self.hparams['batch_size'])\n",
    "        testit = iter(test_dataloader)\n",
    "\n",
    "        # Take one batch from testset \n",
    "        batch = next(testit)\n",
    "\n",
    "        # Generate target ids\n",
    "        outs = self.model.generate(\n",
    "            batch['source_ids'].cuda(), \n",
    "            attention_mask = batch['source_mask'].cuda(), \n",
    "            use_cache = True, \n",
    "            decoder_attention_mask = batch['target_mask'], \n",
    "            max_length = self.hparams['max_output_len'], \n",
    "            num_beams = 4, \n",
    "            repetition_penalty = 2.5, \n",
    "            length_penalty = 0.6, \n",
    "            early_stopping = True, \n",
    "        )\n",
    "        \n",
    "        pred_texts = [self.enTokenizer.decode(ids) for ids in (outs % enTokenizer.get_piece_size()).tolist()]    # temporary solution %\n",
    "        source_texts = [self.boTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "        target_texts = [self.enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "        \n",
    "        file = open(f'./sample_outputs/epoch_{self.epoch_counter:2d}.txt', 'w', encoding = 'utf-8')\n",
    "\n",
    "        for i in range(len(pred_texts)): \n",
    "            lines = textwrap.wrap(\"Tibetan Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "            file.write(\"\\n\".join(lines) + '\\n')\n",
    "            file.write((\"\\nActual translation: %s\" % target_texts[i]) + '\\n')\n",
    "            file.write((\"\\nPredicted translation: %s\" % pred_texts[i]) + '\\n')\n",
    "            file.write('=' * 50 + '\\n' * 2)\n",
    "            \n",
    "        file.close()\n",
    "        self.epoch_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'boTokenizer': boTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.95, \n",
    "    'val_percentage': 0.04, \n",
    "    'learning_rate': 1e-4, \n",
    "    'max_input_len': 50, \n",
    "    'max_output_len': 70, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 10, \n",
    "    'num_gpu': 1, \n",
    "    'weight_decay': 0, \n",
    "    'warmup_steps': 0,  # For scheduler \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('04_t5simple_bo_en_' + now.strftime(\"%Y-%m-%d--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__05_t5simple_bo_en_2020-12-16--02=06=12.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan Text: ཤཀྱའི་བུའི་དགེ་སྦྱོང་འདི་དག་ནི་མཁན་པོ་མེད་པ་དང་་སློབ་དཔོན་མེད་པས་ལེགས་པར་མ་སྦྱངས་པ་དང་\n",
      "་ལེགས་པར་མ་བགོས་པ་དང་་\n",
      "\n",
      "Actual translation: since these ascetic sons of the sakya have no preceptor and no instructor they go to the houses of brahmins and householders without being well presented or well dressed\n",
      "\n",
      "Predicted translation:  ⁇  he is the one who has been able to teach the dharma for it leads to all beings\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: སྒྲ་མཐོན་པོ་དང་་སྒྲ་ཆེན་པོ་དང་་རྒོད་བག་གི་སྤྱོད་ལམ་གྱིས་\n",
      "\n",
      "Actual translation: while speaking shrilly and in loud voices and behaving wildly\n",
      "\n",
      "Predicted translation:  ⁇  he is the one who has been able to teach the dharma for it leads to all beings\n",
      "==================================================\n",
      "\n",
      "Tibetan Text:\n",
      "བྲམ་ཟེ་དང་ཁྱིམ་བདག་གི་ཁྱིམ་དག་ཏུ་འདོང་ཞིང་དེ་དག་དེར་ཟས་སློང་བར་བྱེད་ཟས་སློང་དུ་འཇུག་པར་བྱེད་\n",
      "\n",
      "Actual translation: while there they beg for food implore others to beg for food\n",
      "\n",
      "Predicted translation:  ⁇  he is the one who has been filled with joy at the seat of awakening\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: སྲན་ཚོད་སློང་བར་བྱེད་སྲན་ཚོད་སློང་དུ་འཇུག་པར་བྱེད་དེ་\n",
      "\n",
      "Actual translation: grovel for soup and implore others to grovel for soup\n",
      "\n",
      "Predicted translation:  ⁇  he is the one who has been able to teach the dharma for it leads to cessation as related to knowledge of what is impossible\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: ཞེས་སྨོད་པར་བྱེད་ཕྱར་ཀ་གཏོང་བར་བྱེད་ཁ་ཟེར་བར་བྱེད་དོ་\n",
      "\n",
      "Actual translation: to criticize disparage and slander them they would say\n",
      "\n",
      "Predicted translation:  ⁇  when he heard this the gods in the heaven of the four great kings\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དགེ་སྦྱོང་མགོ་རེག་འདི་དག་ལ་སུ་ཞིག་བསོད་སྙོམས་སྦྱིན་པ་དང་་བྱ་བར་སེམས་\n",
      "\n",
      "Actual translation: who would give these shavenheaded ascetics alms or think to help them\n",
      "\n",
      "Predicted translation:  ⁇  when he heard the sounds of the gods in the heaven of the four great kings\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དགེ་སློང་ནད་པ་ཞིག་ཀྱང་ནད་གཡོག་དང་བྲལ་བས་ཤི་བར་གྱུར་པའི་སྐབས་དེ་\n",
      "\n",
      "Actual translation: one sick monk even died for lack of someone to nurse him\n",
      "\n",
      "Predicted translation:  ⁇  when the king had heard this he said to the blessed one\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: བཅོམ་ལྡན་འདས་ལ་དགེ་སློང་རྣམས་ཀྱིས་གསོལ་པ་དང་་བཅོམ་ལྡན་འདས་ཀྱིས་དགོངས་པ་\n",
      "\n",
      "Actual translation: when that occurred the monks asked the blessed one about it and the blessed one thought\n",
      "\n",
      "Predicted translation:  ⁇  the blessed one then spoke to the following verses to the blessed one\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_idx = 2000\n",
    "end_idx = 2008\n",
    "\n",
    "testset = MyDataset(\n",
    "    boTexts = boTextsAll[start_idx:end_idx], \n",
    "    enTexts = enTextsAll[start_idx:end_idx], \n",
    "    boTokenizer = hparams['boTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    boMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "    bos_token_id = en_bos_id,\n",
    "    eos_token_id = en_eos_id,\n",
    "    pad_token_id = en_pad_id,\n",
    "    num_beams = 4, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 0.6, \n",
    "    early_stopping = True, \n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [boTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Tibetan Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = trim(enTokenizer.encode('who would give these shavenheaded ascetics alms or think to help them'), maxlen = 100, pad_id = en_pad_id, enable_bos_eos=True, bos_id = en_bos_id, eos_id = en_eos_id)\n",
    "enTokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
