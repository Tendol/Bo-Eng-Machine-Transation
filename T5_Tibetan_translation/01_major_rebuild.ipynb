{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Config,\n",
    "    AdamW,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDataPath = '../data/train.bo'\n",
    "tgtDataPath = '../data/train.en'\n",
    "\n",
    "srcTokenizerPath = '../preProcessing/bo.model'\n",
    "tgtTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                      tgt  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcFile = open(srcDataPath, 'r', encoding = 'utf-8')\n",
    "tgtFile = open(tgtDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    srcLine = srcFile.readline().strip()\n",
    "    tgtLine = tgtFile.readline().strip()\n",
    "    if not srcLine or not tgtLine: \n",
    "        break \n",
    "    dataMatrix.append([srcLine, tgtLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['src', 'tgt'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTextsAll = df['src'].tolist()\n",
    "tgtTextsAll = df['tgt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3645, 18003, 531, 6258, 2155], [5, 3334, 0, 6082, 4, 6751, 1031, 2262, 1962, 0]]\n",
      "བྲག་སྐུ་དང་ དེའི་ཚེ་མུ་སྟེགས་ཅན་ལོངས་སྤྱོད་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers that are already trained\n",
    "srcTokenizer = spm.SentencePieceProcessor(model_file=srcTokenizerPath)\n",
    "tgtTokenizer = spm.SentencePieceProcessor(model_file=tgtTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(srcTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(srcTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', srcTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(tgtTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(tgtTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(tgtTokenizer.decode([[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]))\n",
    "print('Vocab size of English Tokenizer:', tgtTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 1 2 3\n"
     ]
    }
   ],
   "source": [
    "src_bos_id = srcTokenizer.piece_to_id('<s>')\n",
    "src_eos_id = srcTokenizer.piece_to_id('</s>')\n",
    "src_pad_id = srcTokenizer.piece_to_id('<pad>')\n",
    "tgt_bos_id = tgtTokenizer.piece_to_id('<s>')\n",
    "tgt_eos_id = tgtTokenizer.piece_to_id('</s>')\n",
    "tgt_pad_id = tgtTokenizer.piece_to_id('<pad>')\n",
    "\n",
    "print(src_bos_id, src_eos_id, src_pad_id, tgt_bos_id, tgt_eos_id, tgt_pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors of tokenization must have the same length. We thus define several helper functions for truncation and padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(sentvec, maxlen, enable_bos_eos, **kwargs): \n",
    "    '''\n",
    "    Truncate a sentence vector to maxlen by deleting the trailing ids. \n",
    "    Args\n",
    "    -- sentvec. List. Vector of tokenization of a sentence \n",
    "    -- maxlen. Int. The max length of tokenization. Must >=3 \n",
    "    -- pad_id. Int. The id for <pad>\n",
    "    -- enable_bos_eos. Bool. Indicate whether to wrap a sentence with <s> and </s> \n",
    "    -- kwargs['bos_id']. Int. The id for <s>\n",
    "    -- kwargs['eos_id']. Int. The id for </s> \n",
    "    '''\n",
    "    \n",
    "    # No error checking for now\n",
    "    ## For a transformer model, the target sentences have to be wrapped by <s> and </s>, but the source sentences don't have to \n",
    "    ## For T5 model, an ids vector is automatically shifted to the right, \n",
    "    \n",
    "    if enable_bos_eos: \n",
    "        maxlen = maxlen - 2    # Need to reserve two positions for <s></s>\n",
    "        bos_id = kwargs['bos_id']\n",
    "        eos_id = kwargs['eos_id']\n",
    "        \n",
    "    # Truncate the sentence if needed \n",
    "    if len(sentvec) > maxlen: \n",
    "        newvec = sentvec[:maxlen].copy()\n",
    "    else: \n",
    "        newvec = sentvec.copy()\n",
    "        \n",
    "    # Return the new vector\n",
    "    if enable_bos_eos: \n",
    "        return [bos_id] + newvec + [eos_id]\n",
    "    else: \n",
    "        return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_get_attention_mask(sentvec, tolen, pad_id): \n",
    "    ''' \n",
    "    If a token list is shorter than tolen, then add <pad> until `tolen` and get the attention mask where 0--><pad> and 1-->non-pad characters \n",
    "    '''\n",
    "    sentlen = len(sentvec)\n",
    "    \n",
    "    # No need to pad if the sentence is long enough \n",
    "    if len(sentvec) >= tolen: \n",
    "        return sentvec, [1] * sentlen\n",
    "    \n",
    "    else: \n",
    "        return sentvec + [pad_id] * (tolen - sentlen), [1] * sentlen + [0] * (tolen - sentlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(sentvec, tolen, pad_id, enable_bos_eos, **kwargs): \n",
    "    '''truncate and then pad a sentence. Return a tuple with ids and attention mask'''\n",
    "    \n",
    "    ids = truncate(sentvec, tolen, enable_bos_eos, **kwargs)\n",
    "    ids, attention_mask = pad_and_get_attention_mask(ids, tolen, pad_id)\n",
    "    return ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some examples to verify that our `trim()` function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400] [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 4, pad_id = tgt_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 200, 300, 400, 500, 3, 3, 3, 3] [1, 1, 1, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 9, pad_id = tgt_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100, 200, 2] [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 4, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_pad_id, eos_id = tgt_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100, 200, 300, 400, 500, 2, 3, 3] [1, 1, 1, 1, 1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 9, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_pad_id, eos_id = tgt_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch iterator\n",
    "\n",
    "Returns a batch of token ids as torch tensors upon each call of `__next__()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchIterator: \n",
    "    def __init__(self, srcTexts, tgtTexts, \n",
    "                 srcTokenizer, tgtTokenizer,\n",
    "                 start_idx, end_idx, batch_size, \n",
    "                 src_pad_id, tgt_pad_id, \n",
    "                 src_bos_id = None, tgt_bos_id = None, \n",
    "                 src_eos_id = None, tgt_eos_id = None\n",
    "                ): \n",
    "        self.srcTexts = srcTexts\n",
    "        self.tgtTexts = tgtTexts\n",
    "        self.srcTokenizer = srcTokenizer \n",
    "        self.tgtTokenizer = tgtTokenizer\n",
    "        self.start_idx = start_idx    # Starting index of original dataset, inclusive\n",
    "        self.end_idx = end_idx    # Ending index of original dataset, exclusive \n",
    "        self.batch_size = batch_size    # batch_size specified by user s\n",
    "        self.src_pad_id = src_pad_id\n",
    "        self.tgt_pad_id = tgt_pad_id\n",
    "        self.src_bos_id = src_bos_id\n",
    "        self.tgt_bos_id = tgt_bos_id \n",
    "        self.src_eos_id = src_eos_id\n",
    "        self.tgt_eos_id = tgt_eos_id \n",
    "        \n",
    "    \n",
    "    # Tokenize a list of texts and trim with special tokens\n",
    "    # Return a tuple (list of [ids], list of [masks])\n",
    "    def tokenize_batch_and_trim(self, text_batch, tokenizer, pad_id, enable_bos_eos, **kwargs):\n",
    "        ids_batch = []\n",
    "        maxlen = 0\n",
    "        res_ids, res_attention_mask = [], []\n",
    "        \n",
    "        # Add <s></s> if needed and get maxlen \n",
    "        for text in text_batch: \n",
    "            ids = tokenizer.encode(text)\n",
    "            # Add <s></s> if needed\n",
    "            ids = truncate(ids, len(ids) + 10, enable_bos_eos, **kwargs)\n",
    "            ids_batch.append(ids)\n",
    "            # Update maxlen \n",
    "            if len(ids) > maxlen: \n",
    "                maxlen = len(ids)\n",
    "        \n",
    "        # Pad to the current maxlen in the batch \n",
    "        for ids in ids_batch: \n",
    "            padded_ids, attention_mask = pad_and_get_attention_mask(ids, maxlen, pad_id)\n",
    "            res_ids.append(padded_ids)\n",
    "            res_attention_mask.append(attention_mask)\n",
    "        \n",
    "        return res_ids, res_attention_mask\n",
    "    \n",
    "    \n",
    "    def __iter__(self): \n",
    "        self.curr_idx = self.start_idx \n",
    "        return self \n",
    "    \n",
    "    \n",
    "    def __next__(self): \n",
    "        if self.curr_idx >= self.end_idx: \n",
    "            raise StopIteration \n",
    "            \n",
    "        # Take care of indices for correct iteration \n",
    "        if self.curr_idx + self.batch_size < self.end_idx: \n",
    "            head, tail = self.curr_idx, self.curr_idx + self.batch_size\n",
    "            self.curr_idx += self.batch_size\n",
    "        else:\n",
    "            head, tail = self.curr_idx, self.end_idx\n",
    "            self.curr_idx = self.end_idx \n",
    "            \n",
    "        # Get source and target texts \n",
    "        src_texts = self.srcTexts[head:tail]\n",
    "        tgt_texts = self.tgtTexts[head:tail]\n",
    "        \n",
    "        # Tokenize\n",
    "        src_ids, src_mask = self.tokenize_batch_and_trim(src_texts, self.srcTokenizer, self.src_pad_id, enable_bos_eos = False)\n",
    "        tgt_ids, tgt_mask = self.tokenize_batch_and_trim(tgt_texts, self.tgtTokenizer, self.tgt_pad_id, enable_bos_eos = True, bos_id = self.tgt_pad_id, eos_id = self.tgt_eos_id)\n",
    "        \n",
    "        # Return the results as dictionaries of torch tensors \n",
    "        return {\n",
    "            'src_ids': torch.LongTensor(src_ids).to(device),\n",
    "            'src_mask': torch.FloatTensor(src_mask).to(device),\n",
    "            'tgt_ids': torch.LongTensor(tgt_ids).to(device),\n",
    "            'tgt_mask': torch.FloatTensor(tgt_mask).to(device),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how batch iterator works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of iterator: 2\n",
      "batch index: 0, src size: torch.Size([8, 15]) = torch.Size([8, 15]); tgt size: torch.Size([8, 20]) = torch.Size([8, 20])\n",
      "sample src ids: tensor([    5, 10680,    23,  4065,    15, 20440,    23,   259,  4064,    83,\n",
      "        28083,   255,   300,     3,     3], device='cuda:0')\n",
      "sample src mask: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "       device='cuda:0')\n",
      "sample tgt ids: tensor([    3,     4,   204,  3644,    13,    10, 18671,    54,    37,    72,\n",
      "         4527,    10,  1766,   326,     2,     3,     3,     3,     3,     3],\n",
      "       device='cuda:0')\n",
      "sample tgt mask: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0.], device='cuda:0')\n",
      "==================================================\n",
      "batch index: 1, src size: torch.Size([2, 6]) = torch.Size([2, 6]); tgt size: torch.Size([2, 12]) = torch.Size([2, 12])\n",
      "sample src ids: tensor([    5, 10680,   442,  4419,     3,     3], device='cuda:0')\n",
      "sample src mask: tensor([1., 1., 1., 1., 0., 0.], device='cuda:0')\n",
      "sample tgt ids: tensor([   3,    4,  204, 3644,  104, 1429,    2,    3,    3,    3,    3,    3],\n",
      "       device='cuda:0')\n",
      "sample tgt mask: tensor([1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.], device='cuda:0')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "mbi = MyBatchIterator(\n",
    "    srcTextsAll, tgtTextsAll, \n",
    "    srcTokenizer, tgtTokenizer,\n",
    "    start_idx = 475, end_idx = 485, batch_size = 8, \n",
    "    src_pad_id = src_pad_id, tgt_pad_id = tgt_pad_id, \n",
    "    src_bos_id = src_bos_id, tgt_bos_id = tgt_bos_id, \n",
    "    src_eos_id = src_eos_id, tgt_eos_id = tgt_eos_id\n",
    ")\n",
    "\n",
    "mbi = iter(mbi)\n",
    "\n",
    "print('length of iterator:', len(mbi))\n",
    "\n",
    "for idx, batch in enumerate(mbi): \n",
    "    print(f\"batch index: {idx}, src size: {batch['src_ids'].size()} = {batch['src_mask'].size()}; tgt size: {batch['tgt_ids'].size()} = {batch['tgt_mask'].size()}\")\n",
    "    print(f\"sample src ids: {batch['src_ids'][0]}\")\n",
    "    print(f\"sample src mask: {batch['src_mask'][0]}\")\n",
    "    print(f\"sample tgt ids: {batch['tgt_ids'][0]}\")\n",
    "    print(f\"sample tgt mask: {batch['tgt_mask'][0]}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes and functions\n",
    "\n",
    "We define a `Timer` class for estimating remaining time for an epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, num_total_units):\n",
    "        # num_total_units: How many units of tasks need to be done\n",
    "        self.start = datetime.datetime.now()\n",
    "        self.num_total_units = num_total_units\n",
    "\n",
    "    def remains(self, num_done_units):\n",
    "        # num_done_units: How many units of tasks are done\n",
    "        now  = datetime.datetime.now()\n",
    "        time_taken = now - self.start\n",
    "        sec_taken = int(time_taken.total_seconds())\n",
    "        time_left = (self.num_total_units - num_done_units) * (now - self.start) / num_done_units\n",
    "        sec_left = int(time_left.total_seconds())\n",
    "        return f\"Time taken {sec_taken // 60:02d}:{sec_taken % 60:02d}, Estimated time left {sec_left // 60:02d}:{sec_left % 60:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate: model, optimizer, scheduler, hyperparameters\n",
    "\n",
    "After reading the documentation for `PretrainedConfig` and `PretrainedModel`, I got the idea that hyperparameters shall be passed as `**kwargs` when calling `from_pretrained()`. To see what hyperparameters are available to configure, \n",
    "\n",
    "```\n",
    "T5model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "T5model.config_class().to_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    num_epochs = 50, \n",
    "    train_batch_size = 8, \n",
    "    val_batch_size = 1, \n",
    "    train_percentage = 0.95, \n",
    "    checkpoint_at = [9, 19, 29, 39]   # At which intermediate epoch do we save model\n",
    "    # --------------------------------------------------\n",
    "    weight_decay = 1e-4, \n",
    "    warmup_steps = 4000, \n",
    "    dropout = 0.2,\n",
    "    target_lr = 1e-4,     # max learning rate achieved by scheduler \n",
    "    adam_betas = (0.9, 0.98), \n",
    "    # adam_eps = 1e-9, \n",
    "    max_length = 100,    # max length of sequence to be generated \n",
    ")\n",
    "\n",
    "T5model = T5ForConditionalGeneration.from_pretrained(\n",
    "    't5-small', \n",
    "    return_dict = True, \n",
    "    # bos_token_id = tgt_bos_id,    # T5 starts generation with <pad> token, so I delete this line to avoid disruption\n",
    "    eos_token_id = tgt_eos_id, \n",
    "    pad_token_id = tgt_pad_id, \n",
    "    dropout_rate = hparams['dropout'], \n",
    "    max_length = hparams['max_length']\n",
    ").to(device)\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        # parameters with weight decay \n",
    "        'params': [param for name, param in T5model.named_parameters() if ('bias' not in name and 'layer_norm.weight' not in name)], \n",
    "        'weight_decay': hparams['weight_decay'], \n",
    "    }, \n",
    "    {\n",
    "        # parameters without weight decay\n",
    "        'params': [param for name, param in T5model.named_parameters() if ('bias' in name or 'layer_norm.weight' in name)], \n",
    "        'weight_decay': 0.0, \n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters, \n",
    "    lr = hparams['target_lr'], \n",
    "    betas = hparams['adam_betas'],\n",
    ")\n",
    "\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = hparams['warmup_steps'], \n",
    "    num_training_steps = hparams['num_epochs'] * math.ceil(len(srcTextsAll) / hparams['train_batch_size']), \n",
    "    num_cycles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, val_iter, model, optimizer, scheduler, hparams): \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_step_counter = 0\n",
    "    val_step_counter = 0\n",
    "    tb_refresh_rate = 60    # Flush tensorboard log every ? sec\n",
    "    msg_refresh_rate = 10    # Flush message log every ? sec \n",
    "    save_model_every = 10    # Save model every ? epoch \n",
    "    best_epoch = 0\n",
    "    \n",
    "    msg_writer = open('message.log', 'w')    # For logging training progress \n",
    "    tb_writer = SummaryWriter(flush_secs = tb_refresh_rate)    # Tensorboard writer \n",
    "    sample_writer = open('sample.log', 'w', encoding = 'utf-8')    # For logging example sentences \n",
    "    \n",
    "    for epoch in range(hparams['num_epochs']): \n",
    "        msg_writer.write(f'Epoch {epoch}/{hparams[\"num_epochs\"]}\\n')\n",
    "        sample_writer.write(f'Epoch {epoch}/{hparams[\"num_epochs\"]}\\n')\n",
    "        msg_writer.flush() \n",
    "        \n",
    "        \n",
    "        ''' Part I: Training loop '''\n",
    "        model.train()    # Flip to train mode\n",
    "        train_loss = 0\n",
    "        \n",
    "        msg_offset = msg_writer.tell()    # Overwrite progress info at this offset \n",
    "        refresh_timer_start = time.time()    # Start counting until refreshing the message log (refresh rate = 10s)\n",
    "        myTimer = Timer(len(train_iter))    # For estimating remaining time for training\n",
    "        \n",
    "        for idx, batch in enumerate(train_iter): \n",
    "            src_ids = batch['src_ids']\n",
    "            src_mask = batch['src_mask']\n",
    "            tgt_ids = batch['tgt_ids']\n",
    "            tgt_mask = batch['tgt_mask']\n",
    "            \n",
    "            decoder_input_ids = tgt_ids[:, :-1]    # Remove the last column, intended EOS\n",
    "            labels = tgt_ids[:, 1:]    # Remove the first column (starting token)\n",
    "            \n",
    "            # Forward, backprop, optimizer, scheduler \n",
    "            optimizer.zero_grad()\n",
    "            loss = T5model.forward(\n",
    "                input_ids = src_ids, \n",
    "                attention_mask = src_mask, \n",
    "                decoder_input_ids = decoder_input_ids, \n",
    "                # decoder_attention_mask = tgt_mask,  # According to T5 doc, decoder attention mask is generated automatically so I won't define it myself. \n",
    "                labels = labels.masked_fill(labels == tgt_pad_id, -100)    # -100 means not to compute loss at this token \n",
    "            ).loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item() / src_ids.size(0)    # For computing average later \n",
    "            \n",
    "            # Tensorboard logging\n",
    "            tb_writer.add_scalar('Epoch/train', epoch, train_step_counter)\n",
    "            tb_writer.add_scalar('Loss(step)/train', loss, train_step_counter)\n",
    "            for param_group in self.optimizer.param_groups:\n",
    "                if param_group['lr']:\n",
    "                    tb_writer.add_scalar('learning_rate*e-5', param_group['lr'] * 1e5, train_step_counter)\n",
    "            train_step_counter += 1\n",
    "    \n",
    "            # Message logging \n",
    "            refresh_timer_end = time.time()\n",
    "            if (refresh_timer_end - refresh_timer_start > msg_refresh_rate): \n",
    "                refresh_timer_start = time.time()    # reset refresh_time\n",
    "                msg_writer.seek(msg_offset)\n",
    "                msg_writer.write(f'Train batches {idx}/{len(train_iter)} completed. ')\n",
    "                msg_writer.write(myTimer.remains(num_done_units = idx))\n",
    "                msg_writer.flush()\n",
    "                \n",
    "        # Training epoch end \n",
    "        msg_writer.seek(msg_offset)\n",
    "        msg_writer.write(f'Train batches {len(train_iter)}/{len(train_iter)} completed. ')\n",
    "        msg_writer.write(myTimer.remains(num_done_units = len(train_iter)))\n",
    "        msg_writer.write('\\n')\n",
    "        msg_writer.flush()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ''' Part II: Eval loop '''\n",
    "        model.eval()    # Flip to eval mode \n",
    "        val_loss = 0\n",
    "        \n",
    "        msg_offset = msg_writer.tell()    # Overwrite progress info at this offset \n",
    "        refresh_timer_start = time.time()    # Start counting until refreshing the message log (refresh rate = 10s)\n",
    "        myTimer = Timer(len(train_iter))    # For estimating remaining time for training\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            for idx, batch in enumerate(val_iter): \n",
    "                src_ids = batch['src_ids']\n",
    "                src_mask = batch['src_mask']\n",
    "                tgt_ids = batch['tgt_ids']\n",
    "                tgt_mask = batch['tgt_mask']\n",
    "\n",
    "                decoder_input_ids = tgt_ids[:, :-1]    # Remove the last column, intended EOS\n",
    "                labels = tgt_ids[:, 1:]    # Remove the first column (starting token)\n",
    "                \n",
    "                loss = T5model.forward(\n",
    "                    input_ids = src_ids, \n",
    "                    attention_mask = src_mask, \n",
    "                    decoder_input_ids = decoder_input_ids, \n",
    "                    # decoder_attention_mask = tgt_mask,  # According to T5 doc, decoder attention mask is generated automatically so I won't define it myself. \n",
    "                    labels = labels.masked_fill(labels == tgt_pad_id, -100)    # -100 means not to compute loss at this token \n",
    "                ).loss\n",
    "                val_loss += loss.item() / src_ids.size(0)    # For computing average later \n",
    "                \n",
    "                # Tensorboard logging \n",
    "                tb_writer.add_scalar('Epoch/val', epoch, val_step_counter)\n",
    "                tb_writer.add_scalar('Loss(step)/val', loss, val_step_counter)\n",
    "                val_step_counter += 1\n",
    "                \n",
    "                # Message logging \n",
    "                refresh_timer_end = time.time()\n",
    "                if (refresh_timer_end - refresh_timer_start > msg_refresh_rate): \n",
    "                    refresh_timer_start = time.time()    # reset refresh_time\n",
    "                    msg_writer.seek(msg_offset)\n",
    "                    msg_writer.write(f'Val batches {idx}/{len(train_iter)} completed. ')\n",
    "                    msg_writer.write(myTimer.remains(num_done_units = idx))\n",
    "                    msg_writer.flush()\n",
    "                    \n",
    "            # Val epoch end \n",
    "            msg_writer.seek(msg_offset)\n",
    "            msg_writer.write(f'Val batches {len(val_iter)}/{len(val_iter)} completed. ')\n",
    "            msg_writer.write(myTimer.remains(num_done_units = len(val_iter)))\n",
    "            msg_writer.write('\\n')\n",
    "            msg_writer.flush() \n",
    "            \n",
    "            \n",
    "        ## Epoch end \n",
    "\n",
    "        # Extra logs\n",
    "        print(f'Epoch {epoch}/{hparams[\"num_epochs\"]} completed. Train_loss: {train_loss / len(train_iter):.3f}. Val_loss: {val_loss / len(val_iter):.3f}')\n",
    "        msg_writer.write(f'Epoch {epoch}/{hparams[\"num_epochs\"]} completed. Train_loss: {train_loss / len(train_iter):.3f}. Val_loss: {val_loss / len(val_iter):.3f}')\n",
    "        tb_writer.add_scalar('Loss(epoch)/train', train_loss / len(train_iter), epoch)\n",
    "        tb_writer.add_scalar('Loss(epoch)/val', val_loss / len(val_iter), epoch)\n",
    "\n",
    "        # Save best model till now\n",
    "        if val_loss / len(val_iter) < min(val_losses, default = 1e9): \n",
    "            best_epoch = epoch\n",
    "            print(f'Saving best state_dict...')\n",
    "            torch.save(model.state_dict(), 'checkpoint_best_epoch.pt')\n",
    "\n",
    "        # Save checkpoint model \n",
    "        if epoch in hparams['checkpoint_at']: \n",
    "            print(f'Saving checkpoint state_dict...')\n",
    "            torch.save(model.state_dict(), f'checkpoint_epoch={epoch}.pt')\n",
    "            \n",
    "        train_losses.append(train_loss / len(train_iter))\n",
    "        val_losses.append(val_loss / len(val_iter))\n",
    "        \n",
    "        # Check sentence examples after each epoch\n",
    "        example_sent_idx = [0, 1, 2, 127, 214, 377, 277, 206]\n",
    "        for idx in example_sent_idx: \n",
    "            translated_sentence = generate_translation(model, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
