{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, \n",
    "    T5Config,\n",
    "    AdamW,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDataPath = '../data/train.bo'\n",
    "tgtDataPath = '../data/train.en'\n",
    "\n",
    "srcTokenizerPath = '../preProcessing/bo.model'\n",
    "tgtTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcFile = open(srcDataPath, 'r', encoding = 'utf-8')\n",
    "tgtFile = open(tgtDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    srcLine = srcFile.readline().strip()\n",
    "    tgtLine = tgtFile.readline().strip()\n",
    "    if not srcLine or not tgtLine: \n",
    "        break \n",
    "    dataMatrix.append([srcLine, tgtLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['src', 'tgt'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTextsAll = df['src'].tolist()\n",
    "tgtTextsAll = df['tgt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers that are already trained\n",
    "srcTokenizer = spm.SentencePieceProcessor(model_file=srcTokenizerPath)\n",
    "tgtTokenizer = spm.SentencePieceProcessor(model_file=tgtTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(srcTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(srcTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', srcTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(tgtTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(tgtTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(tgtTokenizer.decode([[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]))\n",
    "print('Vocab size of English Tokenizer:', tgtTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_bos_id = srcTokenizer.piece_to_id('<s>')\n",
    "src_eos_id = srcTokenizer.piece_to_id('</s>')\n",
    "src_pad_id = srcTokenizer.piece_to_id('<pad>')\n",
    "tgt_bos_id = tgtTokenizer.piece_to_id('<s>')\n",
    "tgt_eos_id = tgtTokenizer.piece_to_id('</s>')\n",
    "tgt_pad_id = tgtTokenizer.piece_to_id('<pad>')\n",
    "\n",
    "print(src_bos_id, src_eos_id, src_pad_id, tgt_bos_id, tgt_eos_id, tgt_pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors of tokenization must have the same length. We thus define several helper functions for truncation and padding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(sentvec, maxlen, enable_bos_eos, **kwargs): \n",
    "    '''\n",
    "    Truncate a sentence vector to maxlen by deleting the trailing ids. \n",
    "    Args\n",
    "    -- sentvec. List. Vector of tokenization of a sentence \n",
    "    -- maxlen. Int. The max length of tokenization. Must >=3 \n",
    "    -- pad_id. Int. The id for <pad>\n",
    "    -- enable_bos_eos. Bool. Indicate whether to wrap a sentence with <s> and </s> \n",
    "    -- kwargs['bos_id']. Int. The id for <s>\n",
    "    -- kwargs['eos_id']. Int. The id for </s> \n",
    "    '''\n",
    "    \n",
    "    # No error checking for now\n",
    "    ## For a transformer model, the target sentences have to be wrapped by <s> and </s>, but the source sentences don't have to \n",
    "    \n",
    "    if enable_bos_eos: \n",
    "        maxlen = maxlen - 2    # Need to reserve two positions for <s></s>\n",
    "        bos_id = kwargs['bos_id']\n",
    "        eos_id = kwargs['eos_id']\n",
    "        \n",
    "    # Truncate the sentence if needed \n",
    "    if len(sentvec) > maxlen: \n",
    "        newvec = sentvec[:maxlen].copy()\n",
    "    else: \n",
    "        newvec = sentvec.copy()\n",
    "        \n",
    "    # Return the new vector\n",
    "    if enable_bos_eos: \n",
    "        return [bos_id] + newvec + [eos_id]\n",
    "    else: \n",
    "        return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_get_attention_mask(sentvec, tolen, pad_id): \n",
    "    ''' \n",
    "    If a token list is shorter than tolen, then add <pad> until `tolen` and get the attention mask where 0--><pad> and 1-->non-pad characters \n",
    "    '''\n",
    "    sentlen = len(sentvec)\n",
    "    \n",
    "    # No need to pad if the sentence is long enough \n",
    "    if len(sentvec) >= tolen: \n",
    "        return sentvec, [1] * sentlen\n",
    "    \n",
    "    else: \n",
    "        return sentvec + [pad_id] * (tolen - sentlen), [1] * sentlen + [0] * (tolen - sentlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(sentvec, tolen, pad_id, enable_bos_eos, **kwargs): \n",
    "    '''truncate and then pad a sentence. Return a tuple with ids and attention mask'''\n",
    "    \n",
    "    ids = truncate(sentvec, tolen, enable_bos_eos, **kwargs)\n",
    "    ids, attention_mask = pad_and_get_attention_mask(ids, tolen, pad_id)\n",
    "    return ids, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some examples to verify that our `trim()` function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 4, pad_id = tgt_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 9, pad_id = tgt_pad_id, enable_bos_eos = False)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 4, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_bos_id, eos_id = tgt_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, attention_mask = trim([100, 200, 300, 400, 500], tolen = 9, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_bos_id, eos_id = tgt_eos_id)\n",
    "print(ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch iterator\n",
    "\n",
    "Returns a batch of token ids as torch tensors upon each call of `__next__()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchIterator: \n",
    "    def __init__(self, srcTexts, tgtTexts, \n",
    "                 srcTokenizer, tgtTokenizer,\n",
    "                 start_idx, end_idx, batch_size, \n",
    "                 src_pad_id, tgt_pad_id, \n",
    "                 src_bos_id = None, tgt_bos_id = None, \n",
    "                 src_eos_id = None, tgt_eos_id = None\n",
    "                ): \n",
    "        self.srcTexts = srcTexts\n",
    "        self.tgtTexts = tgtTexts\n",
    "        self.srcTokenizer = srcTokenizer \n",
    "        self.tgtTokenizer = tgtTokenizer\n",
    "        self.start_idx = start_idx    # Starting index of original dataset, inclusive\n",
    "        self.end_idx = end_idx    # Ending index of original dataset, exclusive \n",
    "        self.batch_size = batch_size    # batch_size specified by user s\n",
    "        self.src_pad_id = src_pad_id\n",
    "        self.tgt_pad_id = tgt_pad_id\n",
    "        self.src_bos_id = src_bos_id\n",
    "        self.tgt_bos_id = tgt_bos_id \n",
    "        self.src_eos_id = src_eos_id\n",
    "        self.tgt_eos_id = tgt_eos_id \n",
    "        \n",
    "    \n",
    "    # Tokenize a list of texts and trim with special tokens\n",
    "    # Return a tuple (list of [ids], list of [masks])\n",
    "    def tokenize_batch_and_trim(self, text_batch, tokenizer, pad_id, enable_bos_eos, **kwargs):\n",
    "        ids_batch = []\n",
    "        maxlen = 0\n",
    "        res_ids, res_attention_mask = [], []\n",
    "        \n",
    "        # Add <s></s> if needed and get maxlen \n",
    "        for text in text_batch: \n",
    "            ids = tokenizer.encode(text)\n",
    "            # Add <s></s> if needed\n",
    "            ids = truncate(ids, len(ids) + 10, enable_bos_eos, **kwargs)\n",
    "            ids_batch.append(ids)\n",
    "            # Update maxlen \n",
    "            if len(ids) > maxlen: \n",
    "                maxlen = len(ids)\n",
    "        \n",
    "        # Pad to the current maxlen in the batch \n",
    "        for ids in ids_batch: \n",
    "            padded_ids, attention_mask = pad_and_get_attention_mask(ids, maxlen, pad_id)\n",
    "            res_ids.append(padded_ids)\n",
    "            res_attention_mask.append(attention_mask)\n",
    "        \n",
    "        return res_ids, res_attention_mask\n",
    "    \n",
    "    \n",
    "    def __iter__(self): \n",
    "        self.curr_idx = self.start_idx \n",
    "        return self \n",
    "    \n",
    "    \n",
    "    def __next__(self): \n",
    "        if self.curr_idx >= self.end_idx: \n",
    "            raise StopIteration \n",
    "            \n",
    "        # Take care of indices for correct iteration \n",
    "        if self.curr_idx + self.batch_size < self.end_idx: \n",
    "            head, tail = self.curr_idx, self.curr_idx + self.batch_size\n",
    "            self.curr_idx += self.batch_size\n",
    "        else:\n",
    "            head, tail = self.curr_idx, self.end_idx\n",
    "            self.curr_idx = self.end_idx \n",
    "            \n",
    "        # Get source and target texts \n",
    "        src_texts = self.srcTexts[head:tail]\n",
    "        tgt_texts = self.tgtTexts[head:tail]\n",
    "        \n",
    "        # Tokenize\n",
    "        src_ids, src_mask = self.tokenize_batch_and_trim(src_texts, self.srcTokenizer, self.src_pad_id, enable_bos_eos = False)\n",
    "        tgt_ids, tgt_mask = self.tokenize_batch_and_trim(tgt_texts, self.tgtTokenizer, self.tgt_pad_id, enable_bos_eos = True, bos_id = self.tgt_bos_id, eos_id = self.tgt_eos_id)\n",
    "        \n",
    "        # Return the results as dictionaries of torch tensors \n",
    "        return {\n",
    "            'src_ids': torch.LongTensor(src_ids).to(device),\n",
    "            'src_mask': torch.FloatTensor(src_mask).to(device),\n",
    "            'tgt_ids': torch.LongTensor(tgt_ids).to(device),\n",
    "            'tgt_mask': torch.FloatTensor(tgt_mask).to(device),\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.ceil((self.end_idx - self.start_idx) / self.batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how batch iterator works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbi = MyBatchIterator(\n",
    "    srcTextsAll, tgtTextsAll, \n",
    "    srcTokenizer, tgtTokenizer,\n",
    "    start_idx = 475, end_idx = 485, batch_size = 8, \n",
    "    src_pad_id = src_pad_id, tgt_pad_id = tgt_pad_id, \n",
    "    src_bos_id = src_bos_id, tgt_bos_id = tgt_bos_id, \n",
    "    src_eos_id = src_eos_id, tgt_eos_id = tgt_eos_id\n",
    ")\n",
    "\n",
    "mbi = iter(mbi)\n",
    "\n",
    "print('length of iterator:', len(mbi))\n",
    "\n",
    "for idx, batch in enumerate(mbi): \n",
    "    print(f\"batch index: {idx}, src size: {batch['src_ids'].size()} = {batch['src_mask'].size()}; tgt size: {batch['tgt_ids'].size()} = {batch['tgt_mask'].size()}\")\n",
    "    print(f\"sample src ids: {batch['src_ids'][0]}\")\n",
    "    print(f\"sample src mask: {batch['src_mask'][0]}\")\n",
    "    print(f\"sample tgt ids: {batch['tgt_ids'][0]}\")\n",
    "    print(f\"sample tgt mask: {batch['tgt_mask'][0]}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper classes and functions\n",
    "\n",
    "We define a `Timer` class for estimating remaining time for an epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, num_total_units):\n",
    "        # num_total_units: How many units of tasks need to be done\n",
    "        self.start = datetime.datetime.now()\n",
    "        self.num_total_units = num_total_units\n",
    "\n",
    "    def remains(self, num_done_units):\n",
    "        # num_done_units: How many units of tasks are done\n",
    "        now  = datetime.datetime.now()\n",
    "        time_taken = now - self.start\n",
    "        sec_taken = int(time_taken.total_seconds())\n",
    "        time_left = (self.num_total_units - num_done_units) * (now - self.start) / num_done_units\n",
    "        sec_left = int(time_left.total_seconds())\n",
    "        return f\"Time taken {sec_taken // 60:02d}:{sec_taken % 60:02d}, Estimated time left {sec_left // 60:02d}:{sec_left % 60:02d}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate: model, optimizer, scheduler, hyperparameters\n",
    "\n",
    "After reading the documentation for `PretrainedConfig` and `PretrainedModel`, I got the idea that hyperparameters shall be passed as `**kwargs` when calling `from_pretrained()`. To see what hyperparameters are available to configure, \n",
    "\n",
    "```\n",
    "T5model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "T5model.config_class().to_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    num_epochs = 50\n",
    "    train_batch_size = 8, \n",
    "    val_batch_size = 1, \n",
    "    train_percentage = 0.95, \n",
    "    # --------------------------------------------------\n",
    "    weight_decay = 1e-4, \n",
    "    warmup_steps = 4000, \n",
    "    dropout = 0.2,\n",
    "    target_lr = 1e-4,     # max learning rate achieved by scheduler \n",
    "    adam_betas = (0.9, 0.98), \n",
    "    # adam_eps = 1e-9, \n",
    "    max_length = 100,    # max length of sequence to be generated \n",
    ")\n",
    "\n",
    "T5model = T5ForConditionalGeneration.from_pretrained(\n",
    "    't5-small', \n",
    "    return_dict = True, \n",
    "    bos_token_id = tgt_bos_id, \n",
    "    eos_token_id = tgt_eos_id, \n",
    "    pad_token_id = tgt_pad_id, \n",
    "    dropout_rate = hparams['dropout'], \n",
    "    max_length = hparams['max_length']\n",
    ")\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        # parameters with weight decay \n",
    "        'params': [param for name, param in T5model.named_parameters() if ('bias' not in name and 'layer_norm.weight' not in name)], \n",
    "        'weight_decay': self.hparams['weight_decay'], \n",
    "    }, \n",
    "    {\n",
    "        # parameters without weight decay\n",
    "        'params': [param for name, param in T5model.named_parameters() if ('bias' in name or 'layer_norm.weight' in name)], \n",
    "        'weight_decay': 0.0, \n",
    "    }\n",
    "]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters, \n",
    "    lr = hparams['target_lr'], \n",
    "    betas = hparams['adam_betas'],\n",
    ")\n",
    "\n",
    "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = hparams['warmup_steps'], \n",
    "    num_training_steps = hparams['num_epochs'] * math.ceil(len(srcTextsAll) / hparams['train_batch_size']), \n",
    "    num_cycles = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
