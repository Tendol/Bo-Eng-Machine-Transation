{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boDataForTokenizerPath = '../data/boTokenData.txt'\n",
    "enDataForTokenizerPath = '../data/enTokenData.txt'\n",
    "\n",
    "boDataPath = '../data/train.bo'\n",
    "enDataPath = '../data/train.en'\n",
    "\n",
    "boTokenizerPath = '../preProcessing/bo.model'\n",
    "enTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bo</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       bo  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                       en  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boFile = open(boDataPath, 'r', encoding = 'utf-8')\n",
    "enFile = open(enDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    boLine = boFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    if not boLine or not enLine: \n",
    "        break \n",
    "    dataMatrix.append([boLine, enLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['bo', 'en'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boTextsAll = df['bo'].tolist()\n",
    "enTextsAll = df['en'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer, but we cannot yet figure out how to truncate and pad the tokenizations to the same length, nor the special characters. Save the code but we will not use it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3644, 18002, 530, 6257, 2154], [4, 3333, 0, 6081, 3, 6750, 1030, 2261, 1961, 0]]\n",
      "ཆོས་སྟོན་ཏོ་་རང་གི་ལ་ཡོད་པའི་ ཨིན་ཡུལ་དང་ལྡན་པའི་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Ignore this cell\n",
    "'''\n",
    "\n",
    "# Load tokenizers that are already trained\n",
    "boTokenizer = spm.SentencePieceProcessor(model_file=boTokenizerPath)\n",
    "enTokenizer = spm.SentencePieceProcessor(model_file=enTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(boTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(boTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', boTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(enTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(enTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(enTokenizer.decode([[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]))\n",
    "print('Vocab size of English Tokenizer:', enTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we use huggingface tokenizer now. The bad news is we can no longer find the right API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan tokenizer vocab size: 32000\n",
      "English tokenizer vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "boTokenizer = SentencePieceBPETokenizer()\n",
    "boTokenizer.train([boDataForTokenizerPath], vocab_size = 32000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([enDataForTokenizerPath], vocab_size = 25000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "print('Tibetan tokenizer vocab size:', boTokenizer.get_vocab_size())\n",
    "print('English tokenizer vocab size:', enTokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [8557, 890, 1675, 148]\n",
      "token: ['▁བཀྲ་ཤིས་', 'བདེ་', 'ལེ', 'གས']\n",
      "mask: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for Tibetan\n",
    "outputs = boTokenizer.encode_batch(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [16272, 1226, 168, 78, 12, 84, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [16272, 1226, 168, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [636, 1345, 79]\n",
      "token: ['▁H', 'ell', 'o']\n",
      "mask: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for English\n",
    "outputs = enTokenizer.encode_batch([\"My name isn't Tenzin Dolma Gyalpo\", 'My name is Tenzin Dolma Gyalpo', 'Hello'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, boTexts, enTexts, boTokenizer, enTokenizer, boMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.boTexts = boTexts\n",
    "        self.enTexts = enTexts\n",
    "        self.boTokenizer = boTokenizer\n",
    "        self.enTokenizer = enTokenizer\n",
    "        \n",
    "        # Enable padding and truncation \n",
    "        self.boTokenizer.enable_padding(length = boMaxLen)\n",
    "        self.boTokenizer.enable_truncation(max_length = boMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    ''' Return the size of dataset '''\n",
    "    def __len__(self): \n",
    "        return len(self.boTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer\n",
    "        boOutputs = self.boTokenizer.encode(self.boTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens \n",
    "        boEncoding = boOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        boMask = boOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(boEncoding), \n",
    "            'source_mask': torch.tensor(boMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.boTokenizer = hparams['boTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        self.scheduler_is_created = False\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # Optimizer\n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        \n",
    "        # Scheduler\n",
    "        # To create a scheduler with linear decay, we need to manually compute the number of training steps and pass it as an argument for the schduler \n",
    "        train_size = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        batch_size = self.hparams['batch_size']\n",
    "        num_processor = max(1, self.hparams['num_gpu'])\n",
    "        num_epoch = self.hparams['num_train_epochs']\n",
    "        total_training_steps = train_size // (batch_size * num_processor) * num_epoch\n",
    "        \n",
    "        # Create a scheduler for adjusting learning rate \n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer = self.optimizer, \n",
    "            num_warmup_steps = self.hparams['warmup_steps'], \n",
    "            num_training_steps = total_training_steps\n",
    "        )\n",
    "        \n",
    "        self.lr_dict = {\n",
    "            'scheduler': self.lr_scheduler, # The LR schduler\n",
    "            'interval': 'step', # The unit of the scheduler's step size\n",
    "            'frequency': 1, # The frequency of the scheduler\n",
    "        }\n",
    "        \n",
    "        return [self.optimizer], [self.lr_dict]\n",
    "\n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        # For monitoring purpose, log learning rate \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr']:\n",
    "                self.log('learning_rate*e-4', param_group['lr'] * 1e4)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            boTexts = boTextsAll[start_idx:end_idx], \n",
    "            enTexts = enTextsAll[start_idx:end_idx], \n",
    "            boTokenizer = self.hparams['boTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            boMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        end_idx = len(boTextsAll)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'boTokenizer': boTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 10, \n",
    "    'num_gpu': 1, \n",
    "    'weight_decay': 0, \n",
    "    'warmup_steps': 0,  # For scheduler \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('02_t5simple_bo_en' + now.strftime(\"%Y-%m-%d--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 1.96 GiB already allocated; 8.60 MiB free; 1.98 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f8bcec65844e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodelLoaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT5FineTuner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'__02_t5simple_bo_en2020-12-10--06=13=29.ckpt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\device_dtype_mixin.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_to\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__update_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m                     \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 4.00 GiB total capacity; 1.96 GiB already allocated; 8.60 MiB free; 1.98 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__02_t5simple_bo_en2020-12-10--06=13=29.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = 0\n",
    "end_idx = 8\n",
    "\n",
    "testset = MyDataset(\n",
    "    boTexts = boTextsAll[start_idx:end_idx], \n",
    "    enTexts = enTextsAll[start_idx:end_idx], \n",
    "    boTokenizer = hparams['boTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    boMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "    num_beams = 4, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 0.6, \n",
    "    early_stopping = True, \n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [boTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Tibetan Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
