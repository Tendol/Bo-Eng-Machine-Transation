{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a transformer from scratch\n",
    "\n",
    "Link for tutorial: https://lionbridge.ai/articles/transformers-in-nlp-creating-a-translator-model-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDataPath = '../data/train.bo'\n",
    "tgtDataPath = '../data/train.en'\n",
    "\n",
    "srcTokenizerPath = '../preProcessing/bo.model'\n",
    "tgtTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      src  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                      tgt  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcFile = open(srcDataPath, 'r', encoding = 'utf-8')\n",
    "tgtFile = open(tgtDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    srcLine = srcFile.readline().strip()\n",
    "    tgtLine = tgtFile.readline().strip()\n",
    "    if not srcLine or not tgtLine: \n",
    "        break \n",
    "    dataMatrix.append([srcLine, tgtLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['src', 'tgt'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcTextsAll = df['src'].tolist()\n",
    "tgtTextsAll = df['tgt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3645, 18003, 531, 6258, 2155], [5, 3334, 0, 6082, 4, 6751, 1031, 2262, 1962, 0]]\n",
      "བྲག་སྐུ་དང་ དེའི་ཚེ་མུ་སྟེགས་ཅན་ལོངས་སྤྱོད་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers that are already trained\n",
    "srcTokenizer = spm.SentencePieceProcessor(model_file=srcTokenizerPath)\n",
    "tgtTokenizer = spm.SentencePieceProcessor(model_file=tgtTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(srcTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(srcTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(srcTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', srcTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(tgtTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(tgtTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(tgtTokenizer.decode([[8804, 181, 13, 5520, 15172, 17895], [888, 21492]]))\n",
    "print('Vocab size of English Tokenizer:', tgtTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the ids for our special tokens `<s>`, `</s>`, `<pad>`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 1 2 3\n"
     ]
    }
   ],
   "source": [
    "src_bos_id = srcTokenizer.piece_to_id('<s>')\n",
    "src_eos_id = srcTokenizer.piece_to_id('</s>')\n",
    "src_pad_id = srcTokenizer.piece_to_id('<pad>')\n",
    "tgt_bos_id = tgtTokenizer.piece_to_id('<s>')\n",
    "tgt_eos_id = tgtTokenizer.piece_to_id('</s>')\n",
    "tgt_pad_id = tgtTokenizer.piece_to_id('<pad>')\n",
    "\n",
    "print(src_bos_id, src_eos_id, src_pad_id, tgt_bos_id, tgt_eos_id, tgt_pad_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors of tokenization must have the same length. We thus define several helper functions for truncation and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(sentvec, maxlen, enable_bos_eos, **kwargs): \n",
    "    '''\n",
    "    Truncate a sentence vector to maxlen by deleting the trailing ids. \n",
    "    Args\n",
    "    -- sentvec. List. Vector of tokenization of a sentence \n",
    "    -- maxlen. Int. The max length of tokenization. Must >=3 \n",
    "    -- pad_id. Int. The id for <pad>\n",
    "    -- enable_bos_eos. Bool. Indicate whether to wrap a sentence with <s> and </s> \n",
    "    -- kwargs['bos_id']. Int. The id for <s>\n",
    "    -- kwargs['eos_id']. Int. The id for </s> \n",
    "    '''\n",
    "    \n",
    "    # No error checking for now\n",
    "    ## For a transformer model, the target sentences have to be wrapped by <s> and </s>, but the source sentences don't have to \n",
    "    \n",
    "    if enable_bos_eos: \n",
    "        maxlen = maxlen - 2    # Need to reserve two positions for <s></s>\n",
    "        bos_id = kwargs['bos_id']\n",
    "        eos_id = kwargs['eos_id']\n",
    "        \n",
    "    # Truncate the sentence if needed \n",
    "    if len(sentvec) > maxlen: \n",
    "        newvec = sentvec[:maxlen].copy()\n",
    "    else: \n",
    "        newvec = sentvec.copy()\n",
    "        \n",
    "    # Return the new vector\n",
    "    if enable_bos_eos: \n",
    "        return [bos_id] + newvec + [eos_id]\n",
    "    else: \n",
    "        return newvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sentvec, maxlen, pad_id): \n",
    "    ''' \n",
    "    Pad a sentence to maxlen \n",
    "    '''\n",
    "    sentlen = len(sentvec)\n",
    "    \n",
    "    # No need to pad if the sentence is long enough \n",
    "    if len(sentvec) >= maxlen: \n",
    "        return sentvec\n",
    "    \n",
    "    else: \n",
    "        return sentvec + [pad_id] * (maxlen - sentlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(sentvec, maxlen, pad_id, enable_bos_eos, **kwargs): \n",
    "    '''truncate and then pad a sentence. Return a tuple with ids and attention mask'''\n",
    "    \n",
    "    ids = truncate(sentvec, maxlen, enable_bos_eos, **kwargs)\n",
    "    ids= pad(ids, maxlen, pad_id)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some examples to verify that our `trim()` function works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300, 400]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim([100, 200, 300, 400, 500], maxlen = 4, pad_id = tgt_pad_id, enable_bos_eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 200, 300, 400, 500, 3, 3, 3, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim([100, 200, 300, 400, 500], maxlen = 9, pad_id = tgt_pad_id, enable_bos_eos = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100, 200, 2]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim([100, 200, 300, 400, 500], maxlen = 4, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_bos_id, eos_id = tgt_eos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 100, 200, 300, 400, 500, 2, 3, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trim([100, 200, 300, 400, 500], maxlen = 9, pad_id = tgt_pad_id, enable_bos_eos = True, bos_id = tgt_bos_id, eos_id = tgt_eos_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an iterator that returns batches of processed tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, srcTexts, tgtTexts): \n",
    "        super().__init__()\n",
    "        self.srcTexts = srcTexts\n",
    "        self.tgtTexts = tgtTexts\n",
    "        \n",
    "    ''' Return the size of dataset '''\n",
    "    def __len__(self): \n",
    "        return len(self.srcTexts)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        return {\n",
    "            'src': self.srcTexts[idx], \n",
    "            'tgt': self.tgtTexts[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatchIterator: \n",
    "    def __init__(self, srcTexts, tgtTexts, \n",
    "                 srcTokenizer, tgtTokenizer,\n",
    "                 start_idx, end_idx, batch_size, \n",
    "                 src_pad_id, tgt_pad_id, \n",
    "                 src_bos_id = None, tgt_bos_id = None, \n",
    "                 src_eos_id = None, tgt_eos_id = None\n",
    "                ): \n",
    "        self.srcTexts = srcTexts\n",
    "        self.tgtTexts = tgtTexts\n",
    "        self.srcTokenizer = srcTokenizer \n",
    "        self.tgtTokenizer = tgtTokenizer\n",
    "        self.start_idx = start_idx    # Starting index of original dataset, inclusive\n",
    "        self.end_idx = end_idx    # Ending index of original dataset, exclusive \n",
    "        self.batch_size = batch_size    # batch_size specified by user s\n",
    "        self.src_pad_id = src_pad_id\n",
    "        self.tgt_pad_id = tgt_pad_id\n",
    "        self.src_bos_id = src_bos_id\n",
    "        self.tgt_bos_id = tgt_bos_id \n",
    "        self.src_eos_id = src_eos_id\n",
    "        self.tgt_eos_id = tgt_eos_id \n",
    "        \n",
    "        \n",
    "    def get_dataloader(self): \n",
    "    # Inclusive, exclusive \n",
    "        dataset = MyDataset(\n",
    "            srcTexts = self.srcTexts[self.start_idx:self.end_idx], \n",
    "            tgtTexts = self.tgtTexts[self.start_idx:self.end_idx], \n",
    "        )\n",
    "        return DataLoader(dataset, batch_size =self.batch_size)\n",
    "    \n",
    "    \n",
    "    # Tokenize a batch of texts and trim with special tokens\n",
    "    def tokenize_batch_and_trim(self, text_batch, tokenizer, pad_id, enable_bos_eos, **kwargs):\n",
    "        ids_batch = []\n",
    "        maxlen = 0\n",
    "\n",
    "        # Iterator each text in the batch \n",
    "        for text in text_batch: \n",
    "            ids = tokenizer.encode(text)\n",
    "            # Add <s></s> if needed\n",
    "            ids = truncate(ids, len(ids) + 10, enable_bos_eos, **kwargs)\n",
    "            ids_batch.append(ids)\n",
    "            # Update maxlen \n",
    "            if len(ids) > maxlen: \n",
    "                maxlen = len(ids)\n",
    "    \n",
    "        # Pad the the current maxlen in the batch \n",
    "        padded_ids_batch = [pad(ids, maxlen, pad_id) for ids in ids_batch]\n",
    "        return torch.tensor(padded_ids_batch).to(device)\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        dl = self.get_dataloader()\n",
    "        self.it = iter(dl)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def __next__(self): \n",
    "        # Get text batch\n",
    "        text_batch_dict = next(self.it)\n",
    "        \n",
    "        # Tokenize text batch\n",
    "        return {\n",
    "            # No special token except for <pad> for source tokenization\n",
    "            'src': self.tokenize_batch_and_trim(text_batch_dict['src'], self.srcTokenizer, self.src_pad_id, enable_bos_eos = False), \n",
    "            # Add <s></s><pad> for target tokenization\n",
    "            'tgt': self.tokenize_batch_and_trim(text_batch_dict['tgt'], self.tgtTokenizer, self.tgt_pad_id, enable_bos_eos = True, bos_id = self.tgt_bos_id, eos_id = self.tgt_eos_id)\n",
    "        }\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how our batch iterator works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index: 0, src size: torch.Size([8, 12]); tgt size: torch.Size([8, 14])\n",
      "sample src ids: tensor([    5, 18950,  3221,   142,   775,   581,   423,  1853,     4,     3,\n",
      "            3,     3], device='cuda:0')\n",
      "sample tgt ids: tensor([   1,   31,    4, 1227,  355,   44,  100,    7,  429,   75,   41,   13,\n",
      "        1105,    2], device='cuda:0')\n",
      "==================================================\n",
      "batch index: 1, src size: torch.Size([6, 8]); tgt size: torch.Size([6, 15])\n",
      "sample src ids: tensor([1182,    6, 4660,    4,    3,    3,    3,    3], device='cuda:0')\n",
      "sample tgt ids: tensor([   1,  365, 2030,    8,    2,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3], device='cuda:0')\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "mbi = MyBatchIterator(\n",
    "    srcTextsAll, tgtTextsAll, \n",
    "    srcTokenizer, tgtTokenizer,\n",
    "    start_idx = 16, end_idx = 30, batch_size = 8, \n",
    "    src_pad_id = src_pad_id, tgt_pad_id = tgt_pad_id, \n",
    "    src_bos_id = src_bos_id, tgt_bos_id = tgt_bos_id, \n",
    "    src_eos_id = src_eos_id, tgt_eos_id = tgt_eos_id\n",
    ")\n",
    "\n",
    "mbi = iter(mbi)\n",
    "\n",
    "for idx, batch in enumerate(mbi): \n",
    "    print(f\"batch index: {idx}, src size: {batch['src'].size()}; tgt size: {batch['tgt'].size()}\")\n",
    "    print(f\"sample src ids: {batch['src'][0]}\")\n",
    "    print(f\"sample tgt ids: {batch['tgt'][0]}\")\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):    # What PositionalEncoding for? \n",
    "    def __init__(self, hparams): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p = hparams['dropout'])\n",
    "        self.d_model = hparams['d_model']\n",
    "        pe = torch.zeros(hparams['max_len'], self.d_model)    # What pe mean? \n",
    "        position = torch.arange(0, hparams['max_len']).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, self.d_model, 2).float() * (\n",
    "                -math.log(10000.0) / self.d_model\n",
    "            )\n",
    "        )    # What for? \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)    # even dimensions\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)    # odd dimensions\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)    # Unsqueeze turns a matrix to a 3D tensor. Transpose 0th and 1st dim? \n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = x * math.sqrt(self.d_model)    # What for\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module): \n",
    "    def __init__(self, hparams) -> None: \n",
    "        super(MyTransformer, self).__init__()\n",
    "        \n",
    "        self.source_embedding = nn.Embedding(\n",
    "            hparams['source_vocab_length'], hparams['d_model']\n",
    "        )\n",
    "        self.pos_encoder = PositionalEncoding(hparams)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            hparams['d_model'], hparams['nhead'], \n",
    "            hparams['dim_feedforward'], hparams['dropout'], \n",
    "            hparams['activation']\n",
    "        )\n",
    "        encoder_norm = nn.LayerNorm(hparams['d_model'])    # What for? \n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, hparams['num_encoder_layers'], encoder_norm\n",
    "        )\n",
    "        \n",
    "        self.target_embedding = nn.Embedding(\n",
    "            hparams['target_vocab_length'], hparams['d_model']\n",
    "        )\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            hparams['d_model'], hparams['nhead'], \n",
    "            hparams['dim_feedforward'], hparams['dropout'], \n",
    "            hparams['activation']\n",
    "        )\n",
    "        decoder_norm = nn.LayerNorm(hparams['d_model'])\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer, hparams['num_decoder_layers'], decoder_norm\n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(hparams['d_model'], hparams['target_vocab_length'])   # The original examples wrote nn.Linear(512, target_vocab_length). I suspect this is a typo as hard-coding numbers is not really cool \n",
    "        \n",
    "        self._reset_parameters()\n",
    "        self.d_model = hparams['d_model']\n",
    "        self.nhead = hparams['nhead']\n",
    "        \n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor,\n",
    "                src_mask: Optional[Tensor] = None, \n",
    "                tgt_mask: Optional[Tensor] = None, \n",
    "                memory_mask: Optional[Tensor] = None, \n",
    "                src_key_padding_mask: Optional[Tensor] = None, \n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, \n",
    "                memory_key_padding_mask: Optional[Tensor] = None\n",
    "               ) -> Tensor: \n",
    "        # Why batch size is the number of columns instead of rows? \n",
    "        if src.size(1) != tgt.size(1): \n",
    "            raise RuntimeError('The batch number of src and tgt must be equal')\n",
    "            \n",
    "        src = self.source_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.encoder(src, mask = src_mask, src_key_padding_mask = src_key_padding_mask)\n",
    "        \n",
    "        tgt = self.target_embedding(tgt)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.decoder(\n",
    "            tgt, memory, tgt_mask = tgt_mask, \n",
    "            memory_mask = memory_mask, \n",
    "            tgt_key_padding_mask = tgt_key_padding_mask, \n",
    "            memory_key_padding_mask = memory_key_padding_mask\n",
    "        )\n",
    "        output = self.out(output)\n",
    "        return output\n",
    "        \n",
    "    \n",
    "    def _reset_parameters(self): \n",
    "        r'''Initiate parameters in the transformer model'''\n",
    "        # How work? \n",
    "        for p in self.parameters(): \n",
    "            if p.dim() > 1: \n",
    "                torch.nn.init.xavier_uniform_(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(\n",
    "    d_model = 512, \n",
    "    dropout = 0.1, \n",
    "    max_len = 100, \n",
    "    nhead = 8,    # Little understand what for \n",
    "    num_encoder_layers = 6, \n",
    "    num_decoder_layers = 6, \n",
    "    dim_feedforward = 2048, \n",
    "    activation = 'relu', \n",
    "    source_vocab_length = srcTokenizer.get_piece_size(),    # Consider increase\n",
    "    target_vocab_length = tgtTokenizer.get_piece_size(),    # Consider increase \n",
    "    num_epochs = 30, \n",
    "    train_batch_size = 8, \n",
    "    val_batch_size = 1,     # For minimal padding or avoiding padding \n",
    "    lr = 1e-4, \n",
    "    adam_betas = (0.9, 0.98), \n",
    "    adam_eps = 1e-9, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyTransformer(hparams).to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = hparams['lr'], betas = hparams['adam_betas'], eps = hparams['adam_eps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the training routine\n",
    "\n",
    "The training loop and eval loop for each epoch is defined below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, val_iter, model, optim, hparams): \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_step_counter = 0\n",
    "    val_step_counter = 0\n",
    "    \n",
    "    msg_writer = open('message.log', 'w')\n",
    "    tb_writer = SummaryWriter(flush_secs=60)    # Tensorboard writer \n",
    "    \n",
    "    for epoch in range(num_epochs): \n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        # Flip to train mode \n",
    "        model.train()\n",
    "        \n",
    "        for idx, batch in enumerate(train_iter): \n",
    "            src = batch['src'].to(device)    # batch_size * maxlen(src)\n",
    "            tgt = batch['tgt'].to(device)    # batch_size * maxlen(tgt)\n",
    "            tgt_input = tgt[:, :-1]    # Rid last token. What for?  \n",
    "            targets = tgt[:, 1:].contiguous().view(-1)    # Rid first <s> token, then view as a 1D vector. What for? \n",
    "            \n",
    "            src_mask = (src != 0).float().to(device)\n",
    "            src_mask = src_mask.masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "            tgt_mask = (tgt_input != 0).float().to(device)\n",
    "            tgt_mask = tgt_mask.masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "            \n",
    "            size = tgt_input.size(1)    # size of target len with final token removed \n",
    "            np_mask = torch.triu(torch.ones(size, size) == 1).transpose(0, 1).to(device)    # This mask looks like Fig.3(b) (causal mask) in T5 paper. What np means? \n",
    "            np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "            \n",
    "            # Forward, backprop, optimizer \n",
    "            optim.zero_grad()\n",
    "            preds = model(\n",
    "                src.transpose(0, 1), \n",
    "                tgt_input.transpose(0, 1), \n",
    "                tgt_mask = np_mask, \n",
    "                # src_mask = src_mask, \n",
    "                # tgt_key_padding_mask = tgt_mask\n",
    "                # I have no idea why these two args are commented out\n",
    "            )\n",
    "            preds = preds.transpose(0, 1).contiguous().view(-1, preds.size(-1))    # Why transpose back? Then convert to 2D tensor reserving column number \n",
    "            loss = F.cross_entropy(preds, targets, ignore_index = 0, reduction = 'sum')\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item() / src.size(0)    # Tutorial uses the constant BATCH_SIZE as denominator, but since the final batch may have a smaller size, I decided to use current batch size \n",
    "            \n",
    "            # Tensorboard logging \n",
    "            tb_writer.add_scalar('Epoch/train', epoch, train_step_counter)\n",
    "            tb_writer.add_scalar('Loss/train', loss, train_step_counter)\n",
    "            train_step_counter += 1\n",
    "       \n",
    "    \n",
    "        # Flip to eval mode \n",
    "        model.eval()\n",
    "        with torch.no_grad(): \n",
    "            for idx, batch in enumerate(val_iter): \n",
    "                src = batch['src'].to(device)    # batch_size * maxlen(src)\n",
    "                tgt = batch['tgt'].to(device)    # batch_size * maxlen(tgt)\n",
    "                tgt_input = tgt[:, :-1]    # Rid last token. What for?  \n",
    "                targets = tgt[:, 1:].contiguous().view(-1)    # Rid first <s> token, then view as a 1D vector. What for? \n",
    "                \n",
    "                src_mask = (src != 0).float().to(device)\n",
    "                src_mask = src_mask.masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "                tgt_mask = (tgt_input != 0).float().to(device)\n",
    "                tgt_mask = tgt_mask.masked_fill(src_mask == 0, float('-inf')).masked_fill(src_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "\n",
    "                size = tgt_input.size(1)    # size of target len with final token removed \n",
    "                np_mask = torch.triu(torch.ones(size, size) == 1).transpose(0, 1).to(device)    # This mask looks like Fig.3(b) (causal mask) in T5 paper. What np means? \n",
    "                np_mask = np_mask.float().masked_fill(np_mask == 0, float('-inf')).masked_fill(np_mask == 1, float(0))    # map 0-->(-inf), 1-->0. What for? \n",
    "                \n",
    "                # Forward \n",
    "                preds = model(\n",
    "                    src.transpose(0, 1), \n",
    "                    tgt_input.transpose(0, 1), \n",
    "                    tgt_mask = np_mask, \n",
    "                    # src_mask = src_mask, \n",
    "                    # tgt_key_padding_mask = tgt_mask\n",
    "                    # I have no idea why these two args are commented out\n",
    "                )\n",
    "                preds = preds.transpose(0, 1).contiguous().view(-1, preds.size(-1))    # Why transpose back? Then convert to 2D tensor reserving column number \n",
    "                loss = F.cross_entropy(preds, targets, ignore_index = 0, reduction = 'sum')\n",
    "                val_loss += loss.item() / src.size(0)\n",
    "                \n",
    "                # Tensorboard logging \n",
    "                tb_writer.add_scalar('Epoch/val', epoch, val_step_counter)\n",
    "                tb_writer.add_scalar('Loss/val', loss, val_step_counter)\n",
    "                val_step_counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra functionalities I want: \n",
    "\n",
    "* Tensorboard logging\n",
    "* Showing progress in a file (epoch, batch idx out of total number of batch, time taken, expected time left)\n",
    "* Report final train_loss and val_loss at the end of epoch\n",
    "* Saving checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
