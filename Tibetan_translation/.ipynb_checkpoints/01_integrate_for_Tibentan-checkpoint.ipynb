{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boDataForTokenizerPath = '../data/boTokenData.txt'\n",
    "enDataForTokenizerPath = '../data/enTokenData.txt'\n",
    "\n",
    "boDataPath = '../data/train.bo'\n",
    "enDataPath = '../data/train.en'\n",
    "\n",
    "boTokenizerPath = '../preProcessing/bo.model'\n",
    "enTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bo</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       bo  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                       en  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boFile = open(boDataPath, 'r', encoding = 'utf-8')\n",
    "enFile = open(enDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    boLine = boFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    if not boLine or not enLine: \n",
    "        break \n",
    "    dataMatrix.append([boLine, enLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['bo', 'en'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boTextsAll = df['bo'].tolist()\n",
    "enTextsAll = df['en'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer, but we cannot yet figure out how to truncate and pad the tokenizations to the same length, nor the special characters. Save the code but we will not use it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3644, 18002, 530, 6257, 2154], [4, 3333, 0, 6081, 3, 6750, 1030, 2261, 1961, 0]]\n",
      "ཆོས་སྟོན་ཏོ་་རང་གི་ལ་ཡོད་པའི་ ཨིན་ཡུལ་དང་ལྡན་པའི་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Ignore this cell\n",
    "'''\n",
    "\n",
    "# Load tokenizers that are already trained\n",
    "boTokenizer = spm.SentencePieceProcessor(model_file=boTokenizerPath)\n",
    "enTokenizer = spm.SentencePieceProcessor(model_file=enTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(boTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(boTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', boTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(enTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(enTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(enTokenizer.decode([[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]))\n",
    "print('Vocab size of English Tokenizer:', enTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we use huggingface tokenizer now. The bad news is we can no longer find the right API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan tokenizer vocab size: 32000\n",
      "English tokenizer vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "boTokenizer = SentencePieceBPETokenizer()\n",
    "boTokenizer.train([boDataForTokenizerPath], vocab_size = 32000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([enDataForTokenizerPath], vocab_size = 25000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "print('Tibetan tokenizer vocab size:', boTokenizer.get_vocab_size())\n",
    "print('English tokenizer vocab size:', enTokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [8557, 890, 1675, 148]\n",
      "token: ['▁བཀྲ་ཤིས་', 'བདེ་', 'ལེ', 'གས']\n",
      "mask: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for Tibetan\n",
    "outputs = boTokenizer.encode_batch(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [16272, 1226, 168, 78, 12, 84, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [16272, 1226, 168, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [636, 1345, 79]\n",
      "token: ['▁H', 'ell', 'o']\n",
      "mask: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for English\n",
    "outputs = enTokenizer.encode_batch([\"My name isn't Tenzin Dolma Gyalpo\", 'My name is Tenzin Dolma Gyalpo', 'Hello'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, boTexts, enTexts, boTokenizer, enTokenizer, boMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.boTexts = boTexts\n",
    "        self.enTexts = enTexts\n",
    "        self.boTokenizer = boTokenizer\n",
    "        self.enTokenizer = enTokenizer\n",
    "        \n",
    "        # Enable padding and truncation \n",
    "        self.boTokenizer.enable_padding(length = boMaxLen)\n",
    "        self.boTokenizer.enable_truncation(max_length = boMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    ''' Return the size of dataset '''\n",
    "    def __len__(self): \n",
    "        return len(self.boTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer\n",
    "        boOutputs = self.boTokenizer.encode(self.boTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens \n",
    "        boEncoding = boOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        boMask = boOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(boEncoding), \n",
    "            'source_mask': torch.tensor(boMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.boTokenizer = hparams['boTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        self.scheduler_is_created = False\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # Optimizer\n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        \n",
    "        # Scheduler\n",
    "        # To create a scheduler with linear decay, we need to manually compute the number of training steps and pass it as an argument for the schduler \n",
    "        train_size = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        batch_size = self.hparams['batch_size']\n",
    "        num_processor = max(1, self.hparams['num_gpu'])\n",
    "        num_epoch = self.hparams['num_train_epochs']\n",
    "        total_training_steps = train_size // (batch_size * num_processor) * num_epoch\n",
    "        \n",
    "        # Create a scheduler for adjusting learning rate \n",
    "        self.lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer = self.optimizer, \n",
    "            num_warmup_steps = self.hparams['warmup_steps'], \n",
    "            num_training_steps = total_training_steps\n",
    "        )\n",
    "        \n",
    "        self.lr_dict = {\n",
    "            'scheduler': self.lr_scheduler, # The LR schduler\n",
    "            'interval': 'step', # The unit of the scheduler's step size\n",
    "            'frequency': 1, # The frequency of the scheduler\n",
    "        }\n",
    "        \n",
    "        return [self.optimizer], [self.lr_dict]\n",
    "\n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        # For monitoring purpose, log learning rate \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if param_group['lr']:\n",
    "                self.log('learning_rate*e-4', param_group['lr'] * 1e4)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            boTexts = boTextsAll[start_idx:end_idx], \n",
    "            enTexts = enTextsAll[start_idx:end_idx], \n",
    "            boTokenizer = self.hparams['boTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            boMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        end_idx = len(boTextsAll)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'boTokenizer': boTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'num_gpu': 1, \n",
    "    'weight_decay': 0, \n",
    "    'warmup_steps': 0,  # For scheduler \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa392494a30407dae7c138a1bf0e240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7ded5b6bb34302a63771130914024e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4491042aa34c73911bc3d53a12e60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc12edb139bd411babfe14dfad56a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81838df0634c4ad787b1e96b3dde73b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'learning_rate*e-4': 0.00013211203100228994,\n",
      " 'test_loss': tensor(5.7944, device='cuda:0'),\n",
      " 'train_loss': tensor(5.7353, device='cuda:0'),\n",
      " 'val_loss': tensor(5.8728, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 5.735324382781982,\n",
       "  'learning_rate*e-4': 0.00013211203100228994,\n",
       "  'val_loss': 5.8727707862854,\n",
       "  'test_loss': 5.794442176818848}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('01_t5simple_bo_en' + now.strftime(\"%Y-%m-%d--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__01_t5simple_bo_en_2020-12-08--15=14=53.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan Text: རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ་བ་ལོ་ལེགས་པ་སྐྱེ་བོ་དང་མི་མང་པོས་གང་བ་བ\n",
      "ྱེད་དུ་བཅུག་གོ་\n",
      "\n",
      "Actual translation: under his rule the kingdom prospered and thrived crops were bountiful and the land teemed with animals and people\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means prosperity and wealth to be free from their minds are not as well he will become all beings who have no miraculous is like this dharma teaching that they do so on up with his head for them or it would you see him into your mind i am any phenomena by an end at death there were my own body through being has been heard me when those four qualities may we go forth take birth about such what did bodhisattvas possess ten away how could perfect conduct\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོགས་དང་རྟ་པའི་ཚོགས་དང་ཤིང་རྟ་པའི་ཚོགས་དང་ད\n",
      "པུང་བུ་ཆུང་གི་ཚོགས་གོ་བསྐོན་ཏེ་ཡུལ་མ་ག་དཧའི་རྒྱལ་པོའི་ཁ་བ་མ་གཏོགས་པ་བཅོམ་ནས་ཕྱིར་ལྡོག་པར་བྱེད་དོ་\n",
      "\n",
      "Actual translation: he called up the four branches of his armed forcesthe elephant corps the cavalry the charioteer corps and the infantryand laid waste to all of magadha save rajagrha before returning\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means and not to be like an end for all beings who are free from their minds will become also them with this dharma teaching that is it would have no longer path as well or you should know i am any other than him on his body by those four qualities they do so forth he had been being which has gone at death there were my own life through me when we go into nirvana but may things up such after buddhas what time did bodhisattvas possess ten\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས་ལྷག་པའི་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་འ\n",
      "ཇིག་རྟེན་གྱི་འཇིག་རྟེན་གྱི་བར་གང་ན་ཉི་མ་དང་ཟླ་བ་འདི་ལྟར་རྫུ་འཕྲུལ་ཆེ་བ་འདི་ལྟར་མཐུ་ཆེ་བ་འདི་གཉིས་ཀྱི\n",
      "་འོད་དག་ཉམས་སུ་མི་མྱོང་བའི་མུན་པ་མུན་ནག་མུན་པར་བྱེད་པས་གནག་པར་གྱུར་པ་གང་དག་ཡིན་པ་དེ་དག་ཀྱང་དེའི་ཚེ་ན\n",
      "་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་ནས་སེམས་ཅན་གང་དག་དེར་སྐྱེས་པ་དག་གིས་རང་གི་ལག་པ་བརྐྱང་བ་ཡང་མི་མཐོང་བ་ད\n",
      "ེ་དག་གིས་ཀྱང་འོད་དེས་སེམས་ཅན་གཅིག་གིས་གཅིག་མཐོང་ནསཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་་ཤེས་ལྡན་ད\n",
      "ག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་ཞེས་ཤེས་པར་གྱུར་ཏོ་\n",
      "\n",
      "Actual translation: bathed in a vast light more luminous than the glow of the gods of the thirtythree so great was this miraculous manifestation it was as if the sun and moon shone in the gulf between worlds so great was its strength that darkness everywhere even the pitchblack darkness of dark places dark from never knowing the light of the sun and moon was filled with a vast light beings born in those places had never even seen so far as their outstretched hands yet by this light these beings saw one another and exclaimed you there there are\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means you should not be like this is it to go forth and so on up for all phenomena are without any buddha realms as well being with their minds will become also them from his own body that they have no miraculous faith or by an end into your majesty he had been qualities i am me about him through my mind but who has come at enlightenment there were those beings may we attain these two conduct when bodhisattvas do then know what does ones such which would\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུན་པའི་མ་མ་གཉིས་དང་དྲི་མ་འཕྱི་བའི་མ་མ་གཉི\n",
      "ས་དང་རྩེ་འགྲོགས་ཀྱི་མ་མ་གཉིས་ལ་རྗེས་སུ་གཏད་དོ་་དེ་མ་མ་བརྒྱད་པོ་དག་གིས་འོ་མ་དང་ཞོ་དང་མར་དང་ཞུན་མར་དང་\n",
      "མར་གྱི་སྙིང་ཁུ་དང་གཞན་ཡང་ཡོ་བྱད་ཀྱི་བྱེ་བྲག་གཙོ་བོ་གཙོ་བོ་དག་གིས་བསྲིངས་པར་བྱེད་སྐྱེད་པར་བྱེད་ཅིང་རྫ\n",
      "ིང་ན་གནས་པའི་པདྨ་བཞིན་དུ་སྐྱེད་པར་བྱེད་དོ་\n",
      "\n",
      "Actual translation: was entrusted to eight nursemaids two to cuddle him two to breastfeed him two to change his diapers and two to play with him fortified with milk curd butter ghee cream and other nourishing foods he grew quickly shooting up like a lotus in a pond\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means you should not be like this is it to go forth and then asked him with their minds are free from all beings for those who have been heard that i shall become them by his parents or will also attain being without any buddha realm he had ones as wellok they do so on up into an end through which has no longer there were my body but at death time when we saw me about what did bodhisattvas see these two sentient tirthika may know if\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀྱི་དབང་ཕྱུག་མཐུ་དང་བརྩོན་འགྲུས་ཐོབ་པ་ས་ཆ\n",
      "ེན་པོའི་དཀྱིལ་འཁོར་མངོན་པར་རྒྱས་པར་བྱས་ཏེ་གནས་པ་རྣམས་ཀྱི་བཟོའི་གནས་དང་ལས་ཀྱི་གནས་ཐ་དད་པར་གྱུར་པ་གང་ད\n",
      "ག་ཡིན་པ་འདི་ལྟ་སྟེ་གླང་པོ་ཆེའི་གཉར་ཞོན་པ་དང་རྟ་ལ་ཞོན་པ་དང་ཤིང་རྟའི་ཐབས་དང་རལ་གྲིའི་ཐབས་དང་འཕོང་དང་ཕྱ\n",
      "ིར་བསྣུར་བ་དང་མདུན་དུ་བསྣུར་བ་དང་ལྕགས་ཀྱུས་སྒྱུར་ཐབས་དང་ཞགས་པ་གདབ་པ་དང་མདའ་བོ་ཆེ་འཕེན་ཐབས་དང་འཛིན་སྟ\n",
      "ངས་དང་གོམ་སྟངས་དང་ཐོར་ཚུགས་དང་གཅད་པ་དང་དྲལ་བ་དང་དབུག་པ་དང་གནས་ལྔ་པོ་འདི་ལྟ་སྟེ་རྒྱང་ནས་ཕོག་པ་དང་སྒྲ་\n",
      "གྲག་པར་ཕོག་པ་དང་གནད་དུ་ཕོག་པ་དང་མི་འཆོར་བར་ཕོག་པ་དང་ཚབས་ཆེ་བ་དེ་དག་ལ་ཡང་ཞུགས་ཤིང་བྱང་བར་གྱུར་ཏོ་\n",
      "\n",
      "Actual translation: he trained in and mastered those arts and skills needed to be crowned and anointed a ksatriya king to attain the might and dedication of a field marshal and to conquer and occupy the world riding on the neck of an elephant riding horseback charioteering swordsmanship archery advancing yielding wielding a hook throwing a lasso casting a spear and how to hold a weapon march tie a topknot slash quarter pierce and strike in five waysstriking from a distance striking a target using acoustic location striking a fatal blow striking\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means and not to be like this is it for all beings who are free from their minds will become also them with his parents or that they have no miraculous wisdom as well being on my own body by those four qualities he had been an end into him but i am you should know me about such what do bodhisattvas come up at any time there were which has ones through your majesty so forth then just when bodhisattva mahasattvas we shall go she without if someone merit\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དེ་ཡང་རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ཡིན་ལ་\n",
      "\n",
      "Actual translation: he too has been crowned and anointed a ksatriya king\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means prosperity and wealth to be reborn as well he will not become all phenomena are like an end for him with their minds or that they have no miraculous is it into this dharma discourse from his seat at death i am you who had been being by them on my own body through which has gone forth so without any other than those beings should know me about your mind but there were also path away such actions do bodhisattvas come may we go she see what does\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: ལག་པ་འགྲམ་པ་ལ་བསྟད་ནས་སེམས་ཁོང་དུ་ཆུད་ཅིང་འདུག་འདུག་ནས་\n",
      "\n",
      "Actual translation: head in hands he sat and sat absorbed in thought\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means you should not be like this is it to go forth and so on up for all phenomena are without any buddha realms as well being with their minds will become also them from his own mind that i am an end into him or he had been at death they have no longer there were those who wish reality me by what do bodhisattvas come may we know my body through which has gone beings then just such six sense supreme samadhi thousand years nor doesk neither\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: ཡུལ་དང་སྲོག་ལ་གནོད་གྱུར་ན་་སྐྱེས་བུས་ཀུན་ཏུ་སྲོག་བསྲུང་བྱ་་བློ་ཡིས་གཉིས་ཀ་དཔྱད་བྱས་ན་་\n",
      "ཡུལ་ནི་ཡང་རྙེད་སྲོག་རྣམས་མིན་\n",
      "\n",
      "Actual translation: when land and life are threatened seek always to protect life when the wise look at both they see land but not life can be found again\n",
      "\n",
      "Predicted translation: the blessed one was in a way of great means and you should not be like this is it to go forth as well or that they are also with their minds will become all beings for them from his own mind he had been being by those who have no miraculous wisdom on my body through an end into him but i am me about did so many such qualities there were any other than what do bodhisattvas come up at enlightenment when time she saw these two without free eat here may we desire away birth pure\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_idx = 0\n",
    "end_idx = 8\n",
    "\n",
    "testset = MyDataset(\n",
    "    boTexts = boTextsAll[start_idx:end_idx], \n",
    "    enTexts = enTextsAll[start_idx:end_idx], \n",
    "    boTokenizer = hparams['boTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    boMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "    num_beams = 1, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    early_stopping = True, \n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [boTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Tibetan Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972, 3731,\n",
       "          135, 1282,  147,  153,  774,  261,  276, 1378,  202,  200,  175,  666,\n",
       "          176,  230,  621,  216,  298,  219,  260,  272, 1677,  168,  415,  213,\n",
       "          316,  717,  171,  184,  256,  296,  161,  352,  185,  263, 1041,  191,\n",
       "          318,  249,  205,  561,  207,  526,  378,  510,  524,  407,  208,  541,\n",
       "          451,  644,  257,  215,  693,  237, 1011,  275,  334,  455,  990,  690,\n",
       "          488,  224,  410,  687,  909,  337,  336,  304,  552,  629,  490,  311,\n",
       "          365,  590, 1014,  908,  625,  457,  360,  725,  420,  733,  697,  867,\n",
       "          434,  916,  301,  797],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  135,\n",
       "          200,  147,  153,  415,  215,  693,  191,  216,  298,  219,  202,  774,\n",
       "          261,  276, 1378,  230,  621,  496,  318,  185,  213,  316,  717,  171,\n",
       "          168,  205,  561,  260,  272, 2464,  642,  175,  666,  249,  207,  355,\n",
       "          262,  208,  541,  451,  459, 1024,  378,  161,  263,  690,  257,  304,\n",
       "          552,  629,  184,  256,  296,  590,  176,  404,  687,  224,  471,  410,\n",
       "         1598,  237, 1011,  275,  334,  455,  990,  861,  488,  337,  336,  311,\n",
       "          365,  510, 1766,  522,  490,  933,  352,  457,  648,  546,  360,  470,\n",
       "          725,  420,  733,  697],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  207,\n",
       "          355,  200,  153,  415,  213,  168,  205,  147,  365,  590,  135,  296,\n",
       "          161,  352,  191,  216,  644,  202,  520,  451,  348, 1033,  175,  666,\n",
       "          224,  185,  276, 1378,  230,  621,  496,  318,  261,  263,  990,  690,\n",
       "          171,  184,  260,  272, 1677, 1402,  249,  257,  215,  693,  510,  524,\n",
       "         3834,  176,  404,  687,  629,  208,  541,  337,  625,  378,  488,  455,\n",
       "          407,  522,  219,  410,  772,  237,  650,  275,  334,  304,  298,  490,\n",
       "          311,  494,  414,  780,  797,  336,  420,  256,  342,  262,  360,  683,\n",
       "          567,  457,  471,  561],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  207,\n",
       "          355,  200,  153,  415,  213,  168,  205,  147,  365,  590,  135,  342,\n",
       "          794,  378,  185,  276, 1378,  202,  774,  261,  216,  298,  191,  304,\n",
       "          219,  260,  687,  909,  171,  208,  938,  621,  318,  257,  263, 2101,\n",
       "          249,  230,  496,  494,  224,  520,  451,  348,  718,  176,  404,  567,\n",
       "          175,  666,  506,  184,  256,  296,  161,  352,  510,  215,  693,  488,\n",
       "          471,  410,  272, 2464,  275,  334,  455,  690,  522,  237, 1011,  470,\n",
       "          336,  311, 1109,  337,  625,  360,  725,  420,  526,  414,  780,  655,\n",
       "         7096,  490,  262,  431],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  135,\n",
       "          200,  147,  153,  415,  213,  168,  205,  191,  216,  298,  219,  202,\n",
       "          774,  261,  276, 1378,  230,  621,  496,  318,  185,  263, 2101,  249,\n",
       "          171,  184,  260,  272, 1677,  453,  175,  666,  224,  161,  455,  990,\n",
       "          690,  257,  304,  552,  629,  176,  404,  687,  215,  693,  510,  378,\n",
       "          522,  208,  541,  207,  355,  262,  337,  625,  457,  360,  256,  420,\n",
       "          772,  352,  237,  451,  470,  275,  334,  471,  410,  567,  488,  524,\n",
       "         3834,  296,  590,  342,  651,  336,  317, 2223,  311,  938,  365,  832,\n",
       "          520,  431, 1509,  883],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972, 3731,\n",
       "          135, 1282,  147,  153, 1930,  175,  666,  176,  230,  200,  621,  216,\n",
       "          644,  202,  415,  215,  693,  191,  378,  185,  276, 1378,  249,  171,\n",
       "          184,  260,  272, 1677,  168,  205,  510,  213,  316, 3121,  261,  263,\n",
       "         1137,  237, 1011,  208,  541,  207,  219,  404,  687,  224,  257,  318,\n",
       "          161,  455,  990,  690,  488,  471,  410, 1598,  590,  296,  520,  451,\n",
       "          459, 1024,  304,  298,  355,  262,  337,  625,  524,  407,  522,  275,\n",
       "          334,  496,  642,  867,  457, 1186,  256,  420,  772,  490,  311,  365,\n",
       "          832,  526,  360,  683],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  207,\n",
       "          355,  200,  153,  415,  213,  168,  205,  147,  365,  590,  135,  296,\n",
       "          161,  352,  191,  216,  644,  202,  520,  451,  348, 1033,  175,  666,\n",
       "          224,  185,  276, 1378,  230,  621,  496,  318,  261,  263,  990,  407,\n",
       "          171,  208,  541,  215,  693,  510,  378,  249,  176,  404,  687,  237,\n",
       "         1011,  184,  260,  272, 2464,  275,  334,  304,  219,  726, 2035,  337,\n",
       "          257,  360,  256,  420,  772,  490,  311,  262,  455,  690,  488,  471,\n",
       "          410, 1598,  298,  342,  651,  457,  847, 1252,  731, 1358,  583, 1740,\n",
       "          600,  683,   75,  981],\n",
       "        [   0,  114,  285,  193,  250,  145,  113,  478,  129,  291,  972,  135,\n",
       "          207,  355,  200,  153,  415,  213,  168,  205,  147,  365,  590,  175,\n",
       "          666,  249,  171,  184,  202,  496,  185,  276, 1378,  230,  621,  216,\n",
       "          298,  191,  318,  261,  263,  990,  407,  176,  404,  687,  224,  257,\n",
       "          304,  219,  260,  272, 1677,  453,  161,  455,  690,  488,  215,  693,\n",
       "          510,  378,  522,  208,  541,  337,  625,  725,  296,  585,  457,  629,\n",
       "          275,  334,  451,  459, 1024,  360,  256,  420,  772,  352,  237,  650,\n",
       "          336,  470,  832, 1109,  414,  780,  520,  774, 2347, 1082,  490,  311,\n",
       "          884,  867,  908,  877]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
