{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SparseAdam\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "from datetime import datetime\n",
    "import textwrap\n",
    "\n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boDataForTokenizerPath = '../data/boTokenData.txt'\n",
    "enDataForTokenizerPath = '../data/enTokenData.txt'\n",
    "\n",
    "boDataPath = '../data/train.bo'\n",
    "enDataPath = '../data/train.en'\n",
    "\n",
    "boTokenizerPath = '../preProcessing/bo.model'\n",
    "enTokenizerPath = '../preProcessing/en.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bo</th>\n",
       "      <th>en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...</td>\n",
       "      <td>under his rule the kingdom prospered and thriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...</td>\n",
       "      <td>he called up the four branches of his armed fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...</td>\n",
       "      <td>bathed in a vast light more luminous than the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...</td>\n",
       "      <td>was entrusted to eight nursemaids two to cuddl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...</td>\n",
       "      <td>he trained in and mastered those arts and skil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106861</th>\n",
       "      <td>མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...</td>\n",
       "      <td>maudgalyayana the thusgone worthy perfect budd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106862</th>\n",
       "      <td>བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...</td>\n",
       "      <td>when the blessed one had spoken venerable maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106863</th>\n",
       "      <td>འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...</td>\n",
       "      <td>this completes the great vehicle sutra the pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106864</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...</td>\n",
       "      <td>this was translated by the indian preceptor pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106865</th>\n",
       "      <td>རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...</td>\n",
       "      <td>the text was later edited and finalized by the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       bo  \\\n",
       "0       རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ...   \n",
       "1       དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོག...   \n",
       "2       སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས...   \n",
       "3       མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུ...   \n",
       "4       རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀ...   \n",
       "...                                                   ...   \n",
       "106861  མད་གལ་གྱི་བུ་དེ་བཞིན་གཤེགས་པ་དགྲ་བཅོམ་པ་ཡང་དག་...   \n",
       "106862  བཅོམ་ལྡན་འདས་ཀྱིས་དེ་སྐད་ཅེས་བཀའ་སྩལ་པ་དང་་ཚེ་...   \n",
       "106863  འཕགས་པ་བཅོམ་ལྡན་འདས་ཀྱི་ཡེ་ཤེས་རྒྱས་པའི་མདོ་སྡ...   \n",
       "106864  རྒྱ་གར་གྱི་མཁན་པོ་པྲཛྙ་བར་མ་དང་་ལོཙྪ་བ་བན་དེ་ཡ...   \n",
       "106865  རྒྱ་གར་གྱི་མཁན་པོ་བི་ཤུད་དྷ་སིང་ཧ་དང་་སརྦ་ཛྙ་ད...   \n",
       "\n",
       "                                                       en  \n",
       "0       under his rule the kingdom prospered and thriv...  \n",
       "1       he called up the four branches of his armed fo...  \n",
       "2       bathed in a vast light more luminous than the ...  \n",
       "3       was entrusted to eight nursemaids two to cuddl...  \n",
       "4       he trained in and mastered those arts and skil...  \n",
       "...                                                   ...  \n",
       "106861  maudgalyayana the thusgone worthy perfect budd...  \n",
       "106862  when the blessed one had spoken venerable maha...  \n",
       "106863  this completes the great vehicle sutra the pre...  \n",
       "106864  this was translated by the indian preceptor pr...  \n",
       "106865  the text was later edited and finalized by the...  \n",
       "\n",
       "[106866 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boFile = open(boDataPath, 'r', encoding = 'utf-8')\n",
    "enFile = open(enDataPath, 'r', encoding = 'utf-8')\n",
    "\n",
    "dataMatrix = []\n",
    "\n",
    "while True: \n",
    "    boLine = boFile.readline().strip()\n",
    "    enLine = enFile.readline().strip()\n",
    "    if not boLine or not enLine: \n",
    "        break \n",
    "    dataMatrix.append([boLine, enLine])\n",
    "  \n",
    "# Create pandas dataframe \n",
    "df = pd.DataFrame(dataMatrix, columns = ['bo', 'en'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boTextsAll = df['bo'].tolist()\n",
    "enTextsAll = df['en'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers for Tibetan and English\n",
    "\n",
    "The code cell below uses Google SentencePiece tokenizer, but we cannot yet figure out how to truncate and pad the tokenizations to the same length, nor the special characters. Save the code but we will not use it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁ངའི་', 'མིང་ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']]\n",
      "[[3644, 18002, 530, 6257, 2154], [4, 3333, 0, 6081, 3, 6750, 1030, 2261, 1961, 0]]\n",
      "ཆོས་སྟོན་ཏོ་་རང་གི་ལ་ཡོད་པའི་ ཨིན་ཡུལ་དང་ལྡན་པའི་\n",
      "ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་\n",
      "Vocab size of Tibetan Tokenizer: 32000\n",
      "[['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dolma', '▁Gyalpo']]\n",
      "[[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]\n",
      "['My name is Tenzin Dolma Gyalpo', 'Hello']\n",
      "Vocab size of English Tokenizer: 25000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "## Ignore this cell\n",
    "'''\n",
    "\n",
    "# Load tokenizers that are already trained\n",
    "boTokenizer = spm.SentencePieceProcessor(model_file=boTokenizerPath)\n",
    "enTokenizer = spm.SentencePieceProcessor(model_file=enTokenizerPath)\n",
    "\n",
    "# Verify for Tibetan\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་'], out_type=str))\n",
    "print(boTokenizer.encode(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'], out_type=int))\n",
    "print(boTokenizer.decode([4149, 306, 6, 245, 4660, 748]))\n",
    "print(boTokenizer.decode(['▁ངའི་', 'མིང་', 'ལ་', 'བསྟན་', 'སྒྲོལ་མ་', 'ཟེར་']))\n",
    "print('Vocab size of Tibetan Tokenizer:', boTokenizer.get_piece_size())\n",
    "\n",
    "# Verify for English\n",
    "print(enTokenizer.encode([\"My name isn't Tenzin Dolma Gyalpo\"], out_type=str))\n",
    "print(enTokenizer.encode(['My name is Tenzin Dolma Gyalpo', 'Hello'], out_type=int))\n",
    "print(enTokenizer.decode([[8803, 180, 12, 5519, 15171, 17894], [887, 21491]]))\n",
    "print('Vocab size of English Tokenizer:', enTokenizer.get_piece_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we use huggingface tokenizer now. The bad news is we can no longer find the right API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan tokenizer vocab size: 32000\n",
      "English tokenizer vocab size: 25000\n"
     ]
    }
   ],
   "source": [
    "boTokenizer = SentencePieceBPETokenizer()\n",
    "boTokenizer.train([boDataForTokenizerPath], vocab_size = 32000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "enTokenizer.train([enDataForTokenizerPath], vocab_size = 25000, special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "print('Tibetan tokenizer vocab size:', boTokenizer.get_vocab_size())\n",
    "print('English tokenizer vocab size:', enTokenizer.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [5608, 816, 9506, 3748, 17603]\n",
      "token: ['▁ངའི་', 'མིང་', 'ལ་བསྟན་', 'སྒྲོལ་', 'མ་ཟེར་']\n",
      "mask: [1, 1, 1, 1, 1]\n",
      "ids: [8557, 890, 1675, 148]\n",
      "token: ['▁བཀྲ་ཤིས་', 'བདེ་', 'ལེ', 'གས']\n",
      "mask: [1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for Tibetan\n",
    "outputs = boTokenizer.encode_batch(['ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'ངའི་མིང་ལ་བསྟན་སྒྲོལ་མ་ཟེར་', 'བཀ྄ྲ་ཤིས་བདེ་ལེགས།'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: [16272, 1226, 168, 78, 12, 84, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', 'n', \"'\", 't', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [16272, 1226, 168, 13164, 22220, 225, 9256, 3135]\n",
      "token: ['▁My', '▁name', '▁is', '▁Tenzin', '▁Dol', 'ma', '▁Gyal', 'po']\n",
      "mask: [1, 1, 1, 1, 1, 1, 1, 1]\n",
      "ids: [636, 1345, 79]\n",
      "token: ['▁H', 'ell', 'o']\n",
      "mask: [1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Verify for English\n",
    "outputs = enTokenizer.encode_batch([\"My name isn't Tenzin Dolma Gyalpo\", 'My name is Tenzin Dolma Gyalpo', 'Hello'])\n",
    "\n",
    "for output in outputs: \n",
    "    print('ids:', output.ids)\n",
    "    print('token:', output.tokens)\n",
    "    print('mask:', output.attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset): \n",
    "    def __init__(self, boTexts, enTexts, boTokenizer, enTokenizer, boMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.boTexts = boTexts\n",
    "        self.enTexts = enTexts\n",
    "        self.boTokenizer = boTokenizer\n",
    "        self.enTokenizer = enTokenizer\n",
    "        \n",
    "        # Enable padding and truncation \n",
    "        self.boTokenizer.enable_padding(length = boMaxLen)\n",
    "        self.boTokenizer.enable_truncation(max_length = boMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    ''' Return the size of dataset '''\n",
    "    def __len__(self): \n",
    "        return len(self.boTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer\n",
    "        boOutputs = self.boTokenizer.encode(self.boTexts[idx])\n",
    "        enOutputs = self.enTokenizer.encode(self.enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens \n",
    "        boEncoding = boOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        boMask = boOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(boEncoding), \n",
    "            'source_mask': torch.tensor(boMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.boTokenizer = hparams['boTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        self.hparams = hparams\n",
    "        \n",
    "        \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, input_ids, attention_mask = None, decoder_input_ids = None, decoder_attention_mask = None, labels = None):  \n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            labels = labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Configure optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        # I have no idea why to configure parameter this way \n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # parameter with weight decay \n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' not in name and 'LayerNorm.weight' not in name)], \n",
    "                'weight_decay': self.hparams['weight_decay'], \n",
    "            }, \n",
    "            {\n",
    "                'params': [param for name, param in model.named_parameters() if ('bias' in name or 'LayerNorm.weight' in name)], \n",
    "                'weight_decay': 0.0, \n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.hparams['learning_rate'])\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    ''' Part 4.1: Training logic '''\n",
    "    def training_step(self, batch, batch_idx):         \n",
    "        loss = self._step(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss \n",
    "    \n",
    "    \n",
    "    def _step(self, batch): \n",
    "        labels = batch['target_ids'] \n",
    "        labels[labels[:, ] == 0] = -100    # Change the pad id from 0 to -100, but I do not know why the example chooses to do so. I will comment it out for now\n",
    "        \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'], \n",
    "            attention_mask = batch['source_mask'], \n",
    "            labels = labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss\n",
    "\n",
    "    \n",
    "    ''' Part 4.2: Validation logic '''\n",
    "    def validation_step(self, batch, batch_idx):        \n",
    "        loss = self._step(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        \n",
    "    ''' Part 4.3: Test logic '''\n",
    "    def test_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    \n",
    "    ''' Part 5: Data loaders '''\n",
    "    def _get_dataloader(self, start_idx, end_idx): \n",
    "        dataset = MyDataset(\n",
    "            boTexts = boTextsAll[start_idx:end_idx], \n",
    "            enTexts = enTextsAll[start_idx:end_idx], \n",
    "            boTokenizer = self.hparams['boTokenizer'], \n",
    "            enTokenizer = self.hparams['enTokenizer'], \n",
    "            boMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(dataset, batch_size = hparams['batch_size'])\n",
    "    \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        start_idx = 0\n",
    "        end_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "        \n",
    "    \n",
    "    def val_dataloader(self): \n",
    "        start_idx = int(self.hparams['train_percentage'] * len(boTextsAll))\n",
    "        end_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        return self._get_dataloader(start_idx, end_idx)\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self): \n",
    "        start_idx = int((self.hparams['train_percentage'] + self.hparams['val_percentage']) * len(boTextsAll))\n",
    "        end_idx = len(boTextsAll)\n",
    "        return self._get_dataloader(start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'boTokenizer': boTokenizer,\n",
    "    'enTokenizer': enTokenizer,\n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'train_percentage': 0.85, \n",
    "    'val_percentage': 0.13, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'num_gpu': 1, \n",
    "    'weight_decay': 0, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e70706e3c0433e9cee38fd999ffbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5578e14860d3418c895c32b0a412daa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e47a3c3a8ae43b7b76c31a18d3d05b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39bdba3cc7d4bbca5816fe014af5787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c4de9ba31b4edfa81f113eb2a9ae94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': tensor(5.6364, device='cuda:0'),\n",
      " 'train_loss': tensor(5.1073, device='cuda:0'),\n",
      " 'val_loss': tensor(5.6757, device='cuda:0')}\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'train_loss': 5.107315540313721,\n",
       "  'val_loss': 5.675739288330078,\n",
       "  'test_loss': 5.636357307434082}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_params = dict(\n",
    "    gpus = hparams['num_gpu'], \n",
    "    max_epochs = hparams['num_train_epochs'], \n",
    "    progress_bar_refresh_rate = 20, \n",
    ")\n",
    "\n",
    "model = T5FineTuner(hparams)\n",
    "\n",
    "trainer = pl.Trainer(**train_params)\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "# Save model for later use\n",
    "now = datetime.now()\n",
    "trainer.save_checkpoint('01_t5simple_bo_en' + now.strftime(\"%Y-%d-%m-%Y--%H=%M=%S\") + '.ckpt')\n",
    "\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previously saved model\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path='__01_t5simple_bo_en_2020-07-12-2020--15=37=10.ckpt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tibetan Text: རྒྱལ་པོ་ཞེས་བྱ་བས་རྒྱལ་སྲིད་འབྱོར་པ་རྒྱས་པ་བདེ་བ་ལོ་ལེགས་པ་སྐྱེ་བོ་དང་མི་མང་པོས་གང་བ་བ\n",
      "ྱེད་དུ་བཅུག་གོ་\n",
      "\n",
      "Actual translation: under his rule the kingdom prospered and thrived crops were bountiful and the land teemed with animals and people\n",
      "\n",
      "Predicted translation: when the blessed one was in order to see the bodhisattva great beings who are born in this world with its gods and humans that is a gateway to the light of the dharma for the sake of awakening you will not be able to go forth as well as he has been taught by the dharma teaching of the dharma from his seat at the time of his death and so on up the thusgone one the perfect buddha known as an end to the four meditative states and the lord of the dharma or the three worlds\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དེས་དཔུང་གི་ཚོགས་ཡན་ལག་བཞི་པ་གླང་པོ་ཆེ་པའི་ཚོགས་དང་རྟ་པའི་ཚོགས་དང་ཤིང་རྟ་པའི་ཚོགས་དང་ད\n",
      "པུང་བུ་ཆུང་གི་ཚོགས་གོ་བསྐོན་ཏེ་ཡུལ་མ་ག་དཧའི་རྒྱལ་པོའི་ཁ་བ་མ་གཏོགས་པ་བཅོམ་ནས་ཕྱིར་ལྡོག་པར་བྱེད་དོ་\n",
      "\n",
      "Actual translation: he called up the four branches of his armed forcesthe elephant corps the cavalry the charioteer corps and the infantryand laid waste to all of magadha save rajagrha before returning\n",
      "\n",
      "Predicted translation: he is a gateway to the four great kings of all sentient beings who are born in the world with its gods and humans will not be able to see the blessed one as well as you have been liberated by the thusgone one for the sake of awakening and so forth from this dharma teaching of the realm of phenomena that i am an end to him like a tree or a human being at the time of his death they do not come into the path of the ten virtuous actions and their minds are free from\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: སུམ་ཅུ་རྩ་གསུམ་པའི་ལྷ་རྣམས་ཀྱི་ཁ་དོག་གི་མཐུ་བས་ལྷག་པའི་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་འ\n",
      "ཇིག་རྟེན་གྱི་འཇིག་རྟེན་གྱི་བར་གང་ན་ཉི་མ་དང་ཟླ་བ་འདི་ལྟར་རྫུ་འཕྲུལ་ཆེ་བ་འདི་ལྟར་མཐུ་ཆེ་བ་འདི་གཉིས་ཀྱི\n",
      "་འོད་དག་ཉམས་སུ་མི་མྱོང་བའི་མུན་པ་མུན་ནག་མུན་པར་བྱེད་པས་གནག་པར་གྱུར་པ་གང་དག་ཡིན་པ་དེ་དག་ཀྱང་དེའི་ཚེ་ན\n",
      "་སྣང་བ་རྒྱ་ཆེན་པོས་ཁྱབ་པར་གྱུར་ནས་སེམས་ཅན་གང་དག་དེར་སྐྱེས་པ་དག་གིས་རང་གི་ལག་པ་བརྐྱང་བ་ཡང་མི་མཐོང་བ་ད\n",
      "ེ་དག་གིས་ཀྱང་འོད་དེས་སེམས་ཅན་གཅིག་གིས་གཅིག་མཐོང་ནསཤེས་ལྡན་དག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་་ཤེས་ལྡན་ད\n",
      "ག་སེམས་ཅན་གཞན་ཡང་འདིར་སྐྱེས་སོ་ཞེས་ཤེས་པར་གྱུར་ཏོ་\n",
      "\n",
      "Actual translation: bathed in a vast light more luminous than the glow of the gods of the thirtythree so great was this miraculous manifestation it was as if the sun and moon shone in the gulf between worlds so great was its strength that darkness everywhere even the pitchblack darkness of dark places dark from never knowing the light of the sun and moon was filled with a vast light beings born in those places had never even seen so far as their outstretched hands yet by this light these beings saw one another and exclaimed you there there are\n",
      "\n",
      "Predicted translation: when the blessed one was in order to see the bodhisattva great beings who are born in this world with its gods and humans that is a gateway to the light of the dharma for the sake of all sentient beings as well as he has been taught by the realm of awakening will not be able to go forth from his seat at the time of his head to the thusgone ones feet and i am an end to him like a lotus in a human being or a hundred thousand years they have become a buddha realms\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: མ་མ་བརྒྱད་པོ་པང་ན་འཚོ་བའི་མ་མ་གཉིས་དང་ནུ་མ་སྣུན་པའི་མ་མ་གཉིས་དང་དྲི་མ་འཕྱི་བའི་མ་མ་གཉི\n",
      "ས་དང་རྩེ་འགྲོགས་ཀྱི་མ་མ་གཉིས་ལ་རྗེས་སུ་གཏད་དོ་་དེ་མ་མ་བརྒྱད་པོ་དག་གིས་འོ་མ་དང་ཞོ་དང་མར་དང་ཞུན་མར་དང་\n",
      "མར་གྱི་སྙིང་ཁུ་དང་གཞན་ཡང་ཡོ་བྱད་ཀྱི་བྱེ་བྲག་གཙོ་བོ་གཙོ་བོ་དག་གིས་བསྲིངས་པར་བྱེད་སྐྱེད་པར་བྱེད་ཅིང་རྫ\n",
      "ིང་ན་གནས་པའི་པདྨ་བཞིན་དུ་སྐྱེད་པར་བྱེད་དོ་\n",
      "\n",
      "Actual translation: was entrusted to eight nursemaids two to cuddle him two to breastfeed him two to change his diapers and two to play with him fortified with milk curd butter ghee cream and other nourishing foods he grew quickly shooting up like a lotus in a pond\n",
      "\n",
      "Predicted translation: he is a gateway to the gods of the dharma for the sake of all sentient beings who are born in the world with its gods and humans will not be able to see the blessed one as well as you have been liberated by the four great kings like an end to him from the realm of phenomena and so on his head to the thusgone ones feet and i will become a human being that is a gateway to the power of the ten powers of the three worlds at the seven jewels of the noble ones\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ལྗོངས་ཀྱི་དབང་ཕྱུག་མཐུ་དང་བརྩོན་འགྲུས་ཐོབ་པ་ས་ཆ\n",
      "ེན་པོའི་དཀྱིལ་འཁོར་མངོན་པར་རྒྱས་པར་བྱས་ཏེ་གནས་པ་རྣམས་ཀྱི་བཟོའི་གནས་དང་ལས་ཀྱི་གནས་ཐ་དད་པར་གྱུར་པ་གང་ད\n",
      "ག་ཡིན་པ་འདི་ལྟ་སྟེ་གླང་པོ་ཆེའི་གཉར་ཞོན་པ་དང་རྟ་ལ་ཞོན་པ་དང་ཤིང་རྟའི་ཐབས་དང་རལ་གྲིའི་ཐབས་དང་འཕོང་དང་ཕྱ\n",
      "ིར་བསྣུར་བ་དང་མདུན་དུ་བསྣུར་བ་དང་ལྕགས་ཀྱུས་སྒྱུར་ཐབས་དང་ཞགས་པ་གདབ་པ་དང་མདའ་བོ་ཆེ་འཕེན་ཐབས་དང་འཛིན་སྟ\n",
      "ངས་དང་གོམ་སྟངས་དང་ཐོར་ཚུགས་དང་གཅད་པ་དང་དྲལ་བ་དང་དབུག་པ་དང་གནས་ལྔ་པོ་འདི་ལྟ་སྟེ་རྒྱང་ནས་ཕོག་པ་དང་སྒྲ་\n",
      "གྲག་པར་ཕོག་པ་དང་གནད་དུ་ཕོག་པ་དང་མི་འཆོར་བར་ཕོག་པ་དང་ཚབས་ཆེ་བ་དེ་དག་ལ་ཡང་ཞུགས་ཤིང་བྱང་བར་གྱུར་ཏོ་\n",
      "\n",
      "Actual translation: he trained in and mastered those arts and skills needed to be crowned and anointed a ksatriya king to attain the might and dedication of a field marshal and to conquer and occupy the world riding on the neck of an elephant riding horseback charioteering swordsmanship archery advancing yielding wielding a hook throwing a lasso casting a spear and how to hold a weapon march tie a topknot slash quarter pierce and strike in five waysstriking from a distance striking a target using acoustic location striking a fatal blow striking\n",
      "\n",
      "Predicted translation: he is a gateway to the gods of the dharma for the sake of all sentient beings who are born in this world with its gods and humans will not be able to see the blessed one as well as they have been liberated by the four great kings like an end to him from his seat at the feet of the thusgone one it was on the three worlds that i am your mind and no longer even if you were to make offerings to the words of the ten powers of the seven precious jewels of the\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: དེ་ཡང་རྒྱལ་པོ་རྒྱལ་རིགས་སྤྱི་བོར་དབང་བསྐུར་བ་ཡིན་ལ་\n",
      "\n",
      "Actual translation: he too has been crowned and anointed a ksatriya king\n",
      "\n",
      "Predicted translation: the one who is a gateway to the gods of the dharma in the world with its gods and humans will be able to go forth as well as he has been taught by the dharma for the sake of awakening and so on his head to the blessed one and then said to him at the time of his feet and sat before him to listen to the dharma teaching of the dharma from the realm of the dharma and vinaya of the dharma that i am born in the three worlds there lived a certain king of\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: ལག་པ་འགྲམ་པ་ལ་བསྟད་ནས་སེམས་ཁོང་དུ་ཆུད་ཅིང་འདུག་འདུག་ནས་\n",
      "\n",
      "Actual translation: head in hands he sat and sat absorbed in thought\n",
      "\n",
      "Predicted translation: he is a gateway to the gods of the dharma for the sake of all sentient beings who are born in this world with its gods and humans will be able to see the blessed one as well as the thusgone one has been taught by means of the dharma that i am an end to you at the time of his feet and so forth from the realm of perfect and complete awakening they have no longer become a buddha realms or a great being like a hundred thousand years through the three worlds there lived a certain\n",
      "==================================================\n",
      "\n",
      "Tibetan Text: ཡུལ་དང་སྲོག་ལ་གནོད་གྱུར་ན་་སྐྱེས་བུས་ཀུན་ཏུ་སྲོག་བསྲུང་བྱ་་བློ་ཡིས་གཉིས་ཀ་དཔྱད་བྱས་ན་་\n",
      "ཡུལ་ནི་ཡང་རྙེད་སྲོག་རྣམས་མིན་\n",
      "\n",
      "Actual translation: when land and life are threatened seek always to protect life when the wise look at both they see land but not life can be found again\n",
      "\n",
      "Predicted translation: he is a gateway to the gods of the dharma for the sake of all sentient beings who are born in the world with its gods and humans will not be able to see the blessed one as well as you have been liberated by the four great kings like an end to him from the realm of phenomena and so on his right knee that i am your mind at the time of his feet and no longer even if they were to go forth in the three worlds there lived a certain householder or a human being\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_idx = 0\n",
    "end_idx = 8\n",
    "\n",
    "testset = MyDataset(\n",
    "    boTexts = boTextsAll[start_idx:end_idx], \n",
    "    enTexts = enTextsAll[start_idx:end_idx], \n",
    "    boTokenizer = hparams['boTokenizer'], \n",
    "    enTokenizer = hparams['enTokenizer'], \n",
    "    boMaxLen = hparams['max_input_len'], \n",
    "    enMaxLen = hparams['max_output_len']\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(testset, batch_size = hparams['batch_size'])\n",
    "testit = iter(test_dataloader)\n",
    "\n",
    "# Take one batch from testset \n",
    "batch = next(testit)\n",
    "\n",
    "# Generate target ids\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask = batch['source_mask'].cuda(), \n",
    "    use_cache = True, \n",
    "    decoder_attention_mask = batch['target_mask'], \n",
    "    max_length = hparams['max_output_len'], \n",
    "    num_beams = 5, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    # early_stopping = True, \n",
    ")\n",
    "\n",
    "pred_texts = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "source_texts = [boTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "target_texts = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(len(pred_texts)): \n",
    "    lines = textwrap.wrap(\"Tibetan Text:\\n%s\\n\" % source_texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % target_texts[i])\n",
    "    print(\"\\nPredicted translation: %s\" % pred_texts[i])\n",
    "    print('=' * 50 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
