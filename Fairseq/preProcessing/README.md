#   NEURAL MACHINE TRANSLATION - PREPROCESSING

## Python dependencies for  preprocessing and tokenization
pip install sentencepience

## Files/Folder brief description
data_orig contains the cleaned parallel corpus for Tibetan and English.

## The following instruction can be used to prepare the data for traning, validation, and testing, and tokenize the data.

bash prepare-bo-en.sh