{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice round: Chinese-English translation\n",
    "\n",
    "Huggingface transformer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Huggingface tokenizer doc: https://huggingface.co/transformers/\n",
    "\n",
    "Useful resources from huggingface -- fine-tuning a model from scratch: https://huggingface.co/blog/how-to-train\n",
    "\n",
    "The code I wrote before might be helpful: https://github.com/submal/ctec-lambus/blob/master/xprmt/xprmt_06.ipynb\n",
    "\n",
    "A code example of fine-tuning T5 for text summarization: https://towardsdatascience.com/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81 \n",
    "\n",
    "LighningModule API\n",
    "https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-apihttps://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html#lightningmodule-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    T5Model, \n",
    "    T5ForConditionalGeneration, \n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import nlp\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Enable GPU if possible \n",
    "device = torch.device(\n",
    "    'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    ")\n",
    "print(f'device = {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3536490363662881</td>\n",
       "      <td>你唯一应该努力超越的人，就是昨天的自己</td>\n",
       "      <td>YesThe only person you should try to be better...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>10613096526</td>\n",
       "      <td>诸事顺遂却不说是走运，只是在骗自己。</td>\n",
       "      <td>Those who have succeeded at anything and don’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>3471250242574624</td>\n",
       "      <td>海明威说过，这世界很美好，值得我们为之奋斗。我同意后半句。——《七宗罪</td>\n",
       "      <td>Hemingway once wrote，The world is a fine place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>3542673203850352</td>\n",
       "      <td>车站，回家的起点! 无论这里上演过多少苦辣酸甜，它一直都是人们希望</td>\n",
       "      <td>Railway stations: starting point of going home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>3498400785985006</td>\n",
       "      <td>你知道就算大雨让这座城市颠倒，我也会给你怀抱。</td>\n",
       "      <td>You know that even if the downpour turned the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                               source  \\\n",
       "1998  3536490363662881                  你唯一应该努力超越的人，就是昨天的自己   \n",
       "1999       10613096526                   诸事顺遂却不说是走运，只是在骗自己。   \n",
       "2000  3471250242574624  海明威说过，这世界很美好，值得我们为之奋斗。我同意后半句。——《七宗罪   \n",
       "2001  3542673203850352    车站，回家的起点! 无论这里上演过多少苦辣酸甜，它一直都是人们希望   \n",
       "2002  3498400785985006              你知道就算大雨让这座城市颠倒，我也会给你怀抱。   \n",
       "\n",
       "                                                 target  \n",
       "1998  YesThe only person you should try to be better...  \n",
       "1999  Those who have succeeded at anything and don’t...  \n",
       "2000  Hemingway once wrote，The world is a fine place...  \n",
       "2001     Railway stations: starting point of going home  \n",
       "2002  You know that even if the downpour turned the ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./cn_en_weibo_data/data.cn-en.json', 'r', encoding = 'utf-8') as myfile:\n",
    "    raw = myfile.read().split('\\n')  \n",
    "\n",
    "# Turn raw strings into a list of dictionaries\n",
    "weiboDict = [json.loads(line) for line in raw]\n",
    "\n",
    "# Load and shuffle data\n",
    "weiboDf= pd.DataFrame(weiboDict).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "weiboDf.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the data is far from clean. However, for prototyping purpose, we will not focus too much on cleaning right now. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and tokenizing Chinese texts\n",
    "\n",
    "We use `jieba` library (结巴分词) for parsing Chinese text. For more information, see https://github.com/fxsjy/jieba/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\presu\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.553 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不要', '只', '追求', '漂亮', '的', '外表', '，', '它会', '蒙蔽', '你', '的', '眼睛', '；', ' ', '不要', '只', '追求', '财富', '，', '它', '只不过', '是', '过眼烟云', '；', '追求', '一个', '能', '让', '你', '真心', '微笑', '的', '人', '，', ' ', '只有', '笑容', '能', '使', '黑夜', '不再', '漫长', '，', '驱走', '阴霾', '，', '带来', '温暖', '阳光']\n"
     ]
    }
   ],
   "source": [
    "chTexts = weiboDf['source']\n",
    "enTexts = weiboDf['target']\n",
    "\n",
    "# Tokenize all Chinese texts in the dataframe and store as a list\n",
    "chTokensGen = [jieba.cut(sentence) for sentence in chTexts]\n",
    "\n",
    "# Output a sample tokenization\n",
    "print(list(chTokensGen[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out with tokenizers based on `sentencePiece`, the tokenization happens at sentence level, and the tokenizer is trained recognize subwords. Therefore we will not use other parsers for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathAllCh = './cn_en_weibo_data/allCh.txt'\n",
    "pathAllEn = './cn_en_weibo_data/allEn.txt'\n",
    "\n",
    "# Store all Chinese text in a single file \n",
    "with open(pathAllCh, 'w', encoding = 'utf-8') as file: \n",
    "    for line in chTexts:\n",
    "        file.write(line + '\\n')\n",
    "    file.close()\n",
    "    \n",
    "# Store all English text in a single file \n",
    "with open(pathAllEn, 'w', encoding = 'utf-8') as file: \n",
    "    for line in enTexts: \n",
    "        file.write(line + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My feeling is that we cannot use a pretrained tokenizer to train it from scratch. Instead, we might need to import Byte-Pair Encoding, or WordPiece, or SentencePiece by scratch. \n",
    "\n",
    "https://huggingface.co/transformers/tokenizer_summary.html#sentencepiece\n",
    "\n",
    "https://github.com/huggingface/tokenizers\n",
    "\n",
    "<a href=\"https://huggingface.co/docs/tokenizers/python/latest/\">Huggingface tokenizer doc</a>\n",
    "\n",
    "In the following cell, we train a `SentencePiece` tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7099, 2582, 3837, 30, 4117, 394, 1188, 1858, 7389, 1402, 1010, 5757, 2229, 6240, 1828, 1899, 5751, 1002, 355, 1651, 3659, 4244, 15, 1670, 1680, 1351] ['▁不要只追求漂亮的外表', ',它会', '你的眼睛', ';', '▁不要只追求', '富', ',它', '只不过', '是过眼云;', '追求', '一个', '能让你', '真心', '微笑的人,', '▁只有', '笑容', '能使', '黑', '夜', '不再', '漫长', ',走', ',', '带来', '温暖', '阳光'] [(0, 10), (10, 13), (13, 17), (17, 18), (20, 26), (26, 27), (27, 29), (29, 32), (32, 37), (36, 39), (39, 41), (41, 44), (44, 46), (46, 51), (53, 56), (56, 58), (58, 60), (60, 61), (61, 62), (62, 64), (64, 66), (66, 68), (68, 69), (68, 71), (70, 73), (73, 75)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.\\\\myTokenizer-vocab.json', '.\\\\myTokenizer-merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "chTokenizer.train([pathAllCh], \n",
    "                vocab_size = 20000, \n",
    "                special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = chTokenizer.encode(chTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "chTokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the option to encode a list of texts as a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁不要只追求漂亮的外表', ',它会', '你的眼睛', ';', '▁不要只追求', '富', ',它', '只不过', '是过眼云;', '追求', '一个', '能让你', '真心', '微笑的人,', '▁只有', '笑容', '能使', '黑', '夜', '不再', '漫长', ',走', ',', '带来', '温暖', '阳光']\n",
      "['▁\"', '▁——', '我希望找到这样一个人,即使', '我', '微笑着', '说“', '我还', '好”', '的时候', ',他', '也能', '觉得', '到', '我的', '痛苦', '。']\n",
      "['▁生活的', '地', '平', '线', '是', '随着', '心的', '开', '而变', '广', '的。']\n"
     ]
    }
   ],
   "source": [
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: padding and truncation\n",
    "\n",
    "Huggingface tokenizer allows us to pad or truncate according to a length. The following are common utilities for padding and truncation: \n",
    "\n",
    "`Tokenizer.enable_padding(**args)` -- Enable padding\n",
    "\n",
    "`Tokenizer.padding` -- Info about padding\n",
    "\n",
    "`Tokenizer.no_padding()` -- Disable padding\n",
    "\n",
    "`Tokenizer.enable_truncation(**args)` -- Enable truncation \n",
    "\n",
    "`Tokenizer.truncation` -- Info about truncation \n",
    "\n",
    "`Tokenizer.no_truncation()` -- Disable truncation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁不要只追求漂亮的外表', ',它会', '你的眼睛', ';', '▁不要只追求', '富', ',它', '只不过', '是过眼云;', '追求', '一个', '能让你', '真心', '微笑的人,', '▁只有', '笑容', '能使', '黑', '夜', '不再', '漫长', ',走', ',', '带来', '温暖', '阳光']\n",
      "['▁\"', '▁——', '我希望找到这样一个人,即使', '我', '微笑着', '说“', '我还', '好”', '的时候', ',他', '也能', '觉得', '到', '我的', '痛苦', '。']\n",
      "['▁生活的', '地', '平', '线', '是', '随着', '心的', '开', '而变', '广', '的。', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "{'length': 15, 'pad_to_multiple_of': None, 'pad_id': 0, 'pad_token': '[PAD]', 'pad_type_id': 0, 'direction': 'right'}\n"
     ]
    }
   ],
   "source": [
    "# With Padding\n",
    "chTokenizer.enable_padding(length = 15)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁不要只追求漂亮的外表', ',它会', '你的眼睛', ';', '▁不要只追求', '富', ',它', '只不过', '是过眼云;', '追求', '一个', '能让你', '真心', '微笑的人,', '▁只有', '笑容', '能使', '黑', '夜', '不再', '漫长', ',走', ',', '带来', '温暖', '阳光']\n",
      "['▁\"', '▁——', '我希望找到这样一个人,即使', '我', '微笑着', '说“', '我还', '好”', '的时候', ',他', '也能', '觉得', '到', '我的', '痛苦', '。']\n",
      "['▁生活的', '地', '平', '线', '是', '随着', '心的', '开', '而变', '广', '的。']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No padding\n",
    "chTokenizer.no_padding()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁不要只追求漂亮的外表', ',它会', '你的眼睛']\n",
      "['▁\"', '▁——', '我希望找到这样一个人,即使']\n",
      "['▁生活的', '地', '平']\n",
      "{'max_length': 3, 'stride': 0, 'strategy': 'longest_first'}\n"
     ]
    }
   ],
   "source": [
    "# With truncation\n",
    "chTokenizer.enable_truncation(max_length = 3)\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁不要只追求漂亮的外表', ',它会', '你的眼睛', ';', '▁不要只追求', '富', ',它', '只不过', '是过眼云;', '追求', '一个', '能让你', '真心', '微笑的人,', '▁只有', '笑容', '能使', '黑', '夜', '不再', '漫长', ',走', ',', '带来', '温暖', '阳光']\n",
      "['▁\"', '▁——', '我希望找到这样一个人,即使', '我', '微笑着', '说“', '我还', '好”', '的时候', ',他', '也能', '觉得', '到', '我的', '痛苦', '。']\n",
      "['▁生活的', '地', '平', '线', '是', '随着', '心的', '开', '而变', '广', '的。']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No truncation\n",
    "chTokenizer.no_truncation()\n",
    "\n",
    "output_batch = chTokenizer.encode_batch(chTexts[:3])\n",
    "\n",
    "for output in output_batch:\n",
    "    print(output.tokens)\n",
    "    \n",
    "print(chTokenizer.truncation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;\">Pending problem.</span> As I tried to follow the tutorial and load the tokenizer saved on disk, unexpected error was reported. For now, skip loading saved tokenizer and proceed with other important steps.  \n",
    "\n",
    "<span style=\"color:red;\">Bottleneck for now.</span> Do we need special token for T5? If yes, how to insert special T5 tokens into our tokenization? Similar to `tokenizers.processors.BertProcessing`, do we have `tokenizers.processors.T5Processing`? \n",
    "\n",
    "<span style=\"color:red;\">Solution.</span> 1. Thoroughtly read documentation for T5 model in huggingface doc; 2. Explore `huggingface/tokenizers` library on github. \n",
    "\n",
    "For now, halt with tokenizer and proceed with language model until bumping into problems. Keep in mind the confusion about special token. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1811, 970, 925, 5501, 4278, 950, 1174, 2443, 20, 1811, 970, 925, 6057, 3651, 936, 4384, 3427, 2195, 925, 1074, 1002, 1407, 874, 2725] ['▁Don’t', '▁go', '▁for', '▁looks;', 'they', '▁can', '▁de', 'ceive', '.', '▁Don’t', '▁go', '▁for', '▁wealth;', 'even', '▁that', '▁fades', '▁away.', '▁Go', '▁for', '▁someone', '▁who', '▁makes', '▁you', '▁smile.'] [(0, 5), (5, 8), (8, 12), (12, 19), (19, 23), (23, 27), (27, 30), (30, 35), (35, 36), (36, 42), (42, 45), (45, 49), (49, 57), (57, 61), (61, 66), (66, 72), (72, 78), (78, 81), (81, 85), (85, 93), (93, 97), (97, 103), (103, 107), (107, 114)] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "enTokenizer = SentencePieceBPETokenizer()\n",
    "\n",
    "enTokenizer.train([pathAllEn], \n",
    "                vocab_size = 20000, \n",
    "               special_tokens = ['<s>', '<pad>', '</s>', '<unk>', '<mask>'])\n",
    "\n",
    "# Show an example of tokenizer works\n",
    "output = enTokenizer.encode(enTexts[0])\n",
    "print(output.ids, output.tokens, output.offsets, output.attention_mask)\n",
    "\n",
    "# We shall save the tokenizer to disk \n",
    "# tokenizer.save_model('.', 'myTokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before training\n",
    "\n",
    "To utilize PyTorch and GPU computation, we need to create instances of `Dataset` object. \n",
    "\n",
    "`DataLoader` class allows us to iterate a dataset with given batch size. We will define dataloaders in `T5FineTuner` class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell overwrites `Dataset` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Load original data and util from memory or file\n",
    "    Texts must be passed as lists \n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 chTexts, enTexts, # Suppose the two colums are of the same length\n",
    "                 chTokenizer, enTokenizer, \n",
    "                 chMaxLen, enMaxLen): \n",
    "        super().__init__()\n",
    "        self.chTexts = chTexts \n",
    "        self.enTexts = enTexts\n",
    "        self.chTokenizer = chTokenizer\n",
    "        self.enTokenizer = enTokenizer \n",
    "        \n",
    "        # Enable padding and truncation\n",
    "        self.chTokenizer.enable_padding(length = chMaxLen)\n",
    "        self.chTokenizer.enable_truncation(max_length = chMaxLen)\n",
    "        self.enTokenizer.enable_padding(length = enMaxLen)\n",
    "        self.enTokenizer.enable_truncation(max_length = enMaxLen)\n",
    "        \n",
    "    '''\n",
    "    Return the size of dataset\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.chTexts)\n",
    "    \n",
    "    '''\n",
    "    -- The routine for querying one data entry \n",
    "    -- The index of must be specified as an argument\n",
    "    -- Return a dictionary \n",
    "    '''\n",
    "    def __getitem__(self, idx): \n",
    "        # Apply tokenizer \n",
    "        chOutputs = chTokenizer.encode(chTexts[idx])\n",
    "        enOutputs = enTokenizer.encode(enTexts[idx])\n",
    "        \n",
    "        # Get numerical tokens\n",
    "        chEncoding = chOutputs.ids\n",
    "        enEncoding = enOutputs.ids\n",
    "        \n",
    "        # Get attention mask \n",
    "        chMask = chOutputs.attention_mask\n",
    "        enMask = enOutputs.attention_mask\n",
    "        \n",
    "        return {\n",
    "            'source_ids': torch.tensor(chEncoding), \n",
    "            'source_mask': torch.tensor(chMask), \n",
    "            'target_ids': torch.tensor(enEncoding), \n",
    "            'target_mask': torch.tensor(enMask)\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n"
     ]
    }
   ],
   "source": [
    "chMaxLen = 100\n",
    "enMaxLen = 100\n",
    "\n",
    "dataset = MyDataset(chTexts[:1500], enTexts[:1500], \n",
    "                    chTokenizer, enTokenizer, \n",
    "                    chMaxLen = chMaxLen, enMaxLen = enMaxLen)\n",
    "\n",
    "print(len(dataset))\n",
    "# print(dataset.__getitem__(0))\n",
    "\n",
    "# dataloader = DataLoader(dataset, batch_size = 16, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class\n",
    "\n",
    "PyTorch native, despite its great flexibility, may trap you in detailed errors that mess up the entire code. For example, you may forget important details like `optimizer.zero_grad()` or `tensor.to(device)` in PyTorch native. For both learning purpose and clarity in the long run, we use `pytorch_lightning` to define model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule): \n",
    "    \n",
    "    ''' Part 1: Define the architecture of model in init '''\n",
    "    def __init__(self, hparams): \n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparams['pretrainedModelName'], \n",
    "            return_dict = True    # I set return_dict true so that outputs  are presented as dictionaries\n",
    "        )\n",
    "        self.chTokenizer = hparams['chTokenizer']\n",
    "        self.enTokenizer = hparams['enTokenizer']\n",
    "        # self.rouge_metric = nlp.load_metric('rouge')\n",
    "        \n",
    "        # No idea what the \"freeze\" is doing\n",
    "        if self.hparams['freeze_embeds']:\n",
    "            self.freeze_embeds()\n",
    "        if self.hparams['freeze_encoder']:\n",
    "            self.freeze_params(self.model.get_encoder())\n",
    "            assert_all_frozen(self.model.get_encoder())\n",
    "            \n",
    "            \n",
    "            \n",
    "    ''' Part 2: Define the forward propagation '''\n",
    "    def forward(self, \n",
    "                input_ids, \n",
    "                attention_mask = None, \n",
    "                decoder_input_ids = None, \n",
    "                decoder_attention_mask = None, \n",
    "                lm_labels = None\n",
    "               ): \n",
    "        # Type `Seq2SeqLMOutput`\n",
    "        return self.model(\n",
    "            input_ids, \n",
    "            attention_mask = attention_mask, \n",
    "            decoder_input_ids = decoder_input_ids, \n",
    "            decoder_attention_mask = decoder_attention_mask, \n",
    "            lm_labels = lm_labels\n",
    "        )\n",
    "    \n",
    "    \n",
    "    ''' Part 3: Prepare optimizer and scheduler '''\n",
    "    def configure_optimizers(self): \n",
    "        model = self.model \n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                # model.named_parameters() can't find doc?\n",
    "                'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \n",
    "                'weight_decay': self.hparams['weight_decay']\n",
    "            }, \n",
    "            {\n",
    "                'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                'weight_decay': 0.0\n",
    "            }\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters, \n",
    "            lr = self.hparams['learning_rate']\n",
    "        )\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "    \n",
    "    \n",
    "    # Override this method to adjust how Trainer calls each optimizer \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure = None, on_tpu = False, using_native_amp = False, using_lbfgs = False):\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()   \n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 4: Define training logic\n",
    "    -- In PyTorch native, we have to manually define the epoch loop, define the batch loop, and manually perform model.train(), loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def training_step(self, batch, batch_idx): \n",
    "        loss = self._step(batch)\n",
    "        tensorboard_logs = {'train_loss': loss}\n",
    "        self.log('my_loss', loss, prog_bar = True, on_step = True, on_epoch = True, logger = True)\n",
    "        return {'loss': loss, 'log': tensorboard_logs}\n",
    "    \n",
    "    \n",
    "    # subroutine for training_step()\n",
    "    def _step(self, batch): \n",
    "        lm_labels = batch['target_ids']    # !! Does not apply! \n",
    "        lm_labels[lm_labels[:, ] == 0] = -100    # !! Verify that id for pad is 0 \n",
    "         \n",
    "        # !! There is a `__call__` method associated with self ?! \n",
    "        outputs = self(\n",
    "            input_ids = batch['source_ids'],    # !! Does not apply! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            lm_labels = lm_labels, \n",
    "            decoder_attention_mask = batch['target_mask']\n",
    "        )\n",
    "        \n",
    "        return outputs.loss    # !! Or should it be outputs[0] ? \n",
    "    \n",
    "    \n",
    "    # Called at the end of training epoch \n",
    "    # Do something with all the outputs from every training step \n",
    "    def training_epoch_end(self, outputs): \n",
    "        avg_train_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'avg_train_loss': avg_train_loss}\n",
    "        return {\n",
    "            'avg_train_loss': avg_train_loss, \n",
    "            'log': tensorboard_logs, \n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    -- Part 5: Define validation logic\n",
    "    -- In PyTorch native, we have to define the batch loop, and manually perform model.eval(), torch.no_grad()\n",
    "    -- In pytorch_lightening, the training_step() method only needs to return the loss of the batch\n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx): \n",
    "        return self._generative_step(batch)\n",
    "    \n",
    "    # subroutine for validation_step()\n",
    "    def _generative_step(self, batch): \n",
    "        t0 = time.time()\n",
    "        \n",
    "        # !! model.generate() Can't find doc !\n",
    "        generated_ids = self.model.generate(\n",
    "            batch['source_ids'],    # !! Does not apply ! \n",
    "            attention_mask = batch['source_mask'], \n",
    "            use_cache = True, \n",
    "            decoder_attention_mask = batch['target_mask'],     # !! Does not apply! \n",
    "            max_length = self.hparams['max_output_len'],\n",
    "            num_beams = 2,     # ?? What is this?\n",
    "            repetition_penalty = 2.5,     # ?? What is this?\n",
    "            length_penalty = 1.0,     # ?? What is this?\n",
    "            early_stopping = True    # ?? What is this?\n",
    "        )\n",
    "        \n",
    "        preds = self.ids_to_clean_text(enTokenizer, generated_ids)    # translation predicted by model \n",
    "        target = self.ids_to_clean_text(enTokenizer, batch['target_ids'])    # !! Does not apply! \n",
    "        \n",
    "        gen_time = (time.time() - t0) / batch['source_ids'].shape[0]    # !! Does not apply\n",
    "        \n",
    "        loss = self._step(batch)\n",
    "        \n",
    "        # Compute metrics\n",
    "        # ?? What is the deal with \"rouge\" in the code example? \n",
    "        base_metrics = {'val_loss': loss}\n",
    "        trans_len = np.mean(list(map(len, generated_ids)))\n",
    "        base_metrics.update(\n",
    "            gen_time = gen_time, \n",
    "            gen_len = trans_len, \n",
    "            preds = preds, \n",
    "            target = target\n",
    "        )\n",
    "        # self.rouge_metric.add_batch(preds, target)\n",
    "        \n",
    "        return base_metrics\n",
    "        \n",
    "    \n",
    "    #\n",
    "    def validation_epoch_end(self, outputs): \n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss}\n",
    "        \n",
    "        # rouge_results = self.rouge_metric.compute()\n",
    "        # rouge_dict = self.parse_score(rouge_results)\n",
    "        # tensorboard_logs.update(rouge1 = rouge_dict['rouge1'], rougeL = rouge_dict['rougeL'])\n",
    "        \n",
    "        # Clear out the lists for next epoch \n",
    "        # !! I don't see those two variables defined anywhere \n",
    "        self.target_gen = []\n",
    "        self.prediction_gen = []\n",
    "        \n",
    "        return {\n",
    "            'avg_val_loss': avg_loss, \n",
    "            # 'rouge1': rouge_results['rouge1'], \n",
    "            # 'rougeL': rouge_results['rougeL'], \n",
    "            'log': tensorboard_logs,\n",
    "            'progress_bar': tensorboard_logs\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''Part 6: Define dataloaders'''\n",
    "    def train_dataloader(self): \n",
    "        train_dataset = get_dataset(\n",
    "            chTexts = chTexts[:1800], \n",
    "            enTexts = enTexts[:1800], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size = self.hparams['train_batch_size'], \n",
    "            drop_last = True, \n",
    "            shuffle = True, \n",
    "            num_workers = 0 \n",
    "        )\n",
    "        \n",
    "        # The code below deals with scheduler. And I have no idea what the code is doing \n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) // (self.hparams['train_batch_size'] * max(1, self.hparams['n_gpu'])))\n",
    "            // self.hparams['gradient_accumulation_steps']\n",
    "            * float(self.hparams['num_train_epochs'])\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparams['warmup_steps'], num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            chTexts = chTexts[1800:1950], \n",
    "            enTexts = enTexts[1800:1950], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size = self.hparams['eval_batch_size'], \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_dataset = get_dataset(\n",
    "            chTexts = chTexts[1950:], \n",
    "            enTexts = enTexts[1950:], \n",
    "            chTokenizer = self.chTokenizer, \n",
    "            enTokenizer = self.enTokenizer, \n",
    "            chMaxLen = self.hparams['max_input_len'], \n",
    "            enMaxLen = self.hparams['max_output_len']\n",
    "        )\n",
    "        \n",
    "        return DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size = self.hparams['eval_batch_size'], \n",
    "            num_workers = 0\n",
    "        )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' ==================================\n",
    "    # Collection of helper functions \n",
    "    # Not predefined by LightningModule\n",
    "    ===================================='''\n",
    "    \n",
    "    # Decode a batch of ids and return a list of strings \n",
    "    def ids_to_clean_text(self, tokenizer, ids_batch): \n",
    "        ids_batch_tensor = torch.tensor(ids_batch)\n",
    "        # Make sure that the ids come as a batch and that decode_batch() method will work properly \n",
    "        assert (ids_batch_tensor.ndim >= 2), 'Ids do not form a batch'\n",
    "        return tokenizer.decode_batch(ids_batch.tolist())\n",
    "            \n",
    "        \n",
    "    # tqdm is a library for showing progress bar \n",
    "    # Retrieve info needed for progresse bar \n",
    "    def get_tqdm_dict(self): \n",
    "        # !! What is self.trainer? I never saw it defined \n",
    "        tqdm_dict = {\n",
    "            'loss': '{:.3f}'.format(self.trainer.avg_loss), \n",
    "            'lr': self.lr_scheduler.get_last_lr()[-1]\n",
    "        }\n",
    "        return tqdm_dict\n",
    "    \n",
    "    '''=================================\n",
    "    # Methods that I have no idea what they are doing \n",
    "    =================================='''\n",
    "    def freeze_params(self, model):\n",
    "        for par in model.parameters():\n",
    "            par.requires_grad = False\n",
    "            \n",
    "            \n",
    "    def freeze_embeds(self):\n",
    "        # Freeze token embeddings and positional embeddings for bart, just token embeddings for t5.\n",
    "        try:\n",
    "            self.freeze_params(self.model.model.shared)\n",
    "            for d in [self.model.model.encoder, self.model.model.decoder]:\n",
    "                freeze_params(d.embed_positions)\n",
    "                freeze_params(d.embed_tokens)\n",
    "        except AttributeError:\n",
    "            self.freeze_params(self.model.shared)\n",
    "            for d in [self.model.encoder, self.model.decoder]:\n",
    "                self.freeze_params(d.embed_tokens)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'chTokenizer': chTokenizer, \n",
    "    'enTokenizer': enTokenizer, \n",
    "    'pretrainedModelName': 't5-small', \n",
    "    'weight_decay': 0.0, \n",
    "    'learning_rate': 3e-4, \n",
    "    'max_input_len': 100, \n",
    "    'max_output_len': 100, \n",
    "    'train_batch_size': 8, \n",
    "    'eval_batch_size': 8, \n",
    "    'num_train_epochs': 2, \n",
    "    'n_gpu': 1\n",
    "    # For now, we do train-test split manually when defining dataloader, instead of loading the following param \n",
    "    # 'n_train': 2000\n",
    "    # 'n_val': 150\n",
    "    # 'n_test': 50\n",
    "}\n",
    "\n",
    "# ?? What are these hyperparameters ? \n",
    "hparamsNoUnderstand = {\n",
    "    'freeze_encoder': False, \n",
    "    'freeze_embeds': False, \n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'resume_from_checkpoint': None, \n",
    "    'val_check_interval': 0.05,\n",
    "    'early_stop_callback': False, \n",
    "    'fp_16': False, \n",
    "    'opt_level': 'O1', \n",
    "    'max_grad_norm': 1.0, \n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "hparams.update(hparamsNoUnderstand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea what the following code cells are doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "## If resuming from checkpoint, add an arg resume_from_checkpoint\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=hparams['gradient_accumulation_steps'],\n",
    "    gpus=hparams['n_gpu'],\n",
    "    max_epochs=hparams['num_train_epochs'], \n",
    "    # early_stop_callback=False,\n",
    "    precision= 16 if hparams['fp_16'] else 32,\n",
    "    amp_level=hparams['opt_level'],\n",
    "    resume_from_checkpoint=hparams['resume_from_checkpoint'],\n",
    "    gradient_clip_val=hparams['max_grad_norm'], \n",
    "    # checkpoint_callback=checkpoint_callback,\n",
    "    val_check_interval=hparams['val_check_interval'],\n",
    "    logger = tb_logger\n",
    "    # callbacks=[LoggingCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen):\n",
    "    return MyDataset(chTexts, enTexts, chTokenizer, enTokenizer, chMaxLen, enMaxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60 M  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validation sanity check'), FloatProgress(value=1.0, bar_style='info', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-8d2413b0f985>:258: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ids_batch_tensor = torch.tensor(ids_batch)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\transformers\\modeling_t5.py:1144: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018e64e2a39645db8544aa30d3e51ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebe00a7157344d283b5649915b0ec77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Validating'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\presu\\miniconda3\\envs\\cs701\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5FineTuner(hparams)\n",
    "trainer = pl.Trainer(**train_params)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_checkpoint(\"safecheck2011121455_t5_ch_en_1.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from saved model \n",
    "modelLoaded = T5FineTuner.load_from_checkpoint(checkpoint_path=\"t5_ch_en_1.ckpt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hparamsDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8fce959ace3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mchTokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0menTokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mchMaxLen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhparamsDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_input_len'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0menMaxLen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhparamsDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_output_len'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hparamsDict' is not defined"
     ]
    }
   ],
   "source": [
    "import textwrap \n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "datashow = get_dataset(\n",
    "            chTexts = chTexts, \n",
    "            enTexts = enTexts, \n",
    "            chTokenizer = chTokenizer, \n",
    "            enTokenizer = enTokenizer, \n",
    "            chMaxLen = hparamsDict['max_input_len'], \n",
    "            enMaxLen = hparamsDict['max_output_len']\n",
    "        )\n",
    "\n",
    "loader = DataLoader(datashow, batch_size = 32)\n",
    "it = iter(loader)\n",
    "\n",
    "batch = next(it)\n",
    "\n",
    "outs = modelLoaded.model.generate(\n",
    "    batch['source_ids'].cuda(), \n",
    "    attention_mask=batch['source_mask'].cuda(),\n",
    "    use_cache=True,\n",
    "    decoder_attention_mask=batch['target_mask'].cuda(),\n",
    "    max_length = hparamsDict['max_output_len'], \n",
    "    num_beams = 2, \n",
    "    repetition_penalty = 2.5, \n",
    "    length_penalty = 1.0, \n",
    "    early_stopping = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [enTokenizer.decode(ids) for ids in outs.tolist()]\n",
    "\n",
    "texts = [chTokenizer.decode(ids) for ids in batch['source_ids'].tolist()]\n",
    "targets = [enTokenizer.decode(ids) for ids in batch['target_ids'].tolist()]\n",
    "\n",
    "for i in range(32):\n",
    "    lines = textwrap.wrap(\"Chinese Text:\\n%s\\n\" % texts[i], width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual translation: %s\" % targets[i])\n",
    "    print(\"\\nPredicted translation: %s\" % preds[i])\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
